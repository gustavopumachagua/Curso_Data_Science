{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atrás 4** | **Siguiente 6** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [🏠](../../README.md) | [⏪](./4.Clustering.ipynb)| [⏩](./6.Reinforcement_Learning.ipynb)|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Reglas de Asociación**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Apriori**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori es un algoritmo utilizado en el aprendizaje automático para descubrir patrones y relaciones entre conjuntos de elementos. En particular, se utiliza en la minería de datos y las reglas de asociación para descubrir patrones frecuentes en conjuntos de datos transaccionales.\n",
    "\n",
    "El algoritmo Apriori se basa en la idea de que los conjuntos de elementos que aparecen con frecuencia juntos en un conjunto de datos son importantes y pueden utilizarse para hacer predicciones sobre nuevos conjuntos de datos. Por lo tanto, el algoritmo Apriori utiliza un enfoque de fuerza bruta para identificar patrones frecuentes, generando todas las combinaciones posibles de elementos y calculando su frecuencia.\n",
    "\n",
    "El algoritmo Apriori consta de tres etapas principales:\n",
    "\n",
    "**1. Generar conjuntos de elementos candidatos:**\n",
    "\n",
    " Se comienza con conjuntos de un solo elemento y se van generando conjuntos de elementos más grandes combinando conjuntos previos, asegurándose de que cada conjunto candidato tenga un tamaño específico.\n",
    "\n",
    "**2. Evaluar la frecuencia de cada conjunto candidato:**\n",
    "\n",
    " Se escanea la base de datos para contar la frecuencia de cada conjunto candidato.\n",
    "\n",
    "**3. Eliminar conjuntos infrecuentes:**\n",
    "\n",
    " Los conjuntos de elementos cuya frecuencia no cumple con un umbral predefinido son eliminados.\n",
    "\n",
    "Este proceso se repite iterativamente hasta que ya no se pueden generar conjuntos de elementos más grandes o hasta que no se encuentren patrones de interés."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el área de Machine Learning, las Reglas de Asociación son una técnica utilizada para encontrar patrones interesantes en conjuntos de datos. La regla de asociación más conocida es la Regla de Asociación Apriori. Esta técnica se utiliza para encontrar patrones frecuentes en conjuntos de datos y, a continuación, utilizar esos patrones para crear reglas de asociación.\n",
    "\n",
    "La Regla de Asociación Apriori se basa en el principio de que si un conjunto de elementos aparece con frecuencia en un conjunto de datos, entonces los elementos individuales de ese conjunto también aparecerán con frecuencia. En otras palabras, si un conjunto de elementos se cumple, también se cumplirán todos los subconjuntos de ese conjunto.\n",
    "\n",
    "Por ejemplo, si un cliente compra pan y leche juntos con frecuencia, entonces es probable que también compre pan o leche por separado. Utilizando la Regla de Asociación Apriori, podemos identificar patrones frecuentes como este en los datos.\n",
    "\n",
    "Aquí te dejo un ejemplo de código en Python utilizando la biblioteca `mlxtend` para aplicar la Regla de Asociación Apriori en un conjunto de datos de compras de supermercado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6892/3091160903.py:10: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  df_onehot = pd.get_dummies(df[0].str.split(',', expand=True).stack()).sum(level=0)\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/mlxtend/frequent_patterns/fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          antecedents                       consequents  \\\n",
      "0                              (beef)                (other vegetables)   \n",
      "1                  (other vegetables)                            (beef)   \n",
      "2                        (rolls/buns)                            (beef)   \n",
      "3                              (beef)                      (rolls/buns)   \n",
      "4                              (beef)                 (root vegetables)   \n",
      "..                                ...                               ...   \n",
      "593  (whole milk, whipped/sour cream)                          (yogurt)   \n",
      "594      (yogurt, whipped/sour cream)                      (whole milk)   \n",
      "595                      (whole milk)      (yogurt, whipped/sour cream)   \n",
      "596                          (yogurt)  (whole milk, whipped/sour cream)   \n",
      "597              (whipped/sour cream)              (whole milk, yogurt)   \n",
      "\n",
      "     antecedent support  consequent support   support  confidence      lift  \\\n",
      "0              0.052466            0.193493  0.019725    0.375969  1.943066   \n",
      "1              0.193493            0.052466  0.019725    0.101944  1.943066   \n",
      "2              0.183935            0.052466  0.013625    0.074074  1.411858   \n",
      "3              0.052466            0.183935  0.013625    0.259690  1.411858   \n",
      "4              0.052466            0.108998  0.017387    0.331395  3.040367   \n",
      "..                  ...                 ...       ...         ...       ...   \n",
      "593            0.032232            0.139502  0.010880    0.337539  2.419607   \n",
      "594            0.020742            0.255516  0.010880    0.524510  2.052747   \n",
      "595            0.255516            0.020742  0.010880    0.042579  2.052747   \n",
      "596            0.139502            0.032232  0.010880    0.077988  2.419607   \n",
      "597            0.071683            0.056024  0.010880    0.151773  2.709053   \n",
      "\n",
      "     leverage  conviction  zhangs_metric  \n",
      "0    0.009574    1.292416       0.512224  \n",
      "1    0.009574    1.055095       0.601792  \n",
      "2    0.003975    1.023337       0.357463  \n",
      "3    0.003975    1.102329       0.307866  \n",
      "4    0.011668    1.332628       0.708251  \n",
      "..        ...         ...            ...  \n",
      "593  0.006383    1.298943       0.606250  \n",
      "594  0.005580    1.565719       0.523711  \n",
      "595  0.005580    1.022807       0.688864  \n",
      "596  0.006383    1.049627       0.681826  \n",
      "597  0.006864    1.112881       0.679582  \n",
      "\n",
      "[598 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Cargamos los datos\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/groceries.csv', header=None, sep=';')\n",
    "\n",
    "# Convertimos los datos a un formato adecuado para la biblioteca mlxtend\n",
    "# (cada fila es una transacción y cada columna representa un producto)\n",
    "df_onehot = pd.get_dummies(df[0].str.split(',', expand=True).stack()).sum(level=0)\n",
    "\n",
    "# Generamos los conjuntos de elementos frecuentes\n",
    "frequent_itemsets = apriori(df_onehot, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Generamos las reglas de asociación a partir de los conjuntos de elementos frecuentes\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Visualizamos las reglas de asociación\n",
    "print(rules)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, utilizamos la biblioteca `mlxtend` para cargar los datos de un conjunto de compras de supermercado y transformarlos en un formato adecuado para la Regla de Asociación Apriori. A continuación, generamos los conjuntos de elementos frecuentes utilizando el método apriori, y generamos las reglas de asociación utilizando el método `association_rules`. Finalmente, visualizamos las reglas de asociación resultantes.\n",
    "\n",
    "En cuanto a la visualización de los resultados, se pueden utilizar gráficas de barras para mostrar los conjuntos de elementos frecuentes y los valores de soporte asociados, así como gráficas de red para mostrar las reglas de asociación y sus valores de confianza y lift."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Eclat**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Eclat` es otro algoritmo utilizado en el área de Reglas de Asociación de Machine Learning, al igual que Apriori. El algoritmo `Eclat` significa \"Equivalence Class Clustering and bottom-up Lattice Traversal\". Este algoritmo se basa en encontrar los conjuntos de elementos que aparecen juntos con mayor frecuencia en un conjunto de transacciones.\n",
    "\n",
    "El algoritmo `Eclat` funciona de manera similar al algoritmo Apriori, pero en lugar de calcular el soporte de cada conjunto de elementos, se calcula la intersección de cada par de elementos y su soporte.\n",
    "\n",
    "En términos de complejidad computacional, `Eclat` es más rápido que Apriori, especialmente cuando se trabaja con conjuntos de datos grandes. Además, `Eclat` puede manejar conjuntos de datos más grandes que Apriori.\n",
    "\n",
    "A continuación, se muestra un ejemplo de cómo utilizar el algoritmo `Eclat` en Python utilizando la biblioteca `mlxtend`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6892/3226077182.py:10: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  df_onehot = pd.get_dummies(df[0].str.split(',', expand=True).stack()).sum(level=0)\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/mlxtend/frequent_patterns/fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          antecedents                       consequents  \\\n",
      "0                              (beef)                (other vegetables)   \n",
      "1                  (other vegetables)                            (beef)   \n",
      "2                        (rolls/buns)                            (beef)   \n",
      "3                              (beef)                      (rolls/buns)   \n",
      "4                              (beef)                 (root vegetables)   \n",
      "..                                ...                               ...   \n",
      "593  (whole milk, whipped/sour cream)                          (yogurt)   \n",
      "594      (yogurt, whipped/sour cream)                      (whole milk)   \n",
      "595                      (whole milk)      (yogurt, whipped/sour cream)   \n",
      "596                          (yogurt)  (whole milk, whipped/sour cream)   \n",
      "597              (whipped/sour cream)              (whole milk, yogurt)   \n",
      "\n",
      "     antecedent support  consequent support   support  confidence      lift  \\\n",
      "0              0.052466            0.193493  0.019725    0.375969  1.943066   \n",
      "1              0.193493            0.052466  0.019725    0.101944  1.943066   \n",
      "2              0.183935            0.052466  0.013625    0.074074  1.411858   \n",
      "3              0.052466            0.183935  0.013625    0.259690  1.411858   \n",
      "4              0.052466            0.108998  0.017387    0.331395  3.040367   \n",
      "..                  ...                 ...       ...         ...       ...   \n",
      "593            0.032232            0.139502  0.010880    0.337539  2.419607   \n",
      "594            0.020742            0.255516  0.010880    0.524510  2.052747   \n",
      "595            0.255516            0.020742  0.010880    0.042579  2.052747   \n",
      "596            0.139502            0.032232  0.010880    0.077988  2.419607   \n",
      "597            0.071683            0.056024  0.010880    0.151773  2.709053   \n",
      "\n",
      "     leverage  conviction  zhangs_metric  \n",
      "0    0.009574    1.292416       0.512224  \n",
      "1    0.009574    1.055095       0.601792  \n",
      "2    0.003975    1.023337       0.357463  \n",
      "3    0.003975    1.102329       0.307866  \n",
      "4    0.011668    1.332628       0.708251  \n",
      "..        ...         ...            ...  \n",
      "593  0.006383    1.298943       0.606250  \n",
      "594  0.005580    1.565719       0.523711  \n",
      "595  0.005580    1.022807       0.688864  \n",
      "596  0.006383    1.049627       0.681826  \n",
      "597  0.006864    1.112881       0.679582  \n",
      "\n",
      "[598 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Cargamos los datos\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/groceries.csv', header=None, sep=';')\n",
    "\n",
    "# Convertimos los datos a un formato adecuado para la biblioteca mlxtend\n",
    "# (cada fila es una transacción y cada columna representa un producto)\n",
    "df_onehot = pd.get_dummies(df[0].str.split(',', expand=True).stack()).sum(level=0)\n",
    "\n",
    "# Generamos los conjuntos de elementos frecuentes utilizando el algoritmo Apriori\n",
    "frequent_itemsets = apriori(df_onehot, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Generamos las reglas de asociación a partir de los conjuntos de elementos frecuentes\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Visualizamos las reglas de asociación\n",
    "print(rules)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atrás 4** | **Siguiente 6** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [🏠](../../README.md) | [⏪](./4.Clustering.ipynb)| [⏩](./6.Reinforcement_Learning.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
