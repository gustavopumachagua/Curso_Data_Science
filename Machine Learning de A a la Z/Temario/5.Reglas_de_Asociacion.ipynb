{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr谩s 4** | **Siguiente 6** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [](../../README.md) | [](./4.Clustering.ipynb)| [](./6.Reinforcement_Learning.ipynb)|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Reglas de Asociaci贸n**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Apriori**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori es un algoritmo utilizado en el aprendizaje autom谩tico para descubrir patrones y relaciones entre conjuntos de elementos. En particular, se utiliza en la miner铆a de datos y las reglas de asociaci贸n para descubrir patrones frecuentes en conjuntos de datos transaccionales.\n",
    "\n",
    "El algoritmo Apriori se basa en la idea de que los conjuntos de elementos que aparecen con frecuencia juntos en un conjunto de datos son importantes y pueden utilizarse para hacer predicciones sobre nuevos conjuntos de datos. Por lo tanto, el algoritmo Apriori utiliza un enfoque de fuerza bruta para identificar patrones frecuentes, generando todas las combinaciones posibles de elementos y calculando su frecuencia.\n",
    "\n",
    "El algoritmo Apriori consta de tres etapas principales:\n",
    "\n",
    "**1. Generar conjuntos de elementos candidatos:**\n",
    "\n",
    " Se comienza con conjuntos de un solo elemento y se van generando conjuntos de elementos m谩s grandes combinando conjuntos previos, asegur谩ndose de que cada conjunto candidato tenga un tama帽o espec铆fico.\n",
    "\n",
    "**2. Evaluar la frecuencia de cada conjunto candidato:**\n",
    "\n",
    " Se escanea la base de datos para contar la frecuencia de cada conjunto candidato.\n",
    "\n",
    "**3. Eliminar conjuntos infrecuentes:**\n",
    "\n",
    " Los conjuntos de elementos cuya frecuencia no cumple con un umbral predefinido son eliminados.\n",
    "\n",
    "Este proceso se repite iterativamente hasta que ya no se pueden generar conjuntos de elementos m谩s grandes o hasta que no se encuentren patrones de inter茅s."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el 谩rea de Machine Learning, las Reglas de Asociaci贸n son una t茅cnica utilizada para encontrar patrones interesantes en conjuntos de datos. La regla de asociaci贸n m谩s conocida es la Regla de Asociaci贸n Apriori. Esta t茅cnica se utiliza para encontrar patrones frecuentes en conjuntos de datos y, a continuaci贸n, utilizar esos patrones para crear reglas de asociaci贸n.\n",
    "\n",
    "La Regla de Asociaci贸n Apriori se basa en el principio de que si un conjunto de elementos aparece con frecuencia en un conjunto de datos, entonces los elementos individuales de ese conjunto tambi茅n aparecer谩n con frecuencia. En otras palabras, si un conjunto de elementos se cumple, tambi茅n se cumplir谩n todos los subconjuntos de ese conjunto.\n",
    "\n",
    "Por ejemplo, si un cliente compra pan y leche juntos con frecuencia, entonces es probable que tambi茅n compre pan o leche por separado. Utilizando la Regla de Asociaci贸n Apriori, podemos identificar patrones frecuentes como este en los datos.\n",
    "\n",
    "Aqu铆 te dejo un ejemplo de c贸digo en Python utilizando la biblioteca `mlxtend` para aplicar la Regla de Asociaci贸n Apriori en un conjunto de datos de compras de supermercado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6892/3091160903.py:10: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  df_onehot = pd.get_dummies(df[0].str.split(',', expand=True).stack()).sum(level=0)\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/mlxtend/frequent_patterns/fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          antecedents                       consequents  \\\n",
      "0                              (beef)                (other vegetables)   \n",
      "1                  (other vegetables)                            (beef)   \n",
      "2                        (rolls/buns)                            (beef)   \n",
      "3                              (beef)                      (rolls/buns)   \n",
      "4                              (beef)                 (root vegetables)   \n",
      "..                                ...                               ...   \n",
      "593  (whole milk, whipped/sour cream)                          (yogurt)   \n",
      "594      (yogurt, whipped/sour cream)                      (whole milk)   \n",
      "595                      (whole milk)      (yogurt, whipped/sour cream)   \n",
      "596                          (yogurt)  (whole milk, whipped/sour cream)   \n",
      "597              (whipped/sour cream)              (whole milk, yogurt)   \n",
      "\n",
      "     antecedent support  consequent support   support  confidence      lift  \\\n",
      "0              0.052466            0.193493  0.019725    0.375969  1.943066   \n",
      "1              0.193493            0.052466  0.019725    0.101944  1.943066   \n",
      "2              0.183935            0.052466  0.013625    0.074074  1.411858   \n",
      "3              0.052466            0.183935  0.013625    0.259690  1.411858   \n",
      "4              0.052466            0.108998  0.017387    0.331395  3.040367   \n",
      "..                  ...                 ...       ...         ...       ...   \n",
      "593            0.032232            0.139502  0.010880    0.337539  2.419607   \n",
      "594            0.020742            0.255516  0.010880    0.524510  2.052747   \n",
      "595            0.255516            0.020742  0.010880    0.042579  2.052747   \n",
      "596            0.139502            0.032232  0.010880    0.077988  2.419607   \n",
      "597            0.071683            0.056024  0.010880    0.151773  2.709053   \n",
      "\n",
      "     leverage  conviction  zhangs_metric  \n",
      "0    0.009574    1.292416       0.512224  \n",
      "1    0.009574    1.055095       0.601792  \n",
      "2    0.003975    1.023337       0.357463  \n",
      "3    0.003975    1.102329       0.307866  \n",
      "4    0.011668    1.332628       0.708251  \n",
      "..        ...         ...            ...  \n",
      "593  0.006383    1.298943       0.606250  \n",
      "594  0.005580    1.565719       0.523711  \n",
      "595  0.005580    1.022807       0.688864  \n",
      "596  0.006383    1.049627       0.681826  \n",
      "597  0.006864    1.112881       0.679582  \n",
      "\n",
      "[598 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Cargamos los datos\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/groceries.csv', header=None, sep=';')\n",
    "\n",
    "# Convertimos los datos a un formato adecuado para la biblioteca mlxtend\n",
    "# (cada fila es una transacci贸n y cada columna representa un producto)\n",
    "df_onehot = pd.get_dummies(df[0].str.split(',', expand=True).stack()).sum(level=0)\n",
    "\n",
    "# Generamos los conjuntos de elementos frecuentes\n",
    "frequent_itemsets = apriori(df_onehot, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Generamos las reglas de asociaci贸n a partir de los conjuntos de elementos frecuentes\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Visualizamos las reglas de asociaci贸n\n",
    "print(rules)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, utilizamos la biblioteca `mlxtend` para cargar los datos de un conjunto de compras de supermercado y transformarlos en un formato adecuado para la Regla de Asociaci贸n Apriori. A continuaci贸n, generamos los conjuntos de elementos frecuentes utilizando el m茅todo apriori, y generamos las reglas de asociaci贸n utilizando el m茅todo `association_rules`. Finalmente, visualizamos las reglas de asociaci贸n resultantes.\n",
    "\n",
    "En cuanto a la visualizaci贸n de los resultados, se pueden utilizar gr谩ficas de barras para mostrar los conjuntos de elementos frecuentes y los valores de soporte asociados, as铆 como gr谩ficas de red para mostrar las reglas de asociaci贸n y sus valores de confianza y lift."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Eclat**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Eclat` es otro algoritmo utilizado en el 谩rea de Reglas de Asociaci贸n de Machine Learning, al igual que Apriori. El algoritmo `Eclat` significa \"Equivalence Class Clustering and bottom-up Lattice Traversal\". Este algoritmo se basa en encontrar los conjuntos de elementos que aparecen juntos con mayor frecuencia en un conjunto de transacciones.\n",
    "\n",
    "El algoritmo `Eclat` funciona de manera similar al algoritmo Apriori, pero en lugar de calcular el soporte de cada conjunto de elementos, se calcula la intersecci贸n de cada par de elementos y su soporte.\n",
    "\n",
    "En t茅rminos de complejidad computacional, `Eclat` es m谩s r谩pido que Apriori, especialmente cuando se trabaja con conjuntos de datos grandes. Adem谩s, `Eclat` puede manejar conjuntos de datos m谩s grandes que Apriori.\n",
    "\n",
    "A continuaci贸n, se muestra un ejemplo de c贸mo utilizar el algoritmo `Eclat` en Python utilizando la biblioteca `mlxtend`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6892/3226077182.py:10: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  df_onehot = pd.get_dummies(df[0].str.split(',', expand=True).stack()).sum(level=0)\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/mlxtend/frequent_patterns/fpcommon.py:110: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          antecedents                       consequents  \\\n",
      "0                              (beef)                (other vegetables)   \n",
      "1                  (other vegetables)                            (beef)   \n",
      "2                        (rolls/buns)                            (beef)   \n",
      "3                              (beef)                      (rolls/buns)   \n",
      "4                              (beef)                 (root vegetables)   \n",
      "..                                ...                               ...   \n",
      "593  (whole milk, whipped/sour cream)                          (yogurt)   \n",
      "594      (yogurt, whipped/sour cream)                      (whole milk)   \n",
      "595                      (whole milk)      (yogurt, whipped/sour cream)   \n",
      "596                          (yogurt)  (whole milk, whipped/sour cream)   \n",
      "597              (whipped/sour cream)              (whole milk, yogurt)   \n",
      "\n",
      "     antecedent support  consequent support   support  confidence      lift  \\\n",
      "0              0.052466            0.193493  0.019725    0.375969  1.943066   \n",
      "1              0.193493            0.052466  0.019725    0.101944  1.943066   \n",
      "2              0.183935            0.052466  0.013625    0.074074  1.411858   \n",
      "3              0.052466            0.183935  0.013625    0.259690  1.411858   \n",
      "4              0.052466            0.108998  0.017387    0.331395  3.040367   \n",
      "..                  ...                 ...       ...         ...       ...   \n",
      "593            0.032232            0.139502  0.010880    0.337539  2.419607   \n",
      "594            0.020742            0.255516  0.010880    0.524510  2.052747   \n",
      "595            0.255516            0.020742  0.010880    0.042579  2.052747   \n",
      "596            0.139502            0.032232  0.010880    0.077988  2.419607   \n",
      "597            0.071683            0.056024  0.010880    0.151773  2.709053   \n",
      "\n",
      "     leverage  conviction  zhangs_metric  \n",
      "0    0.009574    1.292416       0.512224  \n",
      "1    0.009574    1.055095       0.601792  \n",
      "2    0.003975    1.023337       0.357463  \n",
      "3    0.003975    1.102329       0.307866  \n",
      "4    0.011668    1.332628       0.708251  \n",
      "..        ...         ...            ...  \n",
      "593  0.006383    1.298943       0.606250  \n",
      "594  0.005580    1.565719       0.523711  \n",
      "595  0.005580    1.022807       0.688864  \n",
      "596  0.006383    1.049627       0.681826  \n",
      "597  0.006864    1.112881       0.679582  \n",
      "\n",
      "[598 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Cargamos los datos\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/groceries.csv', header=None, sep=';')\n",
    "\n",
    "# Convertimos los datos a un formato adecuado para la biblioteca mlxtend\n",
    "# (cada fila es una transacci贸n y cada columna representa un producto)\n",
    "df_onehot = pd.get_dummies(df[0].str.split(',', expand=True).stack()).sum(level=0)\n",
    "\n",
    "# Generamos los conjuntos de elementos frecuentes utilizando el algoritmo Apriori\n",
    "frequent_itemsets = apriori(df_onehot, min_support=0.01, use_colnames=True)\n",
    "\n",
    "# Generamos las reglas de asociaci贸n a partir de los conjuntos de elementos frecuentes\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Visualizamos las reglas de asociaci贸n\n",
    "print(rules)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr谩s 4** | **Siguiente 6** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [](../../README.md) | [](./4.Clustering.ipynb)| [](./6.Reinforcement_Learning.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
