{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr√°s 15** | **Siguiente 17** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [üè†](../../README.md) | [‚è™](./15.DBSCAN_Clustering.ipynb)| [‚è©](./17.Bagging%20_Machine_learning.ipynb)|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **16. Ajuste de Hiperpar√°metros de Modelos de Machine Learning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introducci√≥n**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ajuste de hiperpar√°metros en el contexto de modelos de Machine Learning se refiere al proceso de encontrar la combinaci√≥n √≥ptima de valores para los par√°metros que no se aprenden directamente del conjunto de datos durante el entrenamiento del modelo.\n",
    "\n",
    "Los hiperpar√°metros son valores que se definen antes del entrenamiento del modelo y que afectan su capacidad para ajustarse al conjunto de datos. Algunos ejemplos comunes de hiperpar√°metros son la tasa de aprendizaje, el n√∫mero de capas en una red neuronal, la cantidad de √°rboles en un bosque aleatorio, entre otros.\n",
    "\n",
    "El ajuste de hiperpar√°metros implica probar diferentes combinaciones de valores de los hiperpar√°metros y evaluar el desempe√±o del modelo en un conjunto de validaci√≥n. El objetivo es encontrar la combinaci√≥n √≥ptima que maximice la precisi√≥n del modelo en el conjunto de datos de prueba.\n",
    "\n",
    "Este proceso puede ser muy laborioso y a menudo se utilizan t√©cnicas como la b√∫squeda de cuadr√≠cula, la b√∫squeda aleatoria y la optimizaci√≥n bayesiana para encontrar la combinaci√≥n √≥ptima de hiperpar√°metros. Una vez que se ha encontrado la combinaci√≥n √≥ptima, se puede utilizar para entrenar el modelo final que se utilizar√° en la producci√≥n."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Datos y t√©cnica a utilizar**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu√≠ te presento algunos ejemplos de datos y t√©cnicas que se pueden utilizar para el ajuste de hiperpar√°metros en modelos de Machine Learning:\n",
    "\n",
    "**1. Datos:**\n",
    "\n",
    "Conjunto de datos de reconocimiento de im√°genes de d√≠gitos escritos a mano `(MNIST)`.\n",
    "\n",
    "**T√©cnica:** B√∫squeda de cuadr√≠cula para ajustar los hiperpar√°metros de un modelo de red neuronal convolucional, como el n√∫mero de capas ocultas, el n√∫mero de filtros en cada capa, el tama√±o del kernel, etc.\n",
    "\n",
    "**2. Datos:**\n",
    "\n",
    " Conjunto de datos de precios de viviendas.\n",
    "\n",
    "**T√©cnica:** B√∫squeda aleatoria para ajustar los hiperpar√°metros de un modelo de regresi√≥n, como el tama√±o del conjunto de entrenamiento, el n√∫mero de variables de entrada, el tipo de modelo (regresi√≥n lineal, regresi√≥n de bosque aleatorio, etc.), y otros hiperpar√°metros espec√≠ficos del modelo.\n",
    "\n",
    "**3. Datos:**\n",
    "\n",
    " Conjunto de datos de detecci√≥n de fraude de tarjetas de cr√©dito.\n",
    "\n",
    "**T√©cnica:** Optimizaci√≥n bayesiana para ajustar los hiperpar√°metros de un modelo de clasificaci√≥n, como el n√∫mero de √°rboles en un bosque aleatorio, el n√∫mero m√°ximo de profundidad, la tasa de aprendizaje, la regularizaci√≥n y otros hiperpar√°metros espec√≠ficos del modelo.\n",
    "\n",
    "Estos son solo algunos ejemplos de datos y t√©cnicas que se pueden utilizar para el ajuste de hiperpar√°metros en modelos de Machine Learning. La elecci√≥n de la t√©cnica y los hiperpar√°metros espec√≠ficos depender√° del problema y los datos que se est√©n utilizando."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explicaci√≥n de bosques aleatorios**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los bosques aleatorios son un tipo de modelo de Machine Learning que se utilizan para problemas de clasificaci√≥n y regresi√≥n. Son una combinaci√≥n de m√∫ltiples √°rboles de decisi√≥n, en los cuales cada √°rbol es entrenado con una muestra aleatoria de datos y una selecci√≥n aleatoria de caracter√≠sticas.\n",
    "\n",
    "La idea principal detr√°s de los bosques aleatorios es que la combinaci√≥n de m√∫ltiples modelos d√©biles y altamente correlacionados puede producir un modelo fuerte y robusto. Cada √°rbol en el bosque se entrena en una muestra aleatoria de los datos, lo que significa que cada √°rbol ver√° una parte diferente del conjunto de datos y ser√° entrenado con una perspectiva diferente. Adem√°s, cada √°rbol se entrena con una selecci√≥n aleatoria de caracter√≠sticas, lo que significa que cada √°rbol se enfoca en diferentes aspectos de los datos.\n",
    "\n",
    "A continuaci√≥n se muestra un ejemplo de c√≥digo en Python para implementar un modelo de bosque aleatorio utilizando el conjunto de datos de `Iris`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisi√≥n del modelo: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# cargar el conjunto de datos de iris\n",
    "iris = load_iris()\n",
    "\n",
    "# dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
    "\n",
    "# crear un modelo de bosque aleatorio con 100 √°rboles\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# ajustar el modelo a los datos de entrenamiento\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# evaluar la precisi√≥n del modelo en los datos de prueba\n",
    "print('Precisi√≥n del modelo:', rfc.score(X_test, y_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se utiliza la biblioteca `Scikit-Learn` para cargar el conjunto de datos de `Iris` y dividirlo en un conjunto de datos de entrenamiento y prueba. Luego se crea un modelo de bosque aleatorio con `100` √°rboles utilizando la clase ```RandomForestClassifier```. Se ajusta el modelo a los datos de entrenamiento y se eval√∫a la precisi√≥n del modelo en los datos de prueba. La precisi√≥n del modelo se imprime en la consola."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hiperpar√°metros de los bosques**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los hiperpar√°metros son valores que se establecen antes del entrenamiento de un modelo de Machine Learning y que influyen en su rendimiento. Para los bosques aleatorios, algunos ejemplos de hiperpar√°metros incluyen el n√∫mero de √°rboles en el bosque, la profundidad m√°xima de los √°rboles, el n√∫mero m√≠nimo de muestras requeridas para dividir un nodo, la fracci√≥n de caracter√≠sticas que se seleccionan al azar para cada √°rbol, entre otros.\n",
    "\n",
    "A continuaci√≥n, se muestra un ejemplo de c√≥digo en Python que utiliza la biblioteca `Scikit-Learn` para entrenar un modelo de bosque aleatorio en el conjunto de datos de `Iris`, ajustando algunos de sus hiperpar√°metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperpar√°metros: {'max_depth': 3, 'max_features': 'sqrt', 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Precisi√≥n del modelo: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# cargar el conjunto de datos de iris\n",
    "iris = load_iris()\n",
    "\n",
    "# dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
    "\n",
    "# crear un modelo de bosque aleatorio\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# definir una cuadr√≠cula de hiperpar√°metros para buscar\n",
    "param_grid = {'n_estimators': [50, 100, 200],\n",
    "              'max_depth': [3, 5, None],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'max_features': ['sqrt', 'log2']}\n",
    "\n",
    "# realizar la b√∫squeda de cuadr√≠cula para encontrar los mejores hiperpar√°metros\n",
    "grid_search = GridSearchCV(rfc, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# imprimir los mejores hiperpar√°metros y la precisi√≥n del modelo en los datos de prueba\n",
    "print('Mejores hiperpar√°metros:', grid_search.best_params_)\n",
    "print('Precisi√≥n del modelo:', grid_search.score(X_test, y_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se crea un modelo de bosque aleatorio sin establecer valores para algunos de sus hiperpar√°metros, para que puedan ser ajustados m√°s tarde mediante la b√∫squeda de cuadr√≠cula. Se define una cuadr√≠cula de hiperpar√°metros utilizando la clase ```GridSearchCV``` de Scikit-Learn, que realiza una b√∫squeda exhaustiva de combinaciones de hiperpar√°metros para encontrar los valores que producen el mejor rendimiento. En este caso, se ajustan los hiperpar√°metros ```n_estimators```, ```max_depth```, ```min_samples_split``` y ```max_features```. Luego, se ajusta el modelo a los datos de entrenamiento utilizando la funci√≥n ```fit``` de la clase ```GridSearchCV``` y se eval√∫a la precisi√≥n del modelo en los datos de prueba. Finalmente, se imprimen los mejores hiperpar√°metros encontrados y la precisi√≥n del modelo en los datos de prueba."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **B√∫squeda en Rejilla (GridSearch)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La b√∫squeda en rejilla, o `GridSearch`, es una t√©cnica de ajuste de hiperpar√°metros que implica la evaluaci√≥n exhaustiva de todas las combinaciones posibles de valores de hiperpar√°metros en un rango predefinido. A continuaci√≥n, se presenta un ejemplo de c√≥digo en Python que utiliza la biblioteca `Scikit-Learn` para realizar una b√∫squeda en rejilla en un modelo de regresi√≥n log√≠stica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperpar√°metros: {'C': 10, 'penalty': 'l1', 'solver': 'saga'}\n",
      "Precisi√≥n del modelo: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# cargar el conjunto de datos de iris\n",
    "iris = load_iris()\n",
    "\n",
    "# dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
    "\n",
    "# crear un modelo de regresi√≥n log√≠stica\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# definir una cuadr√≠cula de hiperpar√°metros para buscar\n",
    "param_grid = {'C': [0.1, 1, 10],\n",
    "              'penalty': ['l1', 'l2'],\n",
    "              'solver': ['liblinear', 'saga']}\n",
    "\n",
    "# realizar la b√∫squeda de cuadr√≠cula para encontrar los mejores hiperpar√°metros\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# imprimir los mejores hiperpar√°metros y la precisi√≥n del modelo en los datos de prueba\n",
    "print('Mejores hiperpar√°metros:', grid_search.best_params_)\n",
    "print('Precisi√≥n del modelo:', grid_search.score(X_test, y_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se define una cuadr√≠cula de hiperpar√°metros utilizando la clase ```GridSearchCV``` de Scikit-Learn. La cuadr√≠cula contiene diferentes valores para los hiperpar√°metros ```C```, ```penalty``` y ```solver``` del modelo de regresi√≥n log√≠stica. Luego, se realiza una b√∫squeda de cuadr√≠cula para evaluar todas las posibles combinaciones de hiperpar√°metros y encontrar los que producen el mejor rendimiento en t√©rminos de precisi√≥n en los datos de entrenamiento. La funci√≥n ```fit``` de la clase ```GridSearchCV``` ajusta el modelo a los datos de entrenamiento utilizando cada combinaci√≥n de valores de hiperpar√°metros y eval√∫a el rendimiento utilizando una validaci√≥n cruzada con 5 divisiones. Finalmente, se imprimen los mejores hiperpar√°metros encontrados y la precisi√≥n del modelo en los datos de prueba."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Informaci√≥n de la Rejilla**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Informaci√≥n de la Rejilla `(GridSearchCV)` es una t√©cnica de ajuste de hiperpar√°metros que utiliza una b√∫squeda exhaustiva sobre una grilla de valores posibles de hiperpar√°metros para encontrar la mejor combinaci√≥n de hiperpar√°metros para un modelo de machine learning. En lugar de ajustar manualmente los hiperpar√°metros, `GridSearchCV` prueba todas las posibles combinaciones de hiperpar√°metros y devuelve la mejor combinaci√≥n.\n",
    "\n",
    "Aqu√≠ hay un ejemplo de c√≥digo que muestra c√≥mo utilizar `GridSearchCV` con `SVM` para encontrar la mejor combinaci√≥n de valores de hiperpar√°metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperpar√°metros: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Precisi√≥n del modelo: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# cargar el conjunto de datos de iris\n",
    "iris = load_iris()\n",
    "\n",
    "# dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "\n",
    "# crear un modelo SVM\n",
    "svm = SVC()\n",
    "\n",
    "# definir una cuadr√≠cula de hiperpar√°metros para buscar\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
    "\n",
    "# realizar la b√∫squeda de cuadr√≠cula para encontrar los mejores hiperpar√°metros\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# imprimir los mejores hiperpar√°metros y la precisi√≥n del modelo en los datos de prueba\n",
    "print('Mejores hiperpar√°metros:', grid_search.best_params_)\n",
    "print('Precisi√≥n del modelo:', grid_search.score(X_test, y_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se utiliza ```GridSearchCV``` para buscar la mejor combinaci√≥n de valores de hiperpar√°metros para el modelo `SVM`. La cuadr√≠cula de hiperpar√°metros a probar se define en el diccionario ```param_grid```. ```GridSearchCV``` ajusta el modelo `SVM` a cada combinaci√≥n posible de valores de hiperpar√°metros y eval√∫a su rendimiento utilizando la validaci√≥n cruzada de 5 pliegues. Finalmente, imprime los mejores hiperpar√°metros encontrados y la precisi√≥n del modelo en los datos de prueba."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Validaci√≥n cruzada y Rejilla**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Validaci√≥n Cruzada `(Cross Validation)` es una t√©cnica utilizada para evaluar el rendimiento de un modelo de Machine Learning. La idea principal es dividir el conjunto de datos en subconjuntos de entrenamiento y prueba, entrenar el modelo en el subconjunto de entrenamiento y evaluar su rendimiento en el subconjunto de prueba. Esta t√©cnica es √∫til para evitar el sobreajuste y para obtener una mejor estimaci√≥n del rendimiento del modelo.\n",
    "\n",
    "La B√∫squeda en Rejilla `(GridSearchCV)` es una t√©cnica de ajuste de hiperpar√°metros que utiliza una b√∫squeda exhaustiva sobre una grilla de valores posibles de hiperpar√°metros para encontrar la mejor combinaci√≥n de hiperpar√°metros para un modelo de Machine Learning.\n",
    "\n",
    "Aqu√≠ hay un ejemplo de c√≥digo que muestra c√≥mo utilizar la Validaci√≥n Cruzada y la B√∫squeda en Rejilla con `SVM` para encontrar la mejor combinaci√≥n de valores de hiperpar√°metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperpar√°metros: {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Precisi√≥n del modelo en los datos de prueba: 0.9736842105263158\n",
      "Precisi√≥n media de la validaci√≥n cruzada: 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# cargar el conjunto de datos de iris\n",
    "iris = load_iris()\n",
    "\n",
    "# dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "\n",
    "# crear un modelo SVM\n",
    "svm = SVC()\n",
    "\n",
    "# definir una cuadr√≠cula de hiperpar√°metros para buscar\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
    "\n",
    "# realizar la b√∫squeda de cuadr√≠cula con validaci√≥n cruzada para encontrar los mejores hiperpar√°metros\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# imprimir los mejores hiperpar√°metros y la precisi√≥n del modelo en los datos de prueba\n",
    "print('Mejores hiperpar√°metros:', grid_search.best_params_)\n",
    "print('Precisi√≥n del modelo en los datos de prueba:', grid_search.score(X_test, y_test))\n",
    "\n",
    "# realizar la validaci√≥n cruzada para evaluar el rendimiento del modelo\n",
    "scores = cross_val_score(grid_search, iris.data, iris.target, cv=5)\n",
    "print('Precisi√≥n media de la validaci√≥n cruzada:', scores.mean())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se utiliza la Validaci√≥n Cruzada con B√∫squeda en Rejilla para encontrar la mejor combinaci√≥n de valores de hiperpar√°metros para el modelo `SVM`. La cuadr√≠cula de hiperpar√°metros a probar se define en el diccionario ```param_grid```. `GridSearchCV` ajusta el modelo `SVM` a cada combinaci√≥n posible de valores de hiperpar√°metros y eval√∫a su rendimiento utilizando la validaci√≥n cruzada de `5` pliegues. Finalmente, imprime los mejores hiperpar√°metros encontrados y la precisi√≥n del modelo en los datos de prueba, as√≠ como la precisi√≥n media de la validaci√≥n cruzada."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conjuntos de hiperpar√°metros explorados**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el contexto de la B√∫squeda en Rejilla `(GridSearchCV)`, los conjuntos de hiperpar√°metros explorados son todas las combinaciones de valores de hiperpar√°metros que se prueban para encontrar la mejor combinaci√≥n que produce el mejor rendimiento del modelo. Estos conjuntos pueden ser bastante grandes y pueden ser un desaf√≠o para la optimizaci√≥n del modelo.\n",
    "\n",
    "Aqu√≠ hay un ejemplo de c√≥digo que muestra c√≥mo utilizar la B√∫squeda en Rejilla con `SVM` para explorar conjuntos de hiperpar√°metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjuntos de hiperpar√°metros explorados: [{'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}, {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf'}, {'C': 0.1, 'gamma': 'auto', 'kernel': 'linear'}, {'C': 0.1, 'gamma': 'auto', 'kernel': 'rbf'}, {'C': 1, 'gamma': 'scale', 'kernel': 'linear'}, {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}, {'C': 1, 'gamma': 'auto', 'kernel': 'linear'}, {'C': 1, 'gamma': 'auto', 'kernel': 'rbf'}, {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}, {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}, {'C': 10, 'gamma': 'auto', 'kernel': 'linear'}, {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}]\n",
      "Rendimiento de cada conjunto de hiperpar√°metros: [0.95533597 0.82094862 0.95533597 0.94703557 0.97312253 0.96403162\n",
      " 0.97312253 0.96403162 0.97272727 0.97312253 0.97272727 0.97272727]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# cargar el conjunto de datos de iris\n",
    "iris = load_iris()\n",
    "\n",
    "# dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "\n",
    "# crear un modelo SVM\n",
    "svm = SVC()\n",
    "\n",
    "# definir una cuadr√≠cula de hiperpar√°metros para buscar\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
    "\n",
    "# realizar la b√∫squeda de cuadr√≠cula para encontrar los mejores hiperpar√°metros\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# imprimir los conjuntos de hiperpar√°metros explorados y su rendimiento\n",
    "print('Conjuntos de hiperpar√°metros explorados:', grid_search.cv_results_['params'])\n",
    "print('Rendimiento de cada conjunto de hiperpar√°metros:', grid_search.cv_results_['mean_test_score'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se utiliza la B√∫squeda en Rejilla con `SVM` para explorar conjuntos de hiperpar√°metros definidos en el diccionario ```param_grid```. `GridSearchCV` ajusta el modelo `SVM` a cada combinaci√≥n posible de valores de hiperpar√°metros y eval√∫a su rendimiento utilizando la validaci√≥n cruzada de `5` pliegues. Luego, imprime los conjuntos de hiperpar√°metros explorados y su rendimiento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ranking de los mejores modelos**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ranking de los mejores modelos es una lista ordenada de los modelos entrenados seg√∫n su rendimiento. Esta lista puede ser √∫til para seleccionar el modelo final para su uso en la producci√≥n. Los modelos se ordenan seg√∫n alguna m√©trica de rendimiento, como la precisi√≥n, la puntuaci√≥n `F1`, el `AUC`, etc.\n",
    "\n",
    "Aqu√≠ hay un ejemplo de c√≥digo que muestra c√≥mo utilizar `GridSearchCV` para entrenar varios modelos y ordenarlos seg√∫n su precisi√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "15 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.92924901        nan 0.96403162        nan 0.97272727]\n",
      "  warnings.warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Modelo: Logistic Regression, Precisi√≥n: 0.9736842105263158, Mejores Hiperpar√°metros: {'C': 10, 'penalty': 'l2'}\n",
      "2. Modelo: Decision Tree, Precisi√≥n: 0.9736842105263158, Mejores Hiperpar√°metros: {'max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# cargar el conjunto de datos de iris\n",
    "iris = load_iris()\n",
    "\n",
    "# dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "\n",
    "# definir una lista de modelos para entrenar\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Decision Tree', DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "# definir una cuadr√≠cula de hiperpar√°metros para buscar\n",
    "param_grid = {\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']},\n",
    "    'Decision Tree': {'max_depth': [3, 5, 7]}\n",
    "}\n",
    "\n",
    "# entrenar los modelos utilizando la b√∫squeda de cuadr√≠cula y la validaci√≥n cruzada de 5 pliegues\n",
    "results = {}\n",
    "for name, model in models:\n",
    "    grid_search = GridSearchCV(model, param_grid[name], cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    results[name] = {'best_params': grid_search.best_params_, 'score': grid_search.score(X_test, y_test)}\n",
    "\n",
    "# ordenar los modelos seg√∫n su precisi√≥n\n",
    "ranked_models = sorted(results.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "\n",
    "# imprimir el ranking de los modelos\n",
    "for i, (name, result) in enumerate(ranked_models):\n",
    "    print(f'{i+1}. Modelo: {name}, Precisi√≥n: {result[\"score\"]}, Mejores Hiperpar√°metros: {result[\"best_params\"]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se definen dos modelos diferentes para entrenar (regresi√≥n log√≠stica y √°rbol de decisi√≥n) y se define una cuadr√≠cula de hiperpar√°metros diferente para cada modelo. `GridSearchCV` se utiliza para entrenar los modelos utilizando la b√∫squeda de cuadr√≠cula y la validaci√≥n cruzada de 5 pliegues. Luego, los modelos se ordenan seg√∫n su precisi√≥n y se imprimen en orden de mejor a peor rendimiento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Los mejores hiperpar√°metros**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los mejores hiperpar√°metros son aquellos que producen el mejor rendimiento en el modelo de machine learning. En la b√∫squeda de hiperpar√°metros, se exploran diferentes combinaciones de hiperpar√°metros para encontrar los que producen el mejor resultado. El mejor conjunto de hiperpar√°metros se selecciona seg√∫n una m√©trica de evaluaci√≥n espec√≠fica, como la precisi√≥n, el `AUC`, la `F1-score`, entre otros.\n",
    "\n",
    "Aqu√≠ hay un ejemplo de c√≥mo obtener los mejores hiperpar√°metros utilizando la b√∫squeda en rejilla y la validaci√≥n cruzada en `Scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperpar√°metros: {'max_depth': None, 'max_features': 'auto', 'n_estimators': 100}\n",
      "Precisi√≥n del modelo: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# cargar el conjunto de datos de iris\n",
    "iris = load_iris()\n",
    "\n",
    "# dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
    "\n",
    "# crear un modelo de bosque aleatorio\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# definir una cuadr√≠cula de hiperpar√°metros para buscar\n",
    "param_grid = {'n_estimators': [50, 100, 200],\n",
    "              'max_depth': [None, 5, 10],\n",
    "              'max_features': ['auto', 'sqrt', 'log2']}\n",
    "\n",
    "# realizar la b√∫squeda de cuadr√≠cula para encontrar los mejores hiperpar√°metros\n",
    "grid_search = GridSearchCV(rfc, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# imprimir los mejores hiperpar√°metros y la precisi√≥n del modelo en los datos de prueba\n",
    "print('Mejores hiperpar√°metros:', grid_search.best_params_)\n",
    "print('Precisi√≥n del modelo:', grid_search.score(X_test, y_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, estamos buscando los mejores hiperpar√°metros para un modelo de bosque aleatorio. Hacemos uso de la b√∫squeda en rejilla para explorar diferentes combinaciones de hiperpar√°metros y usamos la validaci√≥n cruzada con 5 pliegues para evaluar el rendimiento de cada combinaci√≥n de hiperpar√°metros. Finalmente, imprimimos los mejores hiperpar√°metros encontrados y la precisi√≥n del modelo en los datos de prueba."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Usando el mejor modelo**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos obtenido los mejores hiperpar√°metros y hemos ajustado nuestro modelo usando la validaci√≥n cruzada, podemos utilizar el modelo resultante para hacer predicciones en nuevos datos. Aqu√≠ hay un ejemplo de c√≥mo hacer esto usando un modelo de regresi√≥n log√≠stica ajustado a trav√©s de la validaci√≥n cruzada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisi√≥n del modelo en los datos de prueba: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# cargar el conjunto de datos de iris\n",
    "iris = load_iris()\n",
    "\n",
    "# dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
    "\n",
    "# definir una cuadr√≠cula de hiperpar√°metros para buscar\n",
    "param_grid = {'C': [0.1, 1, 10],\n",
    "              'penalty': ['l1', 'l2'],\n",
    "              'solver': ['liblinear', 'saga']}\n",
    "\n",
    "# crear un modelo de regresi√≥n log√≠stica con los mejores hiperpar√°metros\n",
    "best_logreg = LogisticRegression(C=1, penalty='l1', solver='saga', random_state=42)\n",
    "\n",
    "# ajustar el modelo a los datos de entrenamiento\n",
    "best_logreg.fit(X_train, y_train)\n",
    "\n",
    "# hacer predicciones en los datos de prueba\n",
    "y_pred = best_logreg.predict(X_test)\n",
    "\n",
    "# evaluar la precisi√≥n del modelo en los datos de prueba\n",
    "accuracy = best_logreg.score(X_test, y_test)\n",
    "print('Precisi√≥n del modelo en los datos de prueba:', accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, despu√©s de ajustar el modelo con los mejores hiperpar√°metros encontrados mediante la validaci√≥n cruzada, utilizamos el m√©todo ```predict``` para hacer predicciones en los datos de prueba. Luego, evaluamos la precisi√≥n del modelo en los datos de prueba utilizando el m√©todo ```score```."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **B√∫squeda aleatoria (RandomizedSearch)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La b√∫squeda aleatoria `(RandomizedSearch)` es una t√©cnica de optimizaci√≥n de hiperpar√°metros que consiste en buscar los valores √≥ptimos de los par√°metros de un modelo de forma aleatoria en un rango de valores especificado. En lugar de probar todas las combinaciones posibles de par√°metros, lo que puede ser computacionalmente costoso, se seleccionan valores aleatorios dentro de un rango predefinido para cada par√°metro y se eval√∫a su desempe√±o en el modelo.\n",
    "\n",
    "Un ejemplo de c√≥mo se puede implementar `RandomizedSearch` utilizando la biblioteca de aprendizaje autom√°tico `Scikit-learn` en Python es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'min_samples_leaf': 13, 'min_samples_split': 12, 'n_estimators': 16}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define el modelo\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define el espacio de b√∫squeda de los hiperpar√°metros\n",
    "param_dist = {'n_estimators': randint(10, 100),\n",
    "              'max_depth': randint(1, 10),\n",
    "              'min_samples_split': randint(2, 20),\n",
    "              'min_samples_leaf': randint(1, 20)}\n",
    "\n",
    "# Ejecuta la b√∫squeda aleatoria\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
    "                                   n_iter=10, cv=5, n_jobs=-1)\n",
    "\n",
    "# Ajusta el modelo utilizando los mejores par√°metros encontrados\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Muestra los mejores par√°metros encontrados\n",
    "print(random_search.best_params_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se define un modelo `RandomForestClassifier` y se define un espacio de b√∫squeda de los hiperpar√°metros a trav√©s del diccionario `param_dist`. Los valores de los hiperpar√°metros se seleccionan de forma aleatoria dentro de los rangos especificados usando la funci√≥n `randint` de `scipy.stats`.\n",
    "\n",
    "Se ejecuta la b√∫squeda aleatoria utilizando `RandomizedSearchCV` de `Scikit-learn`, que realiza una validaci√≥n cruzada de `5` veces y prueba `10` combinaciones aleatorias de valores de hiperpar√°metros.\n",
    "\n",
    "Finalmente, se ajusta el modelo utilizando los mejores par√°metros encontrados y se imprimen los valores de los mejores par√°metros encontrados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hiperpar√°metros explorados aleatoriamente**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los hiperpar√°metros explorados aleatoriamente se refieren a los valores de los hiperpar√°metros que son seleccionados de forma aleatoria dentro de un rango predefinido durante el proceso de b√∫squeda aleatoria. Estos hiperpar√°metros son los que se eval√∫an para determinar cu√°l combinaci√≥n de valores da lugar al mejor desempe√±o del modelo.\n",
    "\n",
    "A continuaci√≥n, se presenta un ejemplo de c√≥digo que muestra c√≥mo se pueden definir los hiperpar√°metros a ser explorados aleatoriamente en la b√∫squeda de un modelo `RandomForestClassifier` utilizando la biblioteca de aprendizaje autom√°tico `Scikit-learn` en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puma/anaconda3/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 7, 'n_estimators': 46}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define el modelo\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define el espacio de b√∫squeda de los hiperpar√°metros\n",
    "param_dist = {'n_estimators': randint(10, 100),\n",
    "              'max_depth': randint(1, 10),\n",
    "              'min_samples_split': randint(2, 20),\n",
    "              'min_samples_leaf': randint(1, 20)}\n",
    "\n",
    "# Ejecuta la b√∫squeda aleatoria\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
    "                                   n_iter=10, cv=5, n_jobs=-1)\n",
    "\n",
    "# Ajusta el modelo utilizando los mejores par√°metros encontrados\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Muestra los mejores par√°metros encontrados\n",
    "print(random_search.best_params_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, los hiperpar√°metros explorados aleatoriamente son:\n",
    "\n",
    "* **n_estimators:** n√∫mero de √°rboles en el modelo `RandomForestClassifier`, seleccionado aleatoriamente entre 10 y 100.\n",
    "* **max_depth:** profundidad m√°xima de los √°rboles en el modelo `RandomForestClassifier`, seleccionado aleatoriamente entre 1 y 10.\n",
    "* **min_samples_split:** n√∫mero m√≠nimo de muestras requeridas para dividir un nodo interno en el modelo `RandomForestClassifier`, seleccionado aleatoriamente entre 2 y 20.\n",
    "* **min_samples_leaf:** n√∫mero m√≠nimo de muestras requeridas para ser una hoja en el modelo `RandomForestClassifier`, seleccionado aleatoriamente entre 1 y 20.\n",
    "\n",
    "Estos hiperpar√°metros son definidos en el diccionario ```param_dist```, que es pasado como par√°metro a la funci√≥n `RandomizedSearchCV`. Durante la b√∫squeda aleatoria, se seleccionan aleatoriamente 10 combinaciones de valores de los hiperpar√°metros definidos en ```param_dist```, y se eval√∫a el desempe√±o del modelo para cada combinaci√≥n. Al final, el mejor conjunto de valores de hiperpar√°metros se selecciona y se utiliza para ajustar el modelo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr√°s 15** | **Siguiente 17** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [üè†](../../README.md) | [‚è™](./15.DBSCAN_Clustering.ipynb)| [‚è©](./17.Bagging%20_Machine_learning.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
