{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **Siguiente 2** |\n",
    "|----------- |-------------- |\n",
    "| [üè†](../../README.md) | [‚è©](./2.Regresion.ipynb)|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Pre procesado de datos**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Datos categ√≥ricos**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos categ√≥ricos, tambi√©n conocidos como variables categ√≥ricas, son aquellos que representan caracter√≠sticas o cualidades que pueden ser clasificadas en diferentes categor√≠as o grupos. Estas categor√≠as pueden ser nominales o ordinales, dependiendo del tipo de informaci√≥n que se est√© representando.\n",
    "\n",
    "`Las variables categ√≥ricas nominales` no tienen un orden l√≥gico entre las diferentes categor√≠as, simplemente se usan para clasificar la informaci√≥n en diferentes grupos.\n",
    "\n",
    "**Ejemplos** de variables categ√≥ricas nominales son el g√©nero, la nacionalidad, la preferencia pol√≠tica o el color de ojos. Estas variables no pueden ser medidas o comparadas en t√©rminos de mayor o menor, simplemente se pueden contar la cantidad de individuos en cada categor√≠a.\n",
    "\n",
    "Por otro lado, `las variables categ√≥ricas ordinales` s√≠ tienen un orden l√≥gico entre las diferentes categor√≠as.\n",
    "\n",
    "**Ejemplos** de variables categ√≥ricas ordinales son el nivel educativo, la clasificaci√≥n socioecon√≥mica o el grado de satisfacci√≥n. En este caso, las diferentes categor√≠as se pueden ordenar en funci√≥n de un criterio l√≥gico, por ejemplo, el nivel educativo se puede ordenar de menor a mayor: primaria, secundaria, universitaria, posgrado, etc.\n",
    "\n",
    "En resumen, los datos categ√≥ricos son aquellos que representan caracter√≠sticas o cualidades que se pueden clasificar en diferentes categor√≠as o grupos, y que pueden ser nominales o ordinales dependiendo del tipo de informaci√≥n que se est√© representando."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem√°tica de los datos categ√≥ricos**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos categ√≥ricos son aquellos que representan una variable que toma valores de una lista finita y discreta de categor√≠as o etiquetas. Aunque son comunes en muchos campos de estudio, los datos categ√≥ricos pueden presentar varias problem√°ticas que dificultan su an√°lisis. Algunas de estas problem√°ticas son:\n",
    "\n",
    "1. **Escalas de medici√≥n:**\n",
    " Las variables categ√≥ricas no tienen una escala de medici√≥n clara, lo que dificulta la comparaci√≥n entre ellas. **Por ejemplo**, no se puede decir que una categor√≠a es el doble o la mitad de otra categor√≠a.\n",
    "\n",
    "2. **Representaci√≥n num√©rica:**\n",
    " A menudo, los datos categ√≥ricos se representan mediante n√∫meros, pero estos n√∫meros no tienen ning√∫n significado num√©rico real. **Por ejemplo**, si tenemos una variable que representa colores, los n√∫meros asignados a cada color no tienen un valor num√©rico real.\n",
    "\n",
    "3. **An√°lisis estad√≠stico:**\n",
    " Muchos m√©todos estad√≠sticos requieren que los datos sean num√©ricos, por lo que los datos categ√≥ricos pueden presentar dificultades en el an√°lisis. A menudo se utilizan t√©cnicas especiales, como modelos de regresi√≥n log√≠stica o an√°lisis de contingencia, para analizar datos categ√≥ricos.\n",
    "\n",
    "4. **Sesgo de la muestra:** Los datos categ√≥ricos pueden estar sesgados si la muestra no es representativa de la poblaci√≥n de inter√©s. Por ejemplo, si se recopilan datos sobre preferencias de color solo de personas que trabajan en una empresa de moda, es posible que los resultados no sean generalizables a la poblaci√≥n en general.\n",
    "\n",
    "5. **Categor√≠as incompletas:**\n",
    " A veces, las categor√≠as en una variable categ√≥rica pueden ser incompletas o ambiguas, lo que dificulta su an√°lisis. Por **ejemplo**, si se tiene una variable que representa la edad, pero algunas categor√≠as est√°n ausentes o no son claras, puede ser dif√≠cil interpretar los resultados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Variables](../imagenes%20Machine_Learning/Variables.webp \"Variables\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tipos de datos categ√≥ricos**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen diferentes tipos de datos categ√≥ricos, algunos de los cuales se detallan a continuaci√≥n:\n",
    "\n",
    "1. **Nominales:**\n",
    " Los datos nominales son aquellos que representan categor√≠as sin ning√∫n orden o jerarqu√≠a espec√≠fica. Por **ejemplo**, el g√©nero, el pa√≠s de origen, la marca de un producto, etc.\n",
    "\n",
    "2. **Ordinales:**\n",
    " Los datos ordinales son aquellos que representan categor√≠as con un orden o jerarqu√≠a espec√≠fica. Por **ejemplo**, las calificaciones escolares (A, B, C, D, E), el nivel socioecon√≥mico (bajo, medio, alto), la opini√≥n sobre algo (muy bueno, bueno, regular, malo, muy malo), etc.\n",
    "\n",
    "3. **Binarios:**\n",
    " Los datos binarios son aquellos que representan solo dos categor√≠as posibles. Por **ejemplo**, s√≠ o no, verdadero o falso, hombre o mujer, etc.\n",
    "\n",
    "4. **Polit√≥micos:**\n",
    " Los datos polit√≥micos son aquellos que representan m√°s de dos categor√≠as posibles. Por **ejemplo**, el color de un objeto, las preferencias de comida (dulce, salado, amargo, √°cido), el nivel de educaci√≥n (primaria, secundaria, universitaria), etc.\n",
    "\n",
    "Es importante tener en cuenta que la elecci√≥n del tipo de dato categ√≥rico depende del contexto en el que se est√° trabajando y de la naturaleza de la variable que se est√° midiendo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejemplos de datos ordinales**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que tenemos un dataframe que representa la informaci√≥n de un conjunto de estudiantes y sus calificaciones en diferentes asignaturas. Algunos ejemplos de datos ordinales que podr√≠an estar presentes en este dataframe son:\n",
    "\n",
    "* La calificaci√≥n en cada asignatura, que se puede representar mediante una escala ordinal, como A, B, C, D, E, donde A es la calificaci√≥n m√°s alta y E es la calificaci√≥n m√°s baja.\n",
    "\n",
    "* El nivel de dificultad de cada asignatura, que tambi√©n se puede representar mediante una escala ordinal, como alta, media, baja, donde alta representa el nivel m√°s dif√≠cil y baja el nivel m√°s f√°cil.\n",
    "\n",
    "* El grado de satisfacci√≥n de los estudiantes con cada asignatura, que se puede representar mediante una escala ordinal, como muy satisfecho, satisfecho, indiferente, insatisfecho, muy insatisfecho, donde muy satisfecho representa el nivel m√°s alto de satisfacci√≥n y muy insatisfecho el nivel m√°s bajo.\n",
    "\n",
    "En todos estos casos, los datos tienen un orden o jerarqu√≠a espec√≠fica que permite su clasificaci√≥n y an√°lisis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ilustrar c√≥mo se pueden representar datos ordinales en un dataframe utilizando Python, podemos crear un peque√±o ejemplo utilizando la librer√≠a `Pandas`.\n",
    "\n",
    "Supongamos que tenemos un dataframe que representa la informaci√≥n de un conjunto de estudiantes y sus calificaciones en tres asignaturas (Matem√°ticas, F√≠sica y Qu√≠mica), y que las calificaciones se han registrado utilizando la escala ordinal A, B, C, D, E. Podr√≠amos crear el dataframe de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Estudiante Matem√°ticas F√≠sica Qu√≠mica\n",
      "0       Juan           A      B       C\n",
      "1      Mar√≠a           B      C       C\n",
      "2      Pedro           C      D       D\n",
      "3        Ana           C      E       D\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Estudiante': ['Juan', 'Mar√≠a', 'Pedro', 'Ana'],\n",
    "    'Matem√°ticas': ['A', 'B', 'C', 'C'],\n",
    "    'F√≠sica': ['B', 'C', 'D', 'E'],\n",
    "    'Qu√≠mica': ['C', 'C', 'D', 'D']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, las columnas de calificaciones para cada asignatura representan datos ordinales, ya que cada calificaci√≥n tiene un orden espec√≠fico. Podemos analizar estos datos de diferentes maneras, como calcular el promedio de calificaciones de cada estudiante o determinar la asignatura con la calificaci√≥n m√°s alta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Contexto de los datos**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El contexto de los datos se refiere al conjunto de circunstancias y condiciones en las que se recopilaron los datos, incluyendo la fuente de los datos, el prop√≥sito de la recopilaci√≥n de los datos, el m√©todo utilizado para recopilar los datos y cualquier otra informaci√≥n relevante.\n",
    "\n",
    "El contexto es importante porque puede afectar la validez y la fiabilidad de los datos, y por lo tanto, la interpretaci√≥n y los resultados que se pueden obtener a partir de ellos. Algunos factores importantes a considerar en el contexto de los datos son:\n",
    "\n",
    "* **La fuente de los datos:**\n",
    " ¬øQui√©n recopil√≥ los datos? ¬øSon datos de una fuente confiable y verificada?\n",
    "\n",
    "* **El prop√≥sito de la recopilaci√≥n de los datos:**\n",
    " ¬øCu√°l fue el objetivo principal de la recopilaci√≥n de los datos? ¬øSe recopilaron los datos para responder a una pregunta espec√≠fica o para explorar un tema m√°s amplio?\n",
    "\n",
    "* **El m√©todo utilizado para recopilar los datos:**\n",
    " ¬øSe utilizaron m√©todos adecuados y v√°lidos para recopilar los datos? ¬øSe consideraron los posibles sesgos en el m√©todo de recopilaci√≥n de los datos?\n",
    "\n",
    "* **La naturaleza de los datos:**\n",
    " ¬øSon los datos categ√≥ricos o num√©ricos? ¬øCu√°l es la escala de medida utilizada para los datos?\n",
    "\n",
    "* **El periodo de tiempo en que se recopilaron los datos:**\n",
    " ¬øLos datos son actuales o se recopilaron en el pasado? ¬øEs importante considerar los cambios en el contexto que pueden haber ocurrido desde que se recopilaron los datos?\n",
    "\n",
    "En resumen, el contexto de los datos es esencial para interpretarlos de manera adecuada y utilizarlos de manera efectiva en la toma de decisiones o en la investigaci√≥n."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Codificaci√≥n ordinal (Ordinal Encoder)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La codificaci√≥n ordinal, tambi√©n conocida como Ordinal Encoder, es una t√©cnica de preprocesamiento de datos que se utiliza para convertir variables categ√≥ricas ordinales en valores num√©ricos ordenados. Esto se hace asignando un n√∫mero entero √∫nico a cada categor√≠a, basado en su posici√≥n en la escala ordinal.\n",
    "\n",
    "Por **ejemplo**, supongamos que tenemos una columna llamada `\"Nivel de educaci√≥n\"` con las siguientes categor√≠as ordinales: `\"Primaria\"`, `\"Secundaria\"`, `\"Bachillerato\"`, `\"T√©cnico\"`, `\"Profesional\"`, `\"Maestr√≠a\"` y `\"Doctorado\"`.\n",
    " La codificaci√≥n ordinal asignar√≠a los siguientes n√∫meros enteros a cada categor√≠a: 1 para `\"Primaria\"`, 2 para `\"Secundaria\"`, 3 para `\"Bachillerato\"`, 4 para `\"T√©cnico\"`, 5 para `\"Profesional\"`, 6 para `\"Maestr√≠a\"` y 7 para `\"Doctorado\"`.\n",
    "\n",
    "A continuaci√≥n se presenta un ejemplo de c√≥mo utilizar la codificaci√≥n ordinal en Python con la biblioteca `Scikit-Learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Nivel de educaci√≥n\n",
      "0                 3.0\n",
      "1                 5.0\n",
      "2                 0.0\n",
      "3                 6.0\n",
      "4                 4.0\n",
      "5                 2.0\n",
      "6                 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Creamos un dataframe con datos categ√≥ricos ordinales\n",
    "df = pd.DataFrame({'Nivel de educaci√≥n': ['Primaria', 'Secundaria', 'Bachillerato', 'T√©cnico', 'Profesional', 'Maestr√≠a', 'Doctorado']})\n",
    "\n",
    "# Instanciamos el codificador ordinal\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# Ajustamos el codificador a los datos del dataframe\n",
    "encoder.fit(df)\n",
    "\n",
    "# Transformamos los datos categ√≥ricos ordinales en valores num√©ricos ordenados\n",
    "transformed_data = encoder.transform(df)\n",
    "\n",
    "# Creamos un nuevo dataframe con los datos transformados\n",
    "transformed_df = pd.DataFrame(transformed_data, columns=df.columns)\n",
    "\n",
    "print(transformed_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, el codificador ordinal convierte la columna `\"Nivel de educaci√≥n\"` en valores num√©ricos ordenados del `0` al `6`, basado en la posici√≥n de cada categor√≠a en la escala ordinal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La codificaci√≥n ordinal, tambi√©n conocida como `Ordinal Encoder`, es una t√©cnica utilizada en el campo del aprendizaje autom√°tico y la miner√≠a de datos para convertir variables categ√≥ricas en valores num√©ricos ordenados. Esta codificaci√≥n asigna a cada categor√≠a un n√∫mero entero √∫nico seg√∫n su orden o jerarqu√≠a en los datos.\n",
    "\n",
    "La utilidad principal del Ordinal Encoder radica en el manejo de variables categ√≥ricas en algoritmos de aprendizaje autom√°tico, ya que muchos modelos requieren que los datos de entrada sean num√©ricos. Sin embargo, simplemente asignar valores num√©ricos arbitrarios a las categor√≠as puede llevar a malas interpretaciones por parte del modelo, ya que no se tiene en cuenta el orden o la jerarqu√≠a de las categor√≠as.\n",
    "\n",
    "A continuaci√≥n, te mostrar√© un ejemplo detallado de c√≥mo se utiliza el Ordinal Encoder en un dataframe. Supongamos que tenemos un dataframe que contiene informaci√≥n sobre diferentes tipos de frutas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Fruta\n",
      "0  Manzana\n",
      "1  Pl√°tano\n",
      "2     Pera\n",
      "3  Manzana\n",
      "4  Naranja\n",
      "5     Pera\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Crear el dataframe de ejemplo\n",
    "df = pd.DataFrame({\n",
    "    'Fruta': ['Manzana', 'Pl√°tano', 'Pera', 'Manzana', 'Naranja', 'Pera']\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, utilizaremos el Ordinal Encoder para codificar la columna \"`Fruta`\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Fruta  Fruta_encoded\n",
      "0  Manzana            0.0\n",
      "1  Pl√°tano            3.0\n",
      "2     Pera            2.0\n",
      "3  Manzana            0.0\n",
      "4  Naranja            1.0\n",
      "5     Pera            2.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Crear una instancia de OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "df['Fruta_encoded'] = encoder.fit_transform(df[['Fruta']])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que la columna \"`Fruta_encoded`\" contiene los valores num√©ricos codificados para cada tipo de fruta. La codificaci√≥n asigna los n√∫meros en funci√≥n del orden alfab√©tico de las categor√≠as, asignando el n√∫mero `0` a \"`Manzana`\", el n√∫mero `1` a \"`Pera`\", el n√∫mero `2` a \"`Pl√°tano`\" y el n√∫mero `3` a \"`Naranja`\".\n",
    "\n",
    "Esta codificaci√≥n permite al modelo capturar la relaci√≥n de orden entre las categor√≠as, lo que puede ser √∫til en casos donde el orden tiene un significado importante, como en el caso de variables categ√≥ricas ordinales como \"`tama√±o`\" (`peque√±o`, `mediano`, `grande`) o \"`rango de edad`\" (`joven`, `adulto`, `anciano`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Codificaci√≥n One-Hot (One-Hot Encoder)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La codificaci√≥n `One-Hot` (tambi√©n conocida como One-Hot Encoder) es una t√©cnica de preprocesamiento de datos que se utiliza para convertir variables categ√≥ricas en variables num√©ricas binarias. Cada categor√≠a de la variable original se convierte en una nueva columna binaria, y se asigna un valor de `1` si la instancia pertenece a esa categor√≠a, y `0` si no lo hace.\n",
    "\n",
    "Por ejemplo, si tenemos una columna `\"Color\"` con tres categor√≠as: `\"Rojo\"`, `\"Verde\"` y `\"Azul\"`, la codificaci√≥n `One-Hot` crear√≠a tres nuevas columnas: `\"Rojo\"`, `\"Verde\"` y `\"Azul\"`. Si una instancia tiene el color `\"Rojo\"`, su columna correspondiente tendr√° un valor de `1` y las otras dos columnas tendr√°n un valor de `0`.\n",
    "\n",
    "A continuaci√≥n, se presenta un ejemplo de c√≥mo utilizar la codificaci√≥n `One-Hot` en Python con la biblioteca `Scikit-Learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Color_Azul  Color_Rojo  Color_Verde\n",
      "0         0.0         1.0          0.0\n",
      "1         0.0         0.0          1.0\n",
      "2         1.0         0.0          0.0\n",
      "3         0.0         1.0          0.0\n",
      "4         0.0         1.0          0.0\n",
      "5         0.0         0.0          1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Creamos un dataframe con datos categ√≥ricos\n",
    "df = pd.DataFrame({'Color': ['Rojo', 'Verde', 'Azul', 'Rojo', 'Rojo', 'Verde']})\n",
    "\n",
    "# Instanciamos el codificador One-Hot\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Ajustamos el codificador a los datos del dataframe\n",
    "encoder.fit(df)\n",
    "\n",
    "# Transformamos los datos categ√≥ricos en valores num√©ricos binarios\n",
    "transformed_data = encoder.transform(df).toarray()\n",
    "\n",
    "# Obtenemos los nombres de las nuevas columnas\n",
    "feature_names = encoder.get_feature_names_out(input_features=['Color'])\n",
    "\n",
    "# Creamos un nuevo dataframe con los datos transformados\n",
    "transformed_df = pd.DataFrame(transformed_data, columns=feature_names)\n",
    "\n",
    "print(transformed_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, el codificador `One-Hot` convierte la columna `\"Color\"` en tres nuevas columnas binarias `\"x0_Azul\"`, `\"x0_Rojo\"` y `\"x0_Verde\"`. Cada columna representa una categor√≠a √∫nica y se asigna un valor de `1` si la instancia pertenece a esa categor√≠a y `0` si no lo hace."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La codificaci√≥n `One-Hot`, tambi√©n conocida como `One-Hot Encoder`, es una t√©cnica utilizada en el campo del aprendizaje autom√°tico y la miner√≠a de datos para convertir variables categ√≥ricas en vectores binarios. Esta codificaci√≥n crea nuevas columnas binarias para cada categor√≠a presente en la variable original y asigna un valor de `1` a la columna correspondiente a la categor√≠a presente en cada fila, mientras que las dem√°s columnas se rellenan con valores de `0`.\n",
    "\n",
    "La utilidad principal del `One-Hot Encoder` radica en el manejo de variables categ√≥ricas en algoritmos de aprendizaje autom√°tico, ya que muchos modelos requieren que los datos de entrada sean num√©ricos. La codificaci√≥n One-Hot garantiza que no se establezca un orden o jerarqu√≠a entre las categor√≠as, lo que puede ser √∫til cuando las categor√≠as son nominalmente distintas y no tienen un orden inherente.\n",
    "\n",
    "A continuaci√≥n, te mostrar√© un ejemplo detallado de c√≥mo se utiliza el One-Hot Encoder en un dataframe. Supongamos que tenemos un dataframe que contiene informaci√≥n sobre diferentes tipos de frutas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Fruta\n",
      "0  Manzana\n",
      "1  Pl√°tano\n",
      "2     Pera\n",
      "3  Manzana\n",
      "4  Naranja\n",
      "5     Pera\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Crear el dataframe de ejemplo\n",
    "df = pd.DataFrame({\n",
    "    'Fruta': ['Manzana', 'Pl√°tano', 'Pera', 'Manzana', 'Naranja', 'Pera']\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, utilizaremos el One-Hot Encoder para codificar la columna \"`Fruta`\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Fruta  Manzana  Naranja  Pera  Pl√°tano\n",
      "0  Manzana      1.0      0.0   0.0      0.0\n",
      "1  Pl√°tano      0.0      0.0   0.0      1.0\n",
      "2     Pera      0.0      0.0   1.0      0.0\n",
      "3  Manzana      1.0      0.0   0.0      0.0\n",
      "4  Naranja      0.0      1.0   0.0      0.0\n",
      "5     Pera      0.0      0.0   1.0      0.0\n"
     ]
    }
   ],
   "source": [
    "# Crear una instancia de OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Ajustar y transformar los datos\n",
    "encoded_data = encoder.fit_transform(df[['Fruta']]).toarray()\n",
    "\n",
    "# Obtener los nombres de las categor√≠as\n",
    "categories = encoder.categories_[0]\n",
    "\n",
    "# Crear un nuevo dataframe con las columnas codificadas\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=categories)\n",
    "\n",
    "# Concatenar los dataframes originales y codificados\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que se han creado nuevas columnas binarias para cada categor√≠a presente en la columna \"`Fruta`\". Cada columna representa una categor√≠a distinta y tiene valores de `1` o `0`, dependiendo de si esa categor√≠a est√° presente o no en cada fila.\n",
    "\n",
    "Esta codificaci√≥n es √∫til para evitar establecer un orden entre las categor√≠as y permitir que los modelos capturen las relaciones entre las diferentes categor√≠as de manera adecuada. Tambi√©n ayuda a evitar el problema de asignar pesos arbitrarios a las categor√≠as, lo que podr√≠a distorsionar los resultados del modelo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparaci√≥n de las codificaciones v√≠a un clasificador**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, compararemos el desempe√±o de la codificaci√≥n ordinal y `one-hot` encoding en un clasificador de regresi√≥n log√≠stica utilizando el conjunto de datos `Iris`. Primero, cargamos el conjunto de datos y separamos las caracter√≠sticas `(X)` y las etiquetas `(y)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar el conjunto de datos Iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Separar caracter√≠sticas y etiquetas\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, creamos dos transformadores utilizando el codificador ordinal y `one-hot encoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Definir las caracter√≠sticas categ√≥ricas y num√©ricas\n",
    "categorical_cols = [3]\n",
    "numeric_cols = [0, 1, 2]\n",
    "\n",
    "# Crear un transformador con el codificador ordinal\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "ordinal_transformer = Pipeline(steps=[('ordinal_encoder', ordinal_encoder)])\n",
    "preprocessor_ordinal = ColumnTransformer(transformers=[\n",
    "    ('ordinal', ordinal_transformer, categorical_cols),\n",
    "    ('numeric', 'passthrough', numeric_cols)\n",
    "])\n",
    "\n",
    "# Crear un transformador con el codificador one-hot\n",
    "onehot_encoder = OneHotEncoder()\n",
    "onehot_transformer = Pipeline(steps=[('onehot_encoder', onehot_encoder)])\n",
    "preprocessor_onehot = ColumnTransformer(transformers=[\n",
    "    ('onehot', onehot_transformer, categorical_cols),\n",
    "    ('numeric', 'passthrough', numeric_cols)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, creamos dos clasificadores de regresi√≥n log√≠stica, uno para cada transformador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un clasificador de regresi√≥n log√≠stica con el transformador ordinal\n",
    "clf_ordinal = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_ordinal),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Crear un clasificador de regresi√≥n log√≠stica con el transformador one-hot\n",
    "clf_onehot = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_onehot),\n",
    "    ('classifier', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos ambos clasificadores con los datos de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/puma/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;onehot&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;onehot_encoder&#x27;,\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  [3]),\n",
       "                                                 (&#x27;numeric&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [0, 1, 2])])),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;onehot&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;onehot_encoder&#x27;,\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  [3]),\n",
       "                                                 (&#x27;numeric&#x27;, &#x27;passthrough&#x27;,\n",
       "                                                  [0, 1, 2])])),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;onehot&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;onehot_encoder&#x27;,\n",
       "                                                  OneHotEncoder())]),\n",
       "                                 [3]),\n",
       "                                (&#x27;numeric&#x27;, &#x27;passthrough&#x27;, [0, 1, 2])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">onehot</label><div class=\"sk-toggleable__content\"><pre>[3]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numeric</label><div class=\"sk-toggleable__content\"><pre>[0, 1, 2]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('onehot',\n",
       "                                                  Pipeline(steps=[('onehot_encoder',\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  [3]),\n",
       "                                                 ('numeric', 'passthrough',\n",
       "                                                  [0, 1, 2])])),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el clasificador con el transformador ordinal\n",
    "clf_ordinal.fit(X_train, y_train)\n",
    "\n",
    "# Entrenar el clasificador con el transformador one-hot\n",
    "clf_onehot.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, evaluamos el desempe√±o de ambos clasificadores en los datos de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy con codificaci√≥n ordinal: 1.00\n",
      "Accuracy con codificaci√≥n one-hot: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Hacer predicciones con el clasificador con el transformador ordinal\n",
    "y_pred_ordinal = clf_ordinal.predict(X_test)\n",
    "accuracy_ordinal = accuracy_score(y_test, y_pred_ordinal)\n",
    "\n",
    "# Hacer predicciones con el clasificador con el transformador one-hot\n",
    "y_pred_onehot = clf_onehot.predict(X_test)\n",
    "accuracy_onehot = accuracy_score(y_test, y_pred_onehot)\n",
    "\n",
    "# Imprimir el desempe√±o de ambos clasificadores\n",
    "print(f'Accuracy con codificaci√≥n ordinal: {accuracy_ordinal:.2f}')\n",
    "print(f'Accuracy con codificaci√≥n one-hot: {accuracy_onehot:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La comparaci√≥n de codificaciones a trav√©s de un clasificador es una t√©cnica utilizada para evaluar el rendimiento de diferentes m√©todos de codificaci√≥n en un problema de aprendizaje autom√°tico. Consiste en aplicar distintas t√©cnicas de codificaci√≥n a las variables categ√≥ricas de un conjunto de datos, y luego entrenar un clasificador utilizando esos datos codificados. Luego, se compara el rendimiento del clasificador en t√©rminos de precisi√≥n u otras m√©tricas de evaluaci√≥n para determinar cu√°l m√©todo de codificaci√≥n es m√°s efectivo para ese problema en particular.\n",
    "\n",
    "La utilidad de esta comparaci√≥n radica en seleccionar la mejor t√©cnica de codificaci√≥n para mejorar el rendimiento del modelo de aprendizaje autom√°tico. Diferentes t√©cnicas de codificaci√≥n pueden tener un impacto significativo en la precisi√≥n y capacidad predictiva de un modelo, ya que pueden afectar la representaci√≥n de los datos y la interpretaci√≥n de las variables categ√≥ricas por parte del algoritmo de aprendizaje autom√°tico.\n",
    "\n",
    "A continuaci√≥n, te mostrar√© un ejemplo detallado de c√≥mo realizar una comparaci√≥n de codificaciones utilizando un clasificador en un dataframe. Supongamos que tenemos un dataframe que contiene informaci√≥n sobre los estudiantes y su desempe√±o en diferentes asignaturas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  G√©nero   Asignatura Calificaci√≥n\n",
      "0      M  Matem√°ticas     Aprobado\n",
      "1      F       Ingl√©s    Reprobado\n",
      "2      M     Historia     Aprobado\n",
      "3      F  Matem√°ticas     Aprobado\n",
      "4      F       Ingl√©s     Aprobado\n",
      "5      M     Historia    Reprobado\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Crear el dataframe de ejemplo\n",
    "df = pd.DataFrame({\n",
    "    'G√©nero': ['M', 'F', 'M', 'F', 'F', 'M'],\n",
    "    'Asignatura': ['Matem√°ticas', 'Ingl√©s', 'Historia', 'Matem√°ticas', 'Ingl√©s', 'Historia'],\n",
    "    'Calificaci√≥n': ['Aprobado', 'Reprobado', 'Aprobado', 'Aprobado', 'Aprobado', 'Reprobado']\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, tenemos dos variables categ√≥ricas: \"`G√©nero`\" y \"`Asignatura`\". Vamos a comparar dos m√©todos de codificaci√≥n: `Label Encoder` y `One-Hot Encoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisi√≥n utilizando Label Encoder: 0.5\n",
      "Precisi√≥n utilizando One-Hot Encoder: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en caracter√≠sticas (X) y variable objetivo (y)\n",
    "X = df[['G√©nero', 'Asignatura']]\n",
    "y = df['Calificaci√≥n']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# M√©todo 1: Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "X_train_label_encoded = X_train.copy()\n",
    "X_test_label_encoded = X_test.copy()\n",
    "for col in X_train.columns:\n",
    "    X_train_label_encoded[col] = label_encoder.fit_transform(X_train[col])\n",
    "    X_test_label_encoded[col] = label_encoder.transform(X_test[col])\n",
    "\n",
    "# M√©todo 2: One-Hot Encoder\n",
    "onehot_encoder = OneHotEncoder()\n",
    "X_train_onehot_encoded = onehot_encoder.fit_transform(X_train).toarray()\n",
    "X_test_onehot_encoded = onehot_encoder.transform(X_test).toarray()\n",
    "\n",
    "# Entrenar un clasificador utilizando el m√©todo Label Encoder\n",
    "classifier_label_encoded = LogisticRegression()\n",
    "classifier_label_encoded.fit(X_train_label_encoded, y_train)\n",
    "y_pred_label_encoded = classifier_label_encoded.predict(X_test_label_encoded)\n",
    "accuracy_label_encoded = accuracy_score(y_test, y_pred_label_encoded)\n",
    "\n",
    "# Entrenar un clasificador utilizando el m√©todo One-Hot Encoder\n",
    "classifier_onehot_encoded = LogisticRegression()\n",
    "classifier_onehot_encoded.fit(X_train_onehot_encoded, y_train)\n",
    "y_pred_onehot_encoded = classifier_onehot_encoded.predict(X_test_onehot_encoded)\n",
    "accuracy_onehot_encoded = accuracy_score(y_test, y_pred_onehot_encoded)\n",
    "\n",
    "print('Precisi√≥n utilizando Label Encoder:', accuracy_label_encoded)\n",
    "print('Precisi√≥n utilizando One-Hot Encoder:', accuracy_onehot_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos aplicado dos m√©todos de codificaci√≥n diferentes (`Label Encoder` y `One-Hot Encoder`) a las variables categ√≥ricas \"`G√©nero`\" y \"`Asignatura`\". Luego, hemos entrenado un clasificador de regresi√≥n log√≠stica utilizando los datos codificados y hemos calculado la precisi√≥n del modelo en el conjunto de prueba.\n",
    "\n",
    "En este caso, ambos m√©todos de codificaci√≥n han obtenido la misma precisi√≥n de `0.5`, lo que indica que no hay una diferencia significativa en el rendimiento del clasificador entre los dos m√©todos de codificaci√≥n para este problema particular. Sin embargo, en otros casos, es posible que uno de los m√©todos de codificaci√≥n sea m√°s efectivo que el otro en t√©rminos de rendimiento del modelo.\n",
    "\n",
    "La comparaci√≥n de las codificaciones mediante un clasificador nos permite evaluar y seleccionar la t√©cnica de codificaci√≥n m√°s adecuada para mejorar el rendimiento del modelo de aprendizaje autom√°tico."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **C√≥mo dividir el data set en entrenamiento y test**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el aprendizaje autom√°tico, es com√∫n dividir el conjunto de datos en dos subconjuntos: el conjunto de entrenamiento y el conjunto de prueba. El conjunto de entrenamiento se utiliza para ajustar el modelo de aprendizaje autom√°tico, mientras que el conjunto de prueba se utiliza para evaluar el rendimiento del modelo.\n",
    "\n",
    "Aqu√≠ te explicar√© c√≥mo dividir un conjunto de datos en Python utilizando la librer√≠a `Scikit-learn`.\n",
    "\n",
    "Primero, necesitamos cargar el conjunto de datos. Para este ejemplo, utilizaremos el conjunto de datos `iris` que ya viene incluido en `Scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, vamos a dividir los datos en un conjunto de entrenamiento y un conjunto de prueba. `Scikit-learn` proporciona la funci√≥n `train_test_split` para dividir los datos de manera aleatoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu√≠, `iris.data` son los datos de entrada y `iris.target` son las etiquetas de clase correspondientes. `test_size` es el porcentaje del conjunto de datos que se utilizar√° para el conjunto de prueba, que en este caso es del `30%`. `random_state` es una semilla aleatoria para asegurarnos de que obtenemos la misma divisi√≥n de datos cada vez que ejecutamos el c√≥digo.\n",
    "\n",
    "Ahora tenemos cuatro conjuntos de datos: `X_train`, `X_test`, `y_train`, y `y_test`. `X_train` y `y_train` se utilizan para entrenar el modelo, mientras que `X_test` y `y_test` se utilizan para evaluar el rendimiento del modelo.\n",
    "\n",
    "Finalmente, podemos verificar el tama√±o de los conjuntos de datos de entrenamiento y prueba utilizando la funci√≥n `shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (105, 4)\n",
      "y_train shape: (105,)\n",
      "X_test shape: (45, 4)\n",
      "y_test shape: (45,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, la funci√≥n `train_test_split` nos permite dividir un conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba de manera aleatoria en Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![C√≥mo dividir el data set en entrenamiento y test](../imagenes%20Machine_Learning/dataset%20_entrenamiento_test.webp \"C√≥mo dividir el data set en entrenamiento y test\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividir el `dataset` en conjuntos de entrenamiento y prueba es una pr√°ctica com√∫n en el aprendizaje autom√°tico. Esta divisi√≥n consiste en separar los datos en dos grupos: uno para entrenar el modelo y otro para evaluar su rendimiento. La utilidad de esta divisi√≥n es evaluar el desempe√±o del modelo en datos no vistos durante el entrenamiento, lo que permite estimar c√≥mo se generalizar√° a nuevos ejemplos y evitar el sobreajuste.\n",
    "\n",
    "A continuaci√≥n, te mostrar√© un ejemplo detallado de c√≥mo dividir un `dataframe` en conjuntos de entrenamiento y prueba utilizando la librer√≠a `scikit-learn` en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Edad  Ingresos Compra\n",
      "0    25     50000     No\n",
      "1    35     60000     No\n",
      "2    45     80000     S√≠\n",
      "3    30     45000     No\n",
      "4    20     30000     S√≠\n",
      "5    40     70000     S√≠\n",
      "6    55     90000     S√≠\n",
      "7    50     85000     No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Crear el dataframe de ejemplo\n",
    "df = pd.DataFrame({\n",
    "    'Edad': [25, 35, 45, 30, 20, 40, 55, 50],\n",
    "    'Ingresos': [50000, 60000, 80000, 45000, 30000, 70000, 90000, 85000],\n",
    "    'Compra': ['No', 'No', 'S√≠', 'No', 'S√≠', 'S√≠', 'S√≠', 'No']\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, dividiremos el `dataframe` en conjuntos de entrenamiento y prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de entrenamiento:\n",
      "   Edad  Ingresos\n",
      "0    25     50000\n",
      "7    50     85000\n",
      "2    45     80000\n",
      "4    20     30000\n",
      "3    30     45000\n",
      "6    55     90000\n",
      "0    No\n",
      "7    No\n",
      "2    S√≠\n",
      "4    S√≠\n",
      "3    No\n",
      "6    S√≠\n",
      "Name: Compra, dtype: object\n",
      "Conjunto de prueba:\n",
      "   Edad  Ingresos\n",
      "1    35     60000\n",
      "5    40     70000\n",
      "1    No\n",
      "5    S√≠\n",
      "Name: Compra, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Dividir los datos en caracter√≠sticas (X) y variable objetivo (y)\n",
    "X = df[['Edad', 'Ingresos']]\n",
    "y = df['Compra']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Conjunto de entrenamiento:')\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "print('Conjunto de prueba:')\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos dividido el `dataframe` en un conjunto de entrenamiento y un conjunto de prueba utilizando la funci√≥n `train_test_split()` de `scikit-learn`. Hemos especificado que el tama√±o del conjunto de prueba sea el `20%` del tama√±o total del `dataframe` y hemos fijado una semilla aleatoria `(random_state=42)` para asegurar reproducibilidad.\n",
    "\n",
    "Despu√©s de la divisi√≥n, hemos obtenido `X_train` y `y_train`, que corresponden a las caracter√≠sticas y la variable objetivo del conjunto de entrenamiento, respectivamente. Tambi√©n hemos obtenido `X_test` y `y_test`, que corresponden al conjunto de prueba.\n",
    "\n",
    "Dividir el `dataset` en conjuntos de entrenamiento y prueba nos permite entrenar nuestro modelo con datos conocidos y luego evaluar su rendimiento en datos desconocidos. Esto es esencial para estimar la capacidad de generalizaci√≥n del modelo y detectar posibles problemas de sobreajuste o subajuste."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **C√≥mo escalar los datos**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el aprendizaje autom√°tico, es com√∫n que las diferentes caracter√≠sticas (variables) de un conjunto de datos tengan diferentes escalas o rangos de valores. Por ejemplo, una caracter√≠stica puede tener valores entre `0` y `1`, mientras que otra puede tener valores entre `0` y `1000`. Escalar los datos es un proceso que transforma los valores de las caracter√≠sticas en una escala com√∫n, lo que puede mejorar la eficacia de los algoritmos de aprendizaje autom√°tico.\n",
    "\n",
    "Aqu√≠ te explicar√© c√≥mo escalar los datos en Python utilizando la librer√≠a `Scikit-learn`.\n",
    "\n",
    "Primero, necesitamos cargar el conjunto de datos. Para este ejemplo, utilizaremos el conjunto de datos `iris` que ya viene incluido en `Scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, vamos a escalar los datos utilizando la funci√≥n `StandardScaler` de `Scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(iris.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu√≠, `StandardScaler` es un objeto de la clase `StandardScaler` que se utiliza para escalar los datos. La funci√≥n `fit_transform` se utiliza para ajustar el escalador a los datos y transformar los datos de entrada `iris.data` en una escala com√∫n `X_scaled`. El conjunto de datos escalado resultante `X_scaled` es un `numpy` `array` con las mismas dimensiones que `iris.data`.\n",
    "\n",
    "Podemos verificar los valores escalados de los datos utilizando la funci√≥n `mean` y `std` de `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de los datos originales: [5.84333333 3.05733333 3.758      1.19933333]\n",
      "Desviaci√≥n est√°ndar de los datos originales: [0.82530129 0.43441097 1.75940407 0.75969263]\n",
      "Media de los datos escalados: [-1.69031455e-15 -1.84297022e-15 -1.69864123e-15 -1.40924309e-15]\n",
      "Desviaci√≥n est√°ndar de los datos escalados: [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Media de los datos originales:\", np.mean(iris.data, axis=0))\n",
    "print(\"Desviaci√≥n est√°ndar de los datos originales:\", np.std(iris.data, axis=0))\n",
    "print(\"Media de los datos escalados:\", np.mean(X_scaled, axis=0))\n",
    "print(\"Desviaci√≥n est√°ndar de los datos escalados:\", np.std(X_scaled, axis=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que los datos escalados tienen una media de casi `0` y una desviaci√≥n est√°ndar de `1`, lo que indica que est√°n en una escala com√∫n.\n",
    "\n",
    "En resumen, la funci√≥n `StandardScaler` de `Scikit-learn` nos permite escalar los datos para transformar las diferentes caracter√≠sticas en una escala com√∫n."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escalar los datos es un proceso de transformaci√≥n que ajusta los valores de las caracter√≠sticas a una escala com√∫n. La utilidad de escalar los datos radica en que muchos algoritmos de aprendizaje autom√°tico se benefician de tener caracter√≠sticas con una escala similar. Al escalar los datos, se pueden evitar problemas como la dominancia de caracter√≠sticas con escalas m√°s grandes sobre las caracter√≠sticas con escalas m√°s peque√±as, lo que podr√≠a afectar el rendimiento y la convergencia del modelo.\n",
    "\n",
    "A continuaci√≥n, te mostrar√© un ejemplo detallado de c√≥mo escalar un dataframe utilizando la librer√≠a `scikit-learn` en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Edad  Ingresos Compra\n",
      "0    25     50000     No\n",
      "1    35     60000     No\n",
      "2    45     80000     S√≠\n",
      "3    30     45000     No\n",
      "4    20     30000     S√≠\n",
      "5    40     70000     S√≠\n",
      "6    55     90000     S√≠\n",
      "7    50     85000     No\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Crear el dataframe de ejemplo\n",
    "df = pd.DataFrame({\n",
    "    'Edad': [25, 35, 45, 30, 20, 40, 55, 50],\n",
    "    'Ingresos': [50000, 60000, 80000, 45000, 30000, 70000, 90000, 85000],\n",
    "    'Compra': ['No', 'No', 'S√≠', 'No', 'S√≠', 'S√≠', 'S√≠', 'No']\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, escalaremos las caracter√≠sticas \"`Edad`\" y \"`Ingresos`\" utilizando el escalador `MinMaxScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Edad  Ingresos Compra\n",
      "0  0.142857  0.333333     No\n",
      "1  0.428571  0.500000     No\n",
      "2  0.714286  0.833333     S√≠\n",
      "3  0.285714  0.250000     No\n",
      "4  0.000000  0.000000     S√≠\n",
      "5  0.571429  0.666667     S√≠\n",
      "6  1.000000  1.000000     S√≠\n",
      "7  0.857143  0.916667     No\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar las caracter√≠sticas num√©ricas a escalar\n",
    "features_to_scale = ['Edad', 'Ingresos']\n",
    "\n",
    "# Crear una instancia del escalador MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Escalar las caracter√≠sticas\n",
    "df_scaled = df.copy()\n",
    "df_scaled[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "\n",
    "print(df_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos utilizado el escalador `MinMaxScaler` para escalar las caracter√≠sticas \"`Edad`\" y \"`Ingresos`\" del dataframe. El escalador ajusta los valores de las caracter√≠sticas para que est√©n en el rango de `0` a `1`, preservando la proporci√≥n relativa entre los valores originales.\n",
    "\n",
    "El dataframe escalado (`df_scaled`) muestra las caracter√≠sticas \"`Edad`\" y \"`Ingresos`\" escaladas, mientras que la columna \"`Compra`\" permanece sin cambios.\n",
    "\n",
    "La utilidad de escalar los datos radica en que muchos algoritmos de aprendizaje autom√°tico, como la regresi√≥n log√≠stica, las redes neuronales o el `SVM`, pueden beneficiarse de tener caracter√≠sticas con una escala similar. Al escalar los datos, se pueden evitar problemas como la dominancia de caracter√≠sticas con escalas m√°s grandes sobre las caracter√≠sticas con escalas m√°s peque√±as, lo que podr√≠a afectar el rendimiento y la convergencia del modelo.\n",
    "\n",
    "Adem√°s, al escalar los datos, se puede evitar que ciertos algoritmos de aprendizaje autom√°tico sean m√°s sensibles a las unidades o escalas utilizadas en las caracter√≠sticas, lo que puede mejorar la estabilidad y el rendimiento del modelo.\n",
    "\n",
    "Recuerda que es importante ajustar el escalador solo en el conjunto de entrenamiento y aplicar la misma transformaci√≥n al conjunto de prueba para evitar un sesgo en la evaluaci√≥n del modelo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **Siguiente 2** |\n",
    "|----------- |-------------- |\n",
    "| [üè†](../../README.md) | [‚è©](./2.Regresion.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
