{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr√°s 8** |\n",
    "|----------- |-------------- |\n",
    "| [üè†](../../README.md) | [‚è™](./8.Evitando_Problemas_Eticas.ipynb)|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. Automatizaci√≥n, almacenamiento y actualizaci√≥n de datos**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Web Scraping de Im√°genes (Extracci√≥n de OLX)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El web scraping de im√°genes es una t√©cnica utilizada para extraer im√°genes de sitios web de forma autom√°tica. Puedes aplicar esta t√©cnica para extraer im√°genes de productos, fotos de perfil de usuarios, logotipos de empresas, entre otros casos de uso.\n",
    "\n",
    "A continuaci√≥n, te mostrar√© un ejemplo de c√≥mo realizar el web scraping de im√°genes en el sitio web de OLX utilizando Python y la biblioteca Scrapy:\n",
    "\n",
    "1. **Configuraci√≥n inicial:**\n",
    "\n",
    "* Aseg√∫rate de tener instalado Scrapy en tu entorno de Python:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install scrapy`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Crea un nuevo proyecto de Scrapy:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy startproject olx_scraper`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accede al directorio del proyecto:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cd olx_scraper`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Creaci√≥n del spider:**\n",
    "\n",
    "* Crea un nuevo archivo llamado `olx_spider.py` en el directorio `olx_scraper/spiders`.\n",
    "\n",
    "* Abre el archivo `olx_spider.py` y agrega el siguiente c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class OLXSpider(scrapy.Spider):\n",
    "    name = 'olx'\n",
    "    start_urls = ['https://www.olx.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extraer los enlaces de las publicaciones\n",
    "        for ad_link in response.css('.items-list .item a.detailsLink::attr(href)').getall():\n",
    "            yield response.follow(ad_link, self.parse_ad)\n",
    "\n",
    "        # Seguir a la siguiente p√°gina\n",
    "        next_page = response.css('.pageNextPrev a.pageNextPrev::attr(href)').get()\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\n",
    "    def parse_ad(self, response):\n",
    "        # Extraer la URL de la imagen\n",
    "        image_url = response.css('.big-image img::attr(src)').get()\n",
    "\n",
    "        yield {\n",
    "            'image_url': image_url\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Configuraci√≥n de los pipelines:**\n",
    "\n",
    "* Abre el archivo `olx_scraper/settings.py` y encuentra la configuraci√≥n `ITEM_PIPELINES`.\n",
    "\n",
    "* Aseg√∫rate de que el pipeline de im√°genes est√© habilitado y configurado de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {\n",
    "    'scrapy.pipelines.images.ImagesPipeline': 1,\n",
    "}\n",
    "\n",
    "IMAGES_STORE = 'images'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Ejecuci√≥n del spider:**\n",
    "\n",
    "* Desde el directorio principal del proyecto, ejecuta el siguiente comando para iniciar el web scraping y guardar las im√°genes:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy crawl olx`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las im√°genes extra√≠das se guardar√°n en la carpeta images dentro del directorio del proyecto.\n",
    "\n",
    "Con este ejemplo, el spider `OLXSpider` extraer√° los enlaces de las publicaciones en la p√°gina principal de OLX y seguir√° cada enlace para extraer la URL de la imagen. Luego, utilizando el pipeline de im√°genes de Scrapy, las im√°genes se descargar√°n y guardar√°n en la carpeta `images`.\n",
    "\n",
    "Es importante destacar que, en este ejemplo, solo se extraen las URLs de las im√°genes. Si deseas descargar y procesar las im√°genes de manera adicional, puedes personalizar el pipeline de im√°genes o agregar funciones adicionales en el m√©todo `parse_ad` del spider.\n",
    "\n",
    "Recuerda revisar y cumplir con los t√©rminos de uso del sitio web objetivo y ser respetuoso con la carga del servidor al realizar el web scraping de im√°genes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Web Scraping de Archivos (Extracci√≥n de FILES EXAMPLE)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El web scraping de archivos es una t√©cnica utilizada para extraer archivos de diferentes tipos (como PDF, CSV, Excel, im√°genes, etc.) de sitios web de forma automatizada. Esta t√©cnica puede ser √∫til cuando se requiere recopilar una gran cantidad de archivos para su posterior an√°lisis o procesamiento.\n",
    "\n",
    "A continuaci√≥n, te mostrar√© un ejemplo de c√≥mo realizar el web scraping de archivos utilizando Python y la biblioteca Scrapy. Utilizaremos un ejemplo ficticio donde extraeremos archivos CSV de un sitio web llamado \"Files Example\".\n",
    "\n",
    "1. **Configuraci√≥n inicial:**\n",
    "\n",
    "* Aseg√∫rate de tener instalado Scrapy en tu entorno de Python:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install scrapy`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Crea un nuevo proyecto de Scrapy:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy startproject files_scraper`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accede al directorio del proyecto:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cd files_scraper`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Creaci√≥n del spider:**\n",
    "\n",
    "* Crea un nuevo archivo llamado `files_spider.py` en el directorio `files_scraper/spiders`.\n",
    "\n",
    "* Abre el archivo `files_spider.py` y agrega el siguiente c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class FilesSpider(scrapy.Spider):\n",
    "    name = 'files'\n",
    "    start_urls = ['https://www.filesexample.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extraer los enlaces de los archivos\n",
    "        file_links = response.css('.file-list a::attr(href)').getall()\n",
    "\n",
    "        for file_link in file_links:\n",
    "            yield response.follow(file_link, self.save_file)\n",
    "\n",
    "    def save_file(self, response):\n",
    "        # Guardar el archivo en disco\n",
    "        filename = response.url.split('/')[-1]\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log(f'Se ha guardado el archivo {filename}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Ejecuci√≥n del spider:**\n",
    "\n",
    "* Desde el directorio principal del proyecto, ejecuta el siguiente comando para iniciar el web scraping y descargar los archivos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy crawl files`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los archivos extra√≠dos se guardar√°n en el directorio del proyecto.\n",
    "\n",
    "En este ejemplo, el spider `FilesSpider` accede a la p√°gina principal del sitio \"Files Example\" y extrae los enlaces de los archivos. Luego, utiliza la funci√≥n `response.follow()` para seguir cada enlace y ejecutar el m√©todo `save_file`, donde el archivo se guarda en disco utilizando la URL del archivo como nombre.\n",
    "\n",
    "Es importante destacar que debes tener en cuenta la legalidad y los t√©rminos de uso del sitio web objetivo antes de extraer archivos. Adem√°s, ten en cuenta que algunos sitios web pueden tener restricciones de acceso o medidas de seguridad adicionales para evitar el web scraping de archivos.\n",
    "\n",
    "Recuerda ser respetuoso con el sitio web objetivo y evitar realizar solicitudes excesivas o sobrecargar el servidor al realizar el web scraping de archivos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Automatizaci√≥n de Extracci√≥n en Scrapy (Extracci√≥n de ACCUWEATHER)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para automatizar la extracci√≥n de datos en Scrapy, utilizaremos un ejemplo de extracci√≥n de datos del sitio web de AccuWeather. Vamos a extraer informaci√≥n sobre el clima de diferentes ciudades.\n",
    "\n",
    "1. **Configuraci√≥n inicial:**\n",
    "\n",
    "* Aseg√∫rate de tener instalado Scrapy en tu entorno de Python:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install scrapy`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Crea un nuevo proyecto de Scrapy:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy startproject accuweather_scraper`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accede al directorio del proyecto:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cd accuweather_scraper`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Creaci√≥n del spider:**\n",
    "\n",
    "* Crea un nuevo archivo llamado `accuweather_spider.py` en el directorio `accuweather_scraper/spiders`.\n",
    "\n",
    "* Abre el archivo `accuweather_spider.py` y agrega el siguiente c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class AccuWeatherSpider(scrapy.Spider):\n",
    "    name = 'accuweather'\n",
    "    start_urls = ['https://www.accuweather.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extraer los enlaces de las ciudades\n",
    "        city_links = response.css('.city-list a::attr(href)').getall()\n",
    "\n",
    "        for city_link in city_links:\n",
    "            yield response.follow(city_link, self.parse_city)\n",
    "\n",
    "    def parse_city(self, response):\n",
    "        # Extraer informaci√≥n sobre el clima de la ciudad\n",
    "        city_name = response.css('.city-header h1::text').get()\n",
    "        temperature = response.css('.current-weather .temp::text').get()\n",
    "        weather_description = response.css('.current-weather .phrase::text').get()\n",
    "\n",
    "        yield {\n",
    "            'city': city_name,\n",
    "            'temperature': temperature,\n",
    "            'description': weather_description\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Ejecuci√≥n del spider:**\n",
    "\n",
    "* Desde el directorio principal del proyecto, ejecuta el siguiente comando para iniciar el web scraping y obtener la informaci√≥n del clima de las ciudades:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy crawl accuweather -o weather.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los datos extra√≠dos se guardar√°n en un archivo `weather.csv`.\n",
    "\n",
    "En este ejemplo, el spider `AccuWeatherSpider` accede a la p√°gina principal del sitio web de AccuWeather y extrae los enlaces de las ciudades. Luego, utiliza la funci√≥n `response.follow()` para seguir cada enlace y ejecutar el m√©todo `parse_city`, donde se extrae la informaci√≥n sobre el clima de la ciudad, como el nombre de la ciudad, la temperatura actual y la descripci√≥n del clima. Finalmente, los datos extra√≠dos se almacenan en un archivo CSV.\n",
    "\n",
    "Puedes personalizar y ampliar este ejemplo para extraer m√°s datos del sitio web de AccuWeather o de otros sitios web. Recuerda revisar y respetar los t√©rminos de uso del sitio web objetivo y evitar realizar solicitudes excesivas o sobrecargar el servidor al realizar el web scraping.\n",
    "\n",
    "Adem√°s, puedes utilizar las funciones de Scrapy, como las reglas de enlace y el manejo de paginaci√≥n, para realizar extracciones m√°s avanzadas y completas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Automatizaci√≥n de Extracci√≥n en Selenium (Extracci√≥n de ACCUWEATHER)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para automatizar la extracci√≥n de datos utilizando Selenium, usaremos un ejemplo de extracci√≥n de datos del sitio web de AccuWeather. Extraeremos informaci√≥n sobre el clima de diferentes ciudades.\n",
    "\n",
    "1. **Configuraci√≥n inicial:**\n",
    "\n",
    "* Aseg√∫rate de tener instalado Selenium en tu entorno de Python:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install selenium`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Descarga el controlador del navegador que deseas utilizar (por ejemplo, ChromeDriver para Google Chrome) y col√≥calo en una ubicaci√≥n accesible.\n",
    "\n",
    "2. **Importar las bibliotecas necesarias:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Configuraci√≥n del controlador del navegador:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubicaci√≥n del controlador del navegador\n",
    "path_to_chromedriver = '/ruta/al/controlador/chromedriver'\n",
    "\n",
    "# Configuraci√≥n de las opciones del navegador\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # Ejecutar en modo sin cabeza (headless)\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# Crear el servicio del controlador del navegador\n",
    "service = Service(path_to_chromedriver)\n",
    "\n",
    "# Iniciar el navegador\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Navegaci√≥n y extracci√≥n de datos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navegar a la p√°gina principal de AccuWeather\n",
    "driver.get('https://www.accuweather.com')\n",
    "\n",
    "# Extraer los enlaces de las ciudades\n",
    "city_links = driver.find_elements(By.CSS_SELECTOR, '.city-list a')\n",
    "\n",
    "for city_link in city_links:\n",
    "    # Obtener la URL del enlace de la ciudad\n",
    "    city_url = city_link.get_attribute('href')\n",
    "\n",
    "    # Navegar a la p√°gina de la ciudad\n",
    "    driver.get(city_url)\n",
    "\n",
    "    # Extraer informaci√≥n sobre el clima de la ciudad\n",
    "    city_name = driver.find_element(By.CSS_SELECTOR, '.city-header h1').text\n",
    "    temperature = driver.find_element(By.CSS_SELECTOR, '.current-weather .temp').text\n",
    "    weather_description = driver.find_element(By.CSS_SELECTOR, '.current-weather .phrase').text\n",
    "\n",
    "    # Imprimir los datos extra√≠dos\n",
    "    print('City:', city_name)\n",
    "    print('Temperature:', temperature)\n",
    "    print('Weather:', weather_description)\n",
    "    print('---')\n",
    "\n",
    "# Cerrar el navegador\n",
    "driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, utilizamos Selenium para automatizar la navegaci√≥n y la extracci√≥n de datos del sitio web de AccuWeather. Primero, configuramos el controlador del navegador (en este caso, ChromeDriver) y lo iniciamos con las opciones adecuadas. Luego, navegamos a la p√°gina principal de AccuWeather y extraemos los enlaces de las ciudades.\n",
    "\n",
    "Despu√©s, iteramos sobre los enlaces de las ciudades y navegamos a cada p√°gina de la ciudad. Utilizamos los selectores de CSS para localizar y extraer la informaci√≥n sobre el clima de la ciudad, como el nombre de la ciudad, la temperatura y la descripci√≥n del clima.\n",
    "\n",
    "Finalmente, imprimimos los datos extra√≠dos. Puedes personalizar y adaptar este c√≥digo para guardar los datos en una base de datos, en un archivo CSV u otras acciones seg√∫n tus necesidades.\n",
    "\n",
    "Recuerda que al realizar web scraping con Selenium, debes tener en cuenta la pol√≠tica de uso del sitio web objetivo y evitar realizar solicitudes excesivas o sobrecargar el servidor. Adem√°s, aseg√∫rate de cumplir con las pol√≠ticas de privacidad y t√©rminos de servicio del sitio web objetivo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Almacenamiento de datos en MongoDB (Extracci√≥n de OLX)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para almacenar los datos extra√≠dos de OLX en MongoDB, necesitaremos seguir los siguientes pasos:\n",
    "\n",
    "1. **Instalaci√≥n y configuraci√≥n:**\n",
    "\n",
    "* Aseg√∫rate de tener MongoDB instalado en tu sistema. Puedes descargarlo e instalarlo desde el sitio web oficial de MongoDB.\n",
    "* Instala el controlador de Python para MongoDB (pymongo) utilizando el siguiente comando:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install pymongo`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Importar las bibliotecas necesarias:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Configuraci√≥n de la conexi√≥n a la base de datos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer la conexi√≥n con MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "# Crear una instancia de la base de datos\n",
    "db = client['olx']\n",
    "\n",
    "# Crear una colecci√≥n para almacenar los datos\n",
    "collection = db['productos']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Almacenamiento de los datos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que tienes una lista de diccionarios \"items\" que contiene los datos extra√≠dos de OLX\n",
    "\n",
    "# Insertar los datos en la colecci√≥n\n",
    "collection.insert_many(items)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, configuramos una conexi√≥n a la base de datos de MongoDB utilizando la clase `MongoClient` de `pymongo`. Establecemos la conexi√≥n a trav√©s de la URL `mongodb://localhost:27017/`, que representa la direcci√≥n y el puerto del servidor de MongoDB.\n",
    "\n",
    "Luego, creamos una instancia de la base de datos llamada \"olx\" y una colecci√≥n llamada \"productos\" en la base de datos. Puedes elegir nombres diferentes seg√∫n tus necesidades.\n",
    "\n",
    "Finalmente, asumiendo que tienes una lista de diccionarios llamada \"items\" que contiene los datos extra√≠dos de OLX, utilizamos el m√©todo `insert_many()` para almacenar los datos en la colecci√≥n.\n",
    "\n",
    "Puedes realizar consultas y manipulaciones adicionales en la base de datos utilizando los m√©todos y funciones proporcionados por `pymongo`. Recuerda que es importante manejar correctamente los errores y cerrar la conexi√≥n a la base de datos despu√©s de finalizar las operaciones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Actualizaci√≥n peri√≥dica de datos con Selenium, Scrapy y Mongo (ACCUWEATHER)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar una actualizaci√≥n peri√≥dica de datos de ACCUWEATHER utilizando Selenium, Scrapy y MongoDB, podemos seguir los siguientes pasos:\n",
    "\n",
    "1. **Configuraci√≥n inicial:**\n",
    "\n",
    "* Aseg√∫rate de tener instalados Selenium, Scrapy y pymongo en tu entorno.\n",
    "* Configura la conexi√≥n a la base de datos MongoDB, siguiendo los pasos mencionados en la respuesta anterior.\n",
    "\n",
    "2. **Extracci√≥n de datos con Selenium:**\n",
    "\n",
    "* Utiliza Selenium para automatizar la navegaci√≥n en el sitio web de ACCUWEATHER y extraer los datos deseados.\n",
    "* Crea un script de Selenium que abra el navegador, interact√∫e con la p√°gina, y extraiga los datos necesarios utilizando los selectores de elementos HTML y las funciones de Selenium.\n",
    "* Almacena los datos extra√≠dos en una estructura de datos (por ejemplo, una lista de diccionarios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de extracci√≥n de datos utilizando Selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "# Configurar el navegador\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navegar a la p√°gina de ACCUWEATHER\n",
    "driver.get('https://www.accuweather.com/')\n",
    "\n",
    "# Interactuar con la p√°gina para extraer los datos deseados\n",
    "# ...\n",
    "\n",
    "# Almacenar los datos extra√≠dos en una estructura de datos (por ejemplo, una lista de diccionarios)\n",
    "data = [\n",
    "    {'ciudad': 'Lima', 'temperatura': '25¬∞C', 'humedad': '60%'},\n",
    "    {'ciudad': 'Cusco', 'temperatura': '20¬∞C', 'humedad': '55%'},\n",
    "    # ...\n",
    "]\n",
    "\n",
    "# Cerrar el navegador\n",
    "driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Almacenamiento de datos en MongoDB:**\n",
    "\n",
    "* Utiliza pymongo para establecer una conexi√≥n con la base de datos MongoDB y almacenar los datos extra√≠dos.\n",
    "* Utiliza el c√≥digo de configuraci√≥n de conexi√≥n y almacenamiento mencionado en la respuesta anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de almacenamiento de datos en MongoDB utilizando pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Configurar la conexi√≥n a MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['acuweather']\n",
    "collection = db['datos']\n",
    "\n",
    "# Insertar los datos en la colecci√≥n\n",
    "collection.insert_many(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Automatizaci√≥n peri√≥dica con Scrapy:**\n",
    "\n",
    "* Configura un proyecto de Scrapy y crea un spider para realizar la extracci√≥n peri√≥dica de datos utilizando el script de Selenium.\n",
    "* Establece el intervalo de tiempo deseado para la actualizaci√≥n peri√≥dica.\n",
    "* En el spider de Scrapy, ejecuta el script de Selenium y extrae los datos utilizando las funciones y selectores necesarios.\n",
    "* Almacena los datos extra√≠dos en la base de datos MongoDB utilizando pymongo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de spider de Scrapy para la extracci√≥n peri√≥dica de datos utilizando Selenium y almacenamiento en MongoDB\n",
    "import scrapy\n",
    "from selenium import webdriver\n",
    "from pymongo import MongoClient\n",
    "\n",
    "class AccuweatherSpider(scrapy.Spider):\n",
    "    name = 'accuweather_spider'\n",
    "    start_urls = ['https://www.accuweather.com/']\n",
    "\n",
    "    def __init__(self):\n",
    "        # Configurar el navegador\n",
    "        self.driver = webdriver.Chrome()\n",
    "\n",
    "        # Configurar la conexi√≥n a MongoDB\n",
    "        self.client = MongoClient('mongodb://localhost:27017/')\n",
    "        self.db = self.client['acuweather']\n",
    "        self.collection = self.db['datos']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Interactuar con la p√°gina para extraer los datos deseados utilizando Selenium\n",
    "        self.driver.get(response.url)\n",
    "        # ...\n",
    "\n",
    "        # Almacenar los datos extra√≠dos en una estructura de datos\n",
    "        data = [\n",
    "            {'ciudad': 'Lima', 'temperatura': '25¬∞C', 'humedad': '60%'},\n",
    "            {'ciudad': 'Cusco', 'temperatura': '20¬∞C', 'humedad': '55%'},\n",
    "            # ...\n",
    "        ]\n",
    "\n",
    "        # Insertar los datos en la colecci√≥n de MongoDB\n",
    "        self.collection.insert_many(data)\n",
    "\n",
    "    def closed(self, reason):\n",
    "        # Cerrar el navegador y la conexi√≥n a MongoDB\n",
    "        self.driver.quit()\n",
    "        self.client.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el spider de Scrapy, se abrir√° el navegador controlado por Selenium, se extraer√°n los datos de ACCUWEATHER y se almacenar√°n en la base de datos MongoDB. Puedes programar la ejecuci√≥n peri√≥dica del spider utilizando programadores de tareas o cron jobs para mantener actualizados los datos en la base de datos MongoDB.\n",
    "\n",
    "Recuerda adaptar el c√≥digo seg√∫n tus necesidades espec√≠ficas, como los selectores de elementos HTML, las rutas de almacenamiento en MongoDB y la frecuencia de actualizaci√≥n."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr√°s 8** |\n",
    "|----------- |-------------- |\n",
    "| [üè†](../../README.md) | [‚è™](./8.Evitando_Problemas_Eticas.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
