{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr谩s 8** |\n",
    "|----------- |-------------- |\n",
    "| [](../../README.md) | [](./8.Evitando_Problemas_Eticas.ipynb)|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. Automatizaci贸n, almacenamiento y actualizaci贸n de datos**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Web Scraping de Im谩genes (Extracci贸n de OLX)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El web scraping de im谩genes es una t茅cnica utilizada para extraer im谩genes de sitios web de forma autom谩tica. Puedes aplicar esta t茅cnica para extraer im谩genes de productos, fotos de perfil de usuarios, logotipos de empresas, entre otros casos de uso.\n",
    "\n",
    "A continuaci贸n, te mostrar茅 un ejemplo de c贸mo realizar el web scraping de im谩genes en el sitio web de OLX utilizando Python y la biblioteca Scrapy:\n",
    "\n",
    "1. **Configuraci贸n inicial:**\n",
    "\n",
    "* Aseg煤rate de tener instalado Scrapy en tu entorno de Python:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install scrapy`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Crea un nuevo proyecto de Scrapy:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy startproject olx_scraper`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accede al directorio del proyecto:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cd olx_scraper`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Creaci贸n del spider:**\n",
    "\n",
    "* Crea un nuevo archivo llamado `olx_spider.py` en el directorio `olx_scraper/spiders`.\n",
    "\n",
    "* Abre el archivo `olx_spider.py` y agrega el siguiente c贸digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class OLXSpider(scrapy.Spider):\n",
    "    name = 'olx'\n",
    "    start_urls = ['https://www.olx.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extraer los enlaces de las publicaciones\n",
    "        for ad_link in response.css('.items-list .item a.detailsLink::attr(href)').getall():\n",
    "            yield response.follow(ad_link, self.parse_ad)\n",
    "\n",
    "        # Seguir a la siguiente p谩gina\n",
    "        next_page = response.css('.pageNextPrev a.pageNextPrev::attr(href)').get()\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\n",
    "    def parse_ad(self, response):\n",
    "        # Extraer la URL de la imagen\n",
    "        image_url = response.css('.big-image img::attr(src)').get()\n",
    "\n",
    "        yield {\n",
    "            'image_url': image_url\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Configuraci贸n de los pipelines:**\n",
    "\n",
    "* Abre el archivo `olx_scraper/settings.py` y encuentra la configuraci贸n `ITEM_PIPELINES`.\n",
    "\n",
    "* Aseg煤rate de que el pipeline de im谩genes est茅 habilitado y configurado de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {\n",
    "    'scrapy.pipelines.images.ImagesPipeline': 1,\n",
    "}\n",
    "\n",
    "IMAGES_STORE = 'images'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Ejecuci贸n del spider:**\n",
    "\n",
    "* Desde el directorio principal del proyecto, ejecuta el siguiente comando para iniciar el web scraping y guardar las im谩genes:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy crawl olx`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Las im谩genes extra铆das se guardar谩n en la carpeta images dentro del directorio del proyecto.\n",
    "\n",
    "Con este ejemplo, el spider `OLXSpider` extraer谩 los enlaces de las publicaciones en la p谩gina principal de OLX y seguir谩 cada enlace para extraer la URL de la imagen. Luego, utilizando el pipeline de im谩genes de Scrapy, las im谩genes se descargar谩n y guardar谩n en la carpeta `images`.\n",
    "\n",
    "Es importante destacar que, en este ejemplo, solo se extraen las URLs de las im谩genes. Si deseas descargar y procesar las im谩genes de manera adicional, puedes personalizar el pipeline de im谩genes o agregar funciones adicionales en el m茅todo `parse_ad` del spider.\n",
    "\n",
    "Recuerda revisar y cumplir con los t茅rminos de uso del sitio web objetivo y ser respetuoso con la carga del servidor al realizar el web scraping de im谩genes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Web Scraping de Archivos (Extracci贸n de FILES EXAMPLE)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El web scraping de archivos es una t茅cnica utilizada para extraer archivos de diferentes tipos (como PDF, CSV, Excel, im谩genes, etc.) de sitios web de forma automatizada. Esta t茅cnica puede ser 煤til cuando se requiere recopilar una gran cantidad de archivos para su posterior an谩lisis o procesamiento.\n",
    "\n",
    "A continuaci贸n, te mostrar茅 un ejemplo de c贸mo realizar el web scraping de archivos utilizando Python y la biblioteca Scrapy. Utilizaremos un ejemplo ficticio donde extraeremos archivos CSV de un sitio web llamado \"Files Example\".\n",
    "\n",
    "1. **Configuraci贸n inicial:**\n",
    "\n",
    "* Aseg煤rate de tener instalado Scrapy en tu entorno de Python:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install scrapy`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Crea un nuevo proyecto de Scrapy:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy startproject files_scraper`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accede al directorio del proyecto:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cd files_scraper`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Creaci贸n del spider:**\n",
    "\n",
    "* Crea un nuevo archivo llamado `files_spider.py` en el directorio `files_scraper/spiders`.\n",
    "\n",
    "* Abre el archivo `files_spider.py` y agrega el siguiente c贸digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class FilesSpider(scrapy.Spider):\n",
    "    name = 'files'\n",
    "    start_urls = ['https://www.filesexample.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extraer los enlaces de los archivos\n",
    "        file_links = response.css('.file-list a::attr(href)').getall()\n",
    "\n",
    "        for file_link in file_links:\n",
    "            yield response.follow(file_link, self.save_file)\n",
    "\n",
    "    def save_file(self, response):\n",
    "        # Guardar el archivo en disco\n",
    "        filename = response.url.split('/')[-1]\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log(f'Se ha guardado el archivo {filename}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Ejecuci贸n del spider:**\n",
    "\n",
    "* Desde el directorio principal del proyecto, ejecuta el siguiente comando para iniciar el web scraping y descargar los archivos:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy crawl files`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los archivos extra铆dos se guardar谩n en el directorio del proyecto.\n",
    "\n",
    "En este ejemplo, el spider `FilesSpider` accede a la p谩gina principal del sitio \"Files Example\" y extrae los enlaces de los archivos. Luego, utiliza la funci贸n `response.follow()` para seguir cada enlace y ejecutar el m茅todo `save_file`, donde el archivo se guarda en disco utilizando la URL del archivo como nombre.\n",
    "\n",
    "Es importante destacar que debes tener en cuenta la legalidad y los t茅rminos de uso del sitio web objetivo antes de extraer archivos. Adem谩s, ten en cuenta que algunos sitios web pueden tener restricciones de acceso o medidas de seguridad adicionales para evitar el web scraping de archivos.\n",
    "\n",
    "Recuerda ser respetuoso con el sitio web objetivo y evitar realizar solicitudes excesivas o sobrecargar el servidor al realizar el web scraping de archivos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Automatizaci贸n de Extracci贸n en Scrapy (Extracci贸n de ACCUWEATHER)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para automatizar la extracci贸n de datos en Scrapy, utilizaremos un ejemplo de extracci贸n de datos del sitio web de AccuWeather. Vamos a extraer informaci贸n sobre el clima de diferentes ciudades.\n",
    "\n",
    "1. **Configuraci贸n inicial:**\n",
    "\n",
    "* Aseg煤rate de tener instalado Scrapy en tu entorno de Python:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install scrapy`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Crea un nuevo proyecto de Scrapy:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy startproject accuweather_scraper`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accede al directorio del proyecto:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cd accuweather_scraper`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Creaci贸n del spider:**\n",
    "\n",
    "* Crea un nuevo archivo llamado `accuweather_spider.py` en el directorio `accuweather_scraper/spiders`.\n",
    "\n",
    "* Abre el archivo `accuweather_spider.py` y agrega el siguiente c贸digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class AccuWeatherSpider(scrapy.Spider):\n",
    "    name = 'accuweather'\n",
    "    start_urls = ['https://www.accuweather.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extraer los enlaces de las ciudades\n",
    "        city_links = response.css('.city-list a::attr(href)').getall()\n",
    "\n",
    "        for city_link in city_links:\n",
    "            yield response.follow(city_link, self.parse_city)\n",
    "\n",
    "    def parse_city(self, response):\n",
    "        # Extraer informaci贸n sobre el clima de la ciudad\n",
    "        city_name = response.css('.city-header h1::text').get()\n",
    "        temperature = response.css('.current-weather .temp::text').get()\n",
    "        weather_description = response.css('.current-weather .phrase::text').get()\n",
    "\n",
    "        yield {\n",
    "            'city': city_name,\n",
    "            'temperature': temperature,\n",
    "            'description': weather_description\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Ejecuci贸n del spider:**\n",
    "\n",
    "* Desde el directorio principal del proyecto, ejecuta el siguiente comando para iniciar el web scraping y obtener la informaci贸n del clima de las ciudades:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy crawl accuweather -o weather.csv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los datos extra铆dos se guardar谩n en un archivo `weather.csv`.\n",
    "\n",
    "En este ejemplo, el spider `AccuWeatherSpider` accede a la p谩gina principal del sitio web de AccuWeather y extrae los enlaces de las ciudades. Luego, utiliza la funci贸n `response.follow()` para seguir cada enlace y ejecutar el m茅todo `parse_city`, donde se extrae la informaci贸n sobre el clima de la ciudad, como el nombre de la ciudad, la temperatura actual y la descripci贸n del clima. Finalmente, los datos extra铆dos se almacenan en un archivo CSV.\n",
    "\n",
    "Puedes personalizar y ampliar este ejemplo para extraer m谩s datos del sitio web de AccuWeather o de otros sitios web. Recuerda revisar y respetar los t茅rminos de uso del sitio web objetivo y evitar realizar solicitudes excesivas o sobrecargar el servidor al realizar el web scraping.\n",
    "\n",
    "Adem谩s, puedes utilizar las funciones de Scrapy, como las reglas de enlace y el manejo de paginaci贸n, para realizar extracciones m谩s avanzadas y completas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Automatizaci贸n de Extracci贸n en Selenium (Extracci贸n de ACCUWEATHER)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para automatizar la extracci贸n de datos utilizando Selenium, usaremos un ejemplo de extracci贸n de datos del sitio web de AccuWeather. Extraeremos informaci贸n sobre el clima de diferentes ciudades.\n",
    "\n",
    "1. **Configuraci贸n inicial:**\n",
    "\n",
    "* Aseg煤rate de tener instalado Selenium en tu entorno de Python:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install selenium`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Descarga el controlador del navegador que deseas utilizar (por ejemplo, ChromeDriver para Google Chrome) y col贸calo en una ubicaci贸n accesible.\n",
    "\n",
    "2. **Importar las bibliotecas necesarias:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Configuraci贸n del controlador del navegador:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ubicaci贸n del controlador del navegador\n",
    "path_to_chromedriver = '/ruta/al/controlador/chromedriver'\n",
    "\n",
    "# Configuraci贸n de las opciones del navegador\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # Ejecutar en modo sin cabeza (headless)\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# Crear el servicio del controlador del navegador\n",
    "service = Service(path_to_chromedriver)\n",
    "\n",
    "# Iniciar el navegador\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Navegaci贸n y extracci贸n de datos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navegar a la p谩gina principal de AccuWeather\n",
    "driver.get('https://www.accuweather.com')\n",
    "\n",
    "# Extraer los enlaces de las ciudades\n",
    "city_links = driver.find_elements(By.CSS_SELECTOR, '.city-list a')\n",
    "\n",
    "for city_link in city_links:\n",
    "    # Obtener la URL del enlace de la ciudad\n",
    "    city_url = city_link.get_attribute('href')\n",
    "\n",
    "    # Navegar a la p谩gina de la ciudad\n",
    "    driver.get(city_url)\n",
    "\n",
    "    # Extraer informaci贸n sobre el clima de la ciudad\n",
    "    city_name = driver.find_element(By.CSS_SELECTOR, '.city-header h1').text\n",
    "    temperature = driver.find_element(By.CSS_SELECTOR, '.current-weather .temp').text\n",
    "    weather_description = driver.find_element(By.CSS_SELECTOR, '.current-weather .phrase').text\n",
    "\n",
    "    # Imprimir los datos extra铆dos\n",
    "    print('City:', city_name)\n",
    "    print('Temperature:', temperature)\n",
    "    print('Weather:', weather_description)\n",
    "    print('---')\n",
    "\n",
    "# Cerrar el navegador\n",
    "driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, utilizamos Selenium para automatizar la navegaci贸n y la extracci贸n de datos del sitio web de AccuWeather. Primero, configuramos el controlador del navegador (en este caso, ChromeDriver) y lo iniciamos con las opciones adecuadas. Luego, navegamos a la p谩gina principal de AccuWeather y extraemos los enlaces de las ciudades.\n",
    "\n",
    "Despu茅s, iteramos sobre los enlaces de las ciudades y navegamos a cada p谩gina de la ciudad. Utilizamos los selectores de CSS para localizar y extraer la informaci贸n sobre el clima de la ciudad, como el nombre de la ciudad, la temperatura y la descripci贸n del clima.\n",
    "\n",
    "Finalmente, imprimimos los datos extra铆dos. Puedes personalizar y adaptar este c贸digo para guardar los datos en una base de datos, en un archivo CSV u otras acciones seg煤n tus necesidades.\n",
    "\n",
    "Recuerda que al realizar web scraping con Selenium, debes tener en cuenta la pol铆tica de uso del sitio web objetivo y evitar realizar solicitudes excesivas o sobrecargar el servidor. Adem谩s, aseg煤rate de cumplir con las pol铆ticas de privacidad y t茅rminos de servicio del sitio web objetivo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Almacenamiento de datos en MongoDB (Extracci贸n de OLX)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para almacenar los datos extra铆dos de OLX en MongoDB, necesitaremos seguir los siguientes pasos:\n",
    "\n",
    "1. **Instalaci贸n y configuraci贸n:**\n",
    "\n",
    "* Aseg煤rate de tener MongoDB instalado en tu sistema. Puedes descargarlo e instalarlo desde el sitio web oficial de MongoDB.\n",
    "* Instala el controlador de Python para MongoDB (pymongo) utilizando el siguiente comando:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install pymongo`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Importar las bibliotecas necesarias:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Configuraci贸n de la conexi贸n a la base de datos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer la conexi贸n con MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "# Crear una instancia de la base de datos\n",
    "db = client['olx']\n",
    "\n",
    "# Crear una colecci贸n para almacenar los datos\n",
    "collection = db['productos']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Almacenamiento de los datos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que tienes una lista de diccionarios \"items\" que contiene los datos extra铆dos de OLX\n",
    "\n",
    "# Insertar los datos en la colecci贸n\n",
    "collection.insert_many(items)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, configuramos una conexi贸n a la base de datos de MongoDB utilizando la clase `MongoClient` de `pymongo`. Establecemos la conexi贸n a trav茅s de la URL `mongodb://localhost:27017/`, que representa la direcci贸n y el puerto del servidor de MongoDB.\n",
    "\n",
    "Luego, creamos una instancia de la base de datos llamada \"olx\" y una colecci贸n llamada \"productos\" en la base de datos. Puedes elegir nombres diferentes seg煤n tus necesidades.\n",
    "\n",
    "Finalmente, asumiendo que tienes una lista de diccionarios llamada \"items\" que contiene los datos extra铆dos de OLX, utilizamos el m茅todo `insert_many()` para almacenar los datos en la colecci贸n.\n",
    "\n",
    "Puedes realizar consultas y manipulaciones adicionales en la base de datos utilizando los m茅todos y funciones proporcionados por `pymongo`. Recuerda que es importante manejar correctamente los errores y cerrar la conexi贸n a la base de datos despu茅s de finalizar las operaciones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Actualizaci贸n peri贸dica de datos con Selenium, Scrapy y Mongo (ACCUWEATHER)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar una actualizaci贸n peri贸dica de datos de ACCUWEATHER utilizando Selenium, Scrapy y MongoDB, podemos seguir los siguientes pasos:\n",
    "\n",
    "1. **Configuraci贸n inicial:**\n",
    "\n",
    "* Aseg煤rate de tener instalados Selenium, Scrapy y pymongo en tu entorno.\n",
    "* Configura la conexi贸n a la base de datos MongoDB, siguiendo los pasos mencionados en la respuesta anterior.\n",
    "\n",
    "2. **Extracci贸n de datos con Selenium:**\n",
    "\n",
    "* Utiliza Selenium para automatizar la navegaci贸n en el sitio web de ACCUWEATHER y extraer los datos deseados.\n",
    "* Crea un script de Selenium que abra el navegador, interact煤e con la p谩gina, y extraiga los datos necesarios utilizando los selectores de elementos HTML y las funciones de Selenium.\n",
    "* Almacena los datos extra铆dos en una estructura de datos (por ejemplo, una lista de diccionarios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de extracci贸n de datos utilizando Selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "# Configurar el navegador\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navegar a la p谩gina de ACCUWEATHER\n",
    "driver.get('https://www.accuweather.com/')\n",
    "\n",
    "# Interactuar con la p谩gina para extraer los datos deseados\n",
    "# ...\n",
    "\n",
    "# Almacenar los datos extra铆dos en una estructura de datos (por ejemplo, una lista de diccionarios)\n",
    "data = [\n",
    "    {'ciudad': 'Lima', 'temperatura': '25掳C', 'humedad': '60%'},\n",
    "    {'ciudad': 'Cusco', 'temperatura': '20掳C', 'humedad': '55%'},\n",
    "    # ...\n",
    "]\n",
    "\n",
    "# Cerrar el navegador\n",
    "driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Almacenamiento de datos en MongoDB:**\n",
    "\n",
    "* Utiliza pymongo para establecer una conexi贸n con la base de datos MongoDB y almacenar los datos extra铆dos.\n",
    "* Utiliza el c贸digo de configuraci贸n de conexi贸n y almacenamiento mencionado en la respuesta anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de almacenamiento de datos en MongoDB utilizando pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Configurar la conexi贸n a MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['acuweather']\n",
    "collection = db['datos']\n",
    "\n",
    "# Insertar los datos en la colecci贸n\n",
    "collection.insert_many(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Automatizaci贸n peri贸dica con Scrapy:**\n",
    "\n",
    "* Configura un proyecto de Scrapy y crea un spider para realizar la extracci贸n peri贸dica de datos utilizando el script de Selenium.\n",
    "* Establece el intervalo de tiempo deseado para la actualizaci贸n peri贸dica.\n",
    "* En el spider de Scrapy, ejecuta el script de Selenium y extrae los datos utilizando las funciones y selectores necesarios.\n",
    "* Almacena los datos extra铆dos en la base de datos MongoDB utilizando pymongo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de spider de Scrapy para la extracci贸n peri贸dica de datos utilizando Selenium y almacenamiento en MongoDB\n",
    "import scrapy\n",
    "from selenium import webdriver\n",
    "from pymongo import MongoClient\n",
    "\n",
    "class AccuweatherSpider(scrapy.Spider):\n",
    "    name = 'accuweather_spider'\n",
    "    start_urls = ['https://www.accuweather.com/']\n",
    "\n",
    "    def __init__(self):\n",
    "        # Configurar el navegador\n",
    "        self.driver = webdriver.Chrome()\n",
    "\n",
    "        # Configurar la conexi贸n a MongoDB\n",
    "        self.client = MongoClient('mongodb://localhost:27017/')\n",
    "        self.db = self.client['acuweather']\n",
    "        self.collection = self.db['datos']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Interactuar con la p谩gina para extraer los datos deseados utilizando Selenium\n",
    "        self.driver.get(response.url)\n",
    "        # ...\n",
    "\n",
    "        # Almacenar los datos extra铆dos en una estructura de datos\n",
    "        data = [\n",
    "            {'ciudad': 'Lima', 'temperatura': '25掳C', 'humedad': '60%'},\n",
    "            {'ciudad': 'Cusco', 'temperatura': '20掳C', 'humedad': '55%'},\n",
    "            # ...\n",
    "        ]\n",
    "\n",
    "        # Insertar los datos en la colecci贸n de MongoDB\n",
    "        self.collection.insert_many(data)\n",
    "\n",
    "    def closed(self, reason):\n",
    "        # Cerrar el navegador y la conexi贸n a MongoDB\n",
    "        self.driver.quit()\n",
    "        self.client.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el spider de Scrapy, se abrir谩 el navegador controlado por Selenium, se extraer谩n los datos de ACCUWEATHER y se almacenar谩n en la base de datos MongoDB. Puedes programar la ejecuci贸n peri贸dica del spider utilizando programadores de tareas o cron jobs para mantener actualizados los datos en la base de datos MongoDB.\n",
    "\n",
    "Recuerda adaptar el c贸digo seg煤n tus necesidades espec铆ficas, como los selectores de elementos HTML, las rutas de almacenamiento en MongoDB y la frecuencia de actualizaci贸n."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr谩s 8** |\n",
    "|----------- |-------------- |\n",
    "| [](../../README.md) | [](./8.Evitando_Problemas_Eticas.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
