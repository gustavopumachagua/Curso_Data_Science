{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr√°s 2** | **Siguiente 4** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [üè†](../../README.md) | [‚è™](./2.Introduccion_al_Web_Scraping.ipynb)| [‚è©](./4.Varias_Paginas_del_mismo_dominio.ipynb)|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Una sola pagina est√°tica**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introducci√≥n**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Web Scraping de una sola p√°gina est√°tica implica extraer datos de una p√°gina web que no cambia din√°micamente y cuyo contenido es fijo. Este tipo de Web Scraping se utiliza cuando se desea obtener informaci√≥n espec√≠fica de una p√°gina web est√°tica sin interactuar con ella.\n",
    "\n",
    "A continuaci√≥n, te proporcionar√© una explicaci√≥n detallada junto con un ejemplo de c√≥mo realizar Web Scraping en una sola p√°gina est√°tica utilizando Python y la biblioteca `requests`:\n",
    "\n",
    "1. **Importar las bibliotecas necesarias:**\n",
    "\n",
    "En primer lugar, debemos importar la biblioteca `requests` para enviar solicitudes HTTP y obtener el contenido de la p√°gina web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Enviar una solicitud HTTP y obtener el contenido de la p√°gina:**\n",
    "\n",
    "Utilizamos la funci√≥n `get()` de `requests` para enviar una solicitud `GET` a la URL de la p√°gina web y obtener su contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://es.wikipedia.org/wiki/Anexo:Episodios_de_Naruto:_Shippuden\"\n",
    "response = requests.get(url)\n",
    "content = response.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La respuesta de la solicitud se almacena en `response`, y el contenido HTML de la p√°gina se almacena en `content`.\n",
    "\n",
    "3. **Analizar el contenido HTML:**\n",
    "\n",
    "Para extraer informaci√≥n espec√≠fica de la p√°gina, necesitamos analizar el contenido HTML utilizando una biblioteca como `BeautifulSoup`. Importamos `BeautifulSoup` y creamos un objeto `soup` pasando el contenido HTML y el analizador deseado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(content, \"html.parser\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Extraer datos de la p√°gina web:**\n",
    "\n",
    "Utilizando m√©todos y selectores proporcionados por `BeautifulSoup`, podemos extraer datos espec√≠ficos de la p√°gina web. Por ejemplo, si deseamos extraer todos los enlaces de la p√°gina, podemos usar el m√©todo `find_all()` junto con el selector \"`a`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#bodyContent\n",
      "/wiki/Wikipedia:Portada\n",
      "/wiki/Portal:Comunidad\n",
      "/wiki/Portal:Actualidad\n",
      "/wiki/Especial:CambiosRecientes\n",
      "/wiki/Especial:P%C3%A1ginasNuevas\n",
      "/wiki/Especial:Aleatoria\n",
      "/wiki/Ayuda:Contenidos\n",
      "//donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_es.wikipedia.org&uselang=es\n",
      "/wiki/Wikipedia:Informes_de_error\n",
      "#p-lang-btn\n",
      "/wiki/Wikipedia:Portada\n",
      "/wiki/Especial:Buscar\n",
      "/w/index.php?title=Especial:Crear_una_cuenta&returnto=Anexo%3AEpisodios+de+Naruto%3A+Shippuden\n",
      "/w/index.php?title=Especial:Entrar&returnto=Anexo%3AEpisodios+de+Naruto%3A+Shippuden\n",
      "/w/index.php?title=Especial:Crear_una_cuenta&returnto=Anexo%3AEpisodios+de+Naruto%3A+Shippuden\n",
      "/w/index.php?title=Especial:Entrar&returnto=Anexo%3AEpisodios+de+Naruto%3A+Shippuden\n",
      "/wiki/Ayuda:Introducci%C3%B3n\n",
      "/wiki/Especial:MisContribuciones\n",
      "/wiki/Especial:MiDiscusi%C3%B3n\n",
      "https://ar.wikipedia.org/wiki/%D9%82%D8%A7%D8%A6%D9%85%D8%A9_%D8%AD%D9%84%D9%82%D8%A7%D8%AA_%D9%86%D8%A7%D8%B1%D9%88%D8%AA%D9%88_%D8%B4%D9%8A%D8%A8%D9%88%D8%AF%D9%86\n",
      "https://bg.wikipedia.org/wiki/%D0%A1%D0%BF%D0%B8%D1%81%D1%8A%D0%BA_%D1%81_%D0%B5%D0%BF%D0%B8%D0%B7%D0%BE%D0%B4%D0%B8_%D0%BD%D0%B0_%E2%80%9E%D0%9D%D0%B0%D1%80%D1%83%D1%82%D0%BE:_%D0%A3%D1%80%D0%B0%D0%B3%D0%B0%D0%BD%D0%BD%D0%B8_%D1%85%D1%80%D0%BE%D0%BD%D0%B8%D0%BA%D0%B8%E2%80%9C\n",
      "https://cs.wikipedia.org/wiki/Seznam_d%C3%ADl%C5%AF_seri%C3%A1lu_Naruto:_%C5%A0ipp%C3%BAden\n",
      "https://da.wikipedia.org/wiki/Naruto_Shippuden-afsnit\n",
      "https://de.wikipedia.org/wiki/Naruto_Shippuden/Episodenliste\n",
      "https://en.wikipedia.org/wiki/List_of_Naruto:_Shippuden_episodes\n",
      "https://fr.wikipedia.org/wiki/Liste_des_%C3%A9pisodes_de_Naruto_Shippuden\n",
      "https://hr.wikipedia.org/wiki/Dodatak:Popis_Naruto_Shippuden_epizoda\n",
      "https://hu.wikipedia.org/wiki/A_Naruto_sipp%C3%BAden_epiz%C3%B3djainak_list%C3%A1ja\n",
      "https://id.wikipedia.org/wiki/Daftar_episode_Naruto:_Shippuden\n",
      "https://it.wikipedia.org/wiki/Episodi_di_Naruto:_Shippuden\n",
      "https://ko.wikipedia.org/wiki/%EB%82%98%EB%A3%A8%ED%86%A0_%EC%A7%88%ED%92%8D%EC%A0%84%EC%9D%98_%EC%97%90%ED%94%BC%EC%86%8C%EB%93%9C_%EB%AA%A9%EB%A1%9D\n",
      "https://pl.wikipedia.org/wiki/Lista_odcink%C3%B3w_serialu_anime_Naruto_Shipp%C5%ABden\n",
      "https://pt.wikipedia.org/wiki/Lista_de_epis%C3%B3dios_de_Naruto_Shippuden\n",
      "https://ro.wikipedia.org/wiki/Lista_episoadelor_din_Naruto:_Shippuden\n",
      "https://sr.wikipedia.org/wiki/Spisak_epizoda_serije_Naruto_%C5%A0ipuden\n",
      "https://th.wikipedia.org/wiki/%E0%B8%A3%E0%B8%B2%E0%B8%A2%E0%B8%8A%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B8%95%E0%B8%AD%E0%B8%99%E0%B9%83%E0%B8%99%E0%B8%99%E0%B8%B2%E0%B8%A3%E0%B8%B9%E0%B9%82%E0%B8%95%E0%B8%B0_%E0%B8%95%E0%B8%B3%E0%B8%99%E0%B8%B2%E0%B8%99%E0%B8%A7%E0%B8%B2%E0%B8%A2%E0%B8%B8%E0%B8%AA%E0%B8%A5%E0%B8%B2%E0%B8%95%E0%B8%B1%E0%B8%99\n",
      "https://tr.wikipedia.org/wiki/Naruto:_Shippuuden_b%C3%B6l%C3%BCmleri_listesi\n",
      "https://vi.wikipedia.org/wiki/Danh_s%C3%A1ch_t%E1%BA%ADp_phim_Naruto_Shippuden\n",
      "https://zh.wikipedia.org/wiki/%E7%81%AB%E5%BD%B1%E5%BF%8D%E8%80%85%E7%96%BE%E9%A2%A8%E5%82%B3%E5%8B%95%E7%95%AB%E9%9B%86%E6%95%B8%E5%88%97%E8%A1%A8_(%E5%89%8D%E6%9C%9F)\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q21882811#sitelinks-wikipedia\n",
      "/wiki/Anexo:Episodios_de_Naruto:_Shippuden\n",
      "/wiki/Anexo_discusi%C3%B3n:Episodios_de_Naruto:_Shippuden\n",
      "/wiki/Anexo:Episodios_de_Naruto:_Shippuden\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=history\n",
      "/wiki/Anexo:Episodios_de_Naruto:_Shippuden\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=history\n",
      "/wiki/Especial:LoQueEnlazaAqu%C3%AD/Anexo:Episodios_de_Naruto:_Shippuden\n",
      "/wiki/Especial:CambiosEnEnlazadas/Anexo:Episodios_de_Naruto:_Shippuden\n",
      "//commons.wikimedia.org/wiki/Special:UploadWizard?uselang=es\n",
      "/wiki/Especial:P%C3%A1ginasEspeciales\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&oldid=151022104\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=info\n",
      "/w/index.php?title=Especial:Citar&page=Anexo%3AEpisodios_de_Naruto%3A_Shippuden&id=151022104&wpFormIdentifier=titleform\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q21882811\n",
      "/w/index.php?title=Especial:Libro&bookcmd=book_creator&referer=Anexo%3AEpisodios+de+Naruto%3A+Shippuden\n",
      "/w/index.php?title=Especial:DownloadAsPdf&page=Anexo%3AEpisodios_de_Naruto%3A_Shippuden&action=show-download-screen\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&printable=yes\n",
      "/wiki/Anexo:Episodios_de_Naruto\n",
      "/wiki/Naruto:_Shipp%C5%ABden\n",
      "/wiki/Anime\n",
      "/wiki/Anexo:Vol%C3%BAmenes_de_Naruto#Segunda_parte\n",
      "/wiki/Manga\n",
      "/wiki/Masashi_Kishimoto\n",
      "/wiki/Anexo:Vol%C3%BAmenes_de_Naruto#Primera_parte\n",
      "/wiki/Naruto\n",
      "/wiki/Pierrot\n",
      "/wiki/TV_Tokyo\n",
      "/wiki/TV_Tokyo\n",
      "#cite_note-1\n",
      "#cite_note-2\n",
      "#Temporada_1_(2007)\n",
      "#Temporada_2_(2007‚Äì08)\n",
      "#Temporada_3_(2008)\n",
      "#Temporada_4_(2008)\n",
      "#Temporada_5_(2008‚Äì09)\n",
      "#Temporada_6_(2009‚Äì10)\n",
      "#Temporada_7_(2010)\n",
      "#Temporada_8_(2010)\n",
      "#Temporada_9_(2010‚Äì11)\n",
      "#Temporada_10_(2011)\n",
      "#Temporada_11_(2011)\n",
      "#Temporada_12_(2012)\n",
      "#Temporada_13_(2012‚Äì13)\n",
      "#Temporada_14_(2013)\n",
      "#Temporada_15_(2013‚Äì14)\n",
      "#Temporada_16_(2014)\n",
      "#Temporada_17_(2014)\n",
      "#Temporada_18_(2014)\n",
      "#Temporada_19_(2015)\n",
      "#Temporada_20_(2015‚Äì16)\n",
      "#Temporada_21_(2016‚Äì17)\n",
      "#Lanzamiento_en_DVD\n",
      "#Referencias\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=1\n",
      "#Temporada_1_(2007)\n",
      "#Temporada_2_(2007‚Äì08)\n",
      "#Temporada_3_(2008)\n",
      "#Temporada_4_(2008)\n",
      "#Temporada_5_(2008‚Äì09)\n",
      "#Temporada_6_(2009‚Äì10)\n",
      "#Temporada_7_(2010)\n",
      "#Temporada_8_(2010)\n",
      "#Temporada_9_(2010‚Äì11)\n",
      "#Temporada_10_(2011)\n",
      "#Temporada_11_(2011)\n",
      "#Temporada_12_(2012)\n",
      "#Temporada_13_(2012‚Äì13)\n",
      "#Temporada_14_(2013)\n",
      "#Temporada_15_(2013‚Äì14)\n",
      "#Temporada_16_(2014)\n",
      "#Temporada_17_(2014)\n",
      "#Temporada_18_(2014)\n",
      "#Temporada_19_(2015)\n",
      "#Temporada_20_(2015‚Äì16)\n",
      "#Temporada_21_(2016)\n",
      "#Temporada_22_(2016‚Äì17)\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=2\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=3\n",
      "/wiki/Anexo:Primera_temporada_de_Naruto:_Shippuden#Episodios\n",
      "https://es.wikipedia.org/w/index.php?title=Anexo:Primera_temporada_de_Naruto:_Shippuden&action=edit\n",
      "#cite_note-Anexo:Primera_temporada_de_Naruto:_Shippuden_episodes-3\n",
      "#cite_note-Anexo:Primera_temporada_de_Naruto:_Shippuden_episodes-3\n",
      "#cite_note-4\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=4\n",
      "#cite_note-episodes-5\n",
      "#cite_note-episodes-5\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=5\n",
      "#cite_note-episodes-5\n",
      "#cite_note-episodes-5\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=6\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=7\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=8\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=9\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=10\n",
      "#cite_note-arcos-6\n",
      "#cite_note-7\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=11\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=12\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=13\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=14\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=15\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=16\n",
      "#cite_note-arcos-6\n",
      "/wiki/Road_to_Ninja:_Naruto_the_Movie\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=17\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=18\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=19\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=20\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=21\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=22\n",
      "#cite_note-arcos-6\n",
      "#cite_note-arcos-6\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=23\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=24\n",
      "#cite_note-arcos-6\n",
      "#cite_note-arcos-6\n",
      "#cite_note-arcos-6\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=25\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=26\n",
      "#cite_note-series1title-8\n",
      "#cite_note-naruto01-9\n",
      "#cite_note-naruto01-9\n",
      "#cite_note-naruto01-9\n",
      "#cite_note-naruto01-9\n",
      "#cite_note-naruto01-9\n",
      "#cite_note-naruto01-9\n",
      "#cite_note-naruto01-9\n",
      "#cite_note-naruto01-9\n",
      "#cite_note-series2title-10\n",
      "#cite_note-naruto02-11\n",
      "#cite_note-naruto02-11\n",
      "#cite_note-naruto02-11\n",
      "#cite_note-naruto02-11\n",
      "#cite_note-naruto02-11\n",
      "#cite_note-series3title3-12\n",
      "#cite_note-naruto03-13\n",
      "#cite_note-naruto03-13\n",
      "#cite_note-naruto03-13\n",
      "#cite_note-naruto03-13\n",
      "#cite_note-series4titl4e-14\n",
      "#cite_note-naruto04-15\n",
      "#cite_note-naruto04-15\n",
      "#cite_note-naruto04-15\n",
      "#cite_note-naruto04-15\n",
      "#cite_note-naruto05-16\n",
      "#cite_note-naruto05-16\n",
      "#cite_note-naruto05-16\n",
      "#cite_note-naruto05-16\n",
      "#cite_note-naruto05-16\n",
      "#cite_note-naruto05-16\n",
      "#cite_note-gaidentitle-17\n",
      "#cite_note-naruto06-18\n",
      "#cite_note-naruto06-18\n",
      "#cite_note-naruto06-18\n",
      "#cite_note-naruto06-18\n",
      "#cite_note-naruto06-18\n",
      "#cite_note-naruto06-18\n",
      "#cite_note-naruto06-18\n",
      "#cite_note-naruto07-19\n",
      "#cite_note-naruto07-19\n",
      "#cite_note-naruto11-20\n",
      "#cite_note-naruto11-20\n",
      "#cite_note-naruto11-20\n",
      "#cite_note-naruto11-20\n",
      "#cite_note-naruto11-20\n",
      "#cite_note-narutospecial-21\n",
      "#cite_note-narutospecial-21\n",
      "#cite_note-naruto12-22\n",
      "#cite_note-naruto12-22\n",
      "#cite_note-naruto12-22\n",
      "#cite_note-naruto12-22\n",
      "#cite_note-naruto12-22\n",
      "#cite_note-naruto12-22\n",
      "#cite_note-naruto13-23\n",
      "#cite_note-naruto13-23\n",
      "#cite_note-naruto13-23\n",
      "#cite_note-naruto14-24\n",
      "#cite_note-naruto14-24\n",
      "#cite_note-naruto15-25\n",
      "#cite_note-naruto15-25\n",
      "#cite_note-naruto15-25\n",
      "#cite_note-naruto15-25\n",
      "#cite_note-naruto15-25\n",
      "#cite_note-naruto15-25\n",
      "#cite_note-naruto16-26\n",
      "#cite_note-naruto16-26\n",
      "#cite_note-naruto16-26\n",
      "#cite_note-naruto16-26\n",
      "#cite_note-naruto16-26\n",
      "#cite_note-naruto16-26\n",
      "#cite_note-naruto16-26\n",
      "#cite_note-naruto17-27\n",
      "#cite_note-naruto17-27\n",
      "#cite_note-naruto17-27\n",
      "#cite_note-naruto18-28\n",
      "#cite_note-naruto18-28\n",
      "#cite_note-naruto18-28\n",
      "#cite_note-naruto19-29\n",
      "#cite_note-naruto19-29\n",
      "#cite_note-naruto19-29\n",
      "#cite_note-naruto19-29\n",
      "#cite_note-naruto19-29\n",
      "#cite_note-30\n",
      "#cite_note-31\n",
      "#cite_note-32\n",
      "#cite_note-33\n",
      "#cite_note-34\n",
      "#cite_note-35\n",
      "#cite_note-36\n",
      "#cite_note-37\n",
      "#cite_note-38\n",
      "#cite_note-39\n",
      "#cite_note-40\n",
      "#cite_note-41\n",
      "#cite_note-42\n",
      "#cite_note-43\n",
      "#cite_note-44\n",
      "#cite_note-45\n",
      "#cite_note-46\n",
      "#cite_note-47\n",
      "#cite_note-48\n",
      "#cite_note-49\n",
      "#cite_note-50\n",
      "#cite_note-51\n",
      "#cite_note-52\n",
      "#cite_note-53\n",
      "#cite_note-54\n",
      "#cite_note-55\n",
      "#cite_note-56\n",
      "/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&action=edit&section=27\n",
      "#cite_ref-1\n",
      "https://web.archive.org/web/20161001190711/http://mediaarts-db.jp/an/anime_series/9886\n",
      "http://mediaarts-db.jp/an/anime_series/9886\n",
      "#cite_ref-2\n",
      "https://web.archive.org/web/20170323123510/http://www.animenewsnetwork.com/news/2017-03-23/naruto-shippuden-anime-ending-on-500th-episode-confirmed/.113787\n",
      "http://www.animenewsnetwork.com/news/2017-03-23/naruto-shippuden-anime-ending-on-500th-episode-confirmed/.113787\n",
      "#cite_ref-Anexo:Primera_temporada_de_Naruto:_Shippuden_episodes_3-0\n",
      "#cite_ref-Anexo:Primera_temporada_de_Naruto:_Shippuden_episodes_3-1\n",
      "https://www.animenewsnetwork.com/encyclopedia/anime.php?id=7293&page=25\n",
      "/wiki/AnimeNewsNetwork\n",
      "#cite_ref-4\n",
      "https://www.animedatos.com/2019/02/capitulos-naruto-shippuden-sin-relleno.html\n",
      "#cite_ref-episodes_5-0\n",
      "#cite_ref-episodes_5-1\n",
      "#cite_ref-episodes_5-2\n",
      "#cite_ref-episodes_5-3\n",
      "https://www.animenewsnetwork.com/encyclopedia/anime.php?id=7293&page=25\n",
      "/wiki/AnimeNewsNetwork\n",
      "#cite_ref-arcos_6-0\n",
      "#cite_ref-arcos_6-1\n",
      "#cite_ref-arcos_6-2\n",
      "#cite_ref-arcos_6-3\n",
      "#cite_ref-arcos_6-4\n",
      "#cite_ref-arcos_6-5\n",
      "#cite_ref-arcos_6-6\n",
      "#cite_ref-arcos_6-7\n",
      "#cite_ref-arcos_6-8\n",
      "#cite_ref-arcos_6-9\n",
      "#cite_ref-arcos_6-10\n",
      "#cite_ref-arcos_6-11\n",
      "#cite_ref-arcos_6-12\n",
      "#cite_ref-arcos_6-13\n",
      "#cite_ref-arcos_6-14\n",
      "#cite_ref-arcos_6-15\n",
      "#cite_ref-arcos_6-16\n",
      "#cite_ref-arcos_6-17\n",
      "#cite_ref-arcos_6-18\n",
      "#cite_ref-arcos_6-19\n",
      "#cite_ref-arcos_6-20\n",
      "#cite_ref-arcos_6-21\n",
      "#cite_ref-arcos_6-22\n",
      "#cite_ref-arcos_6-23\n",
      "#cite_ref-arcos_6-24\n",
      "https://www.animedatos.com/2019/02/capitulos-naruto-shippuden-sin-relleno.html\n",
      "#cite_ref-7\n",
      "https://www.webcitation.org/query?url=http%3A%2F%2Fwww.tv-tokyo.co.jp%2Fanime%2Fnaruto%2Fepisodes.html&date=2009-12-26\n",
      "http://www.tv-tokyo.co.jp/anime/naruto/episodes.html\n",
      "#cite_ref-series1title_8-0\n",
      "https://web.archive.org/web/20110615162641/http://www.play-asia.com/paOS-13-71-a7-49-en-70-22dk.html\n",
      "https://www.play-asia.com/paOS-13-71-a7-49-en-70-22dk.html\n",
      "#cite_ref-naruto01_9-0\n",
      "#cite_ref-naruto01_9-1\n",
      "#cite_ref-naruto01_9-2\n",
      "#cite_ref-naruto01_9-3\n",
      "#cite_ref-naruto01_9-4\n",
      "#cite_ref-naruto01_9-5\n",
      "#cite_ref-naruto01_9-6\n",
      "#cite_ref-naruto01_9-7\n",
      "https://web.archive.org/web/20160808214230/http://www.aniplex.co.jp/naruto/tv/01.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/01.html\n",
      "#cite_ref-series2title_10-0\n",
      "https://web.archive.org/web/20110615163132/http://www.play-asia.com/paOS-13-71-a7-77-4-49-en-70-2jxr.html\n",
      "https://www.play-asia.com/paOS-13-71-a7-77-4-49-en-70-2jxr.html\n",
      "#cite_ref-naruto02_11-0\n",
      "#cite_ref-naruto02_11-1\n",
      "#cite_ref-naruto02_11-2\n",
      "#cite_ref-naruto02_11-3\n",
      "#cite_ref-naruto02_11-4\n",
      "https://web.archive.org/web/20160812191623/http://www.aniplex.co.jp/naruto/tv/02.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/02.html\n",
      "#cite_ref-series3title3_12-0\n",
      "https://web.archive.org/web/20080918101537/http://www.sonymusicshop.jp/detail.asp?goods=ANSB000002671\n",
      "/wiki/Aniplex\n",
      "http://www.sonymusicshop.jp/detail.asp?goods=ANSB000002671\n",
      "#cite_ref-naruto03_13-0\n",
      "#cite_ref-naruto03_13-1\n",
      "#cite_ref-naruto03_13-2\n",
      "#cite_ref-naruto03_13-3\n",
      "https://web.archive.org/web/20160802233942/http://www.aniplex.co.jp/naruto/tv/03.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/03.html\n",
      "#cite_ref-series4titl4e_14-0\n",
      "https://web.archive.org/web/20081015075958/http://www.neowing.co.jp/detailview.html?KEY=ANSB-2681\n",
      "http://www.neowing.co.jp/detailview.html?KEY=ANSB-2681\n",
      "#cite_ref-naruto04_15-0\n",
      "#cite_ref-naruto04_15-1\n",
      "#cite_ref-naruto04_15-2\n",
      "#cite_ref-naruto04_15-3\n",
      "https://web.archive.org/web/20160812193023/http://www.aniplex.co.jp/naruto/tv/04.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/04.html\n",
      "#cite_ref-naruto05_16-0\n",
      "#cite_ref-naruto05_16-1\n",
      "#cite_ref-naruto05_16-2\n",
      "#cite_ref-naruto05_16-3\n",
      "#cite_ref-naruto05_16-4\n",
      "#cite_ref-naruto05_16-5\n",
      "https://web.archive.org/web/20160713214602/http://www.aniplex.co.jp/naruto/tv/05.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/05.html\n",
      "#cite_ref-gaidentitle_17-0\n",
      "https://web.archive.org/web/20110511164957/http://www.neowing.co.jp/detailview.html?KEY=ANZB-3381\n",
      "http://www.neowing.co.jp/detailview.html?KEY=ANZB-3381\n",
      "#cite_ref-naruto06_18-0\n",
      "#cite_ref-naruto06_18-1\n",
      "#cite_ref-naruto06_18-2\n",
      "#cite_ref-naruto06_18-3\n",
      "#cite_ref-naruto06_18-4\n",
      "#cite_ref-naruto06_18-5\n",
      "#cite_ref-naruto06_18-6\n",
      "https://web.archive.org/web/20160712233536/http://www.aniplex.co.jp/naruto/tv/06.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/06.html\n",
      "#cite_ref-naruto07_19-0\n",
      "#cite_ref-naruto07_19-1\n",
      "https://web.archive.org/web/20160713215010/http://www.aniplex.co.jp/naruto/tv/07.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/07.html\n",
      "#cite_ref-naruto11_20-0\n",
      "#cite_ref-naruto11_20-1\n",
      "#cite_ref-naruto11_20-2\n",
      "#cite_ref-naruto11_20-3\n",
      "#cite_ref-naruto11_20-4\n",
      "https://web.archive.org/web/20160713215610/http://www.aniplex.co.jp/naruto/tv/11.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/11.html\n",
      "#cite_ref-narutospecial_21-0\n",
      "#cite_ref-narutospecial_21-1\n",
      "https://web.archive.org/web/20170302185417/http://www.aniplex.co.jp/naruto/tv/special.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/special.html\n",
      "#cite_ref-naruto12_22-0\n",
      "#cite_ref-naruto12_22-1\n",
      "#cite_ref-naruto12_22-2\n",
      "#cite_ref-naruto12_22-3\n",
      "#cite_ref-naruto12_22-4\n",
      "#cite_ref-naruto12_22-5\n",
      "https://web.archive.org/web/20160713214615/http://www.aniplex.co.jp/naruto/tv/12.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/12.html\n",
      "#cite_ref-naruto13_23-0\n",
      "#cite_ref-naruto13_23-1\n",
      "#cite_ref-naruto13_23-2\n",
      "https://web.archive.org/web/20160713220029/http://www.aniplex.co.jp/naruto/tv/13.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/13.html\n",
      "#cite_ref-naruto14_24-0\n",
      "#cite_ref-naruto14_24-1\n",
      "https://web.archive.org/web/20160704224020/http://www.aniplex.co.jp/naruto/tv/14.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/14.html\n",
      "#cite_ref-naruto15_25-0\n",
      "#cite_ref-naruto15_25-1\n",
      "#cite_ref-naruto15_25-2\n",
      "#cite_ref-naruto15_25-3\n",
      "#cite_ref-naruto15_25-4\n",
      "#cite_ref-naruto15_25-5\n",
      "https://web.archive.org/web/20160713214135/http://www.aniplex.co.jp/naruto/tv/15.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/15.html\n",
      "#cite_ref-naruto16_26-0\n",
      "#cite_ref-naruto16_26-1\n",
      "#cite_ref-naruto16_26-2\n",
      "#cite_ref-naruto16_26-3\n",
      "#cite_ref-naruto16_26-4\n",
      "#cite_ref-naruto16_26-5\n",
      "#cite_ref-naruto16_26-6\n",
      "https://web.archive.org/web/20160712112722/http://www.aniplex.co.jp/naruto/tv/16.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/16.html\n",
      "#cite_ref-naruto17_27-0\n",
      "#cite_ref-naruto17_27-1\n",
      "#cite_ref-naruto17_27-2\n",
      "https://web.archive.org/web/20160713214150/http://www.aniplex.co.jp/naruto/tv/17.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/17.html\n",
      "#cite_ref-naruto18_28-0\n",
      "#cite_ref-naruto18_28-1\n",
      "#cite_ref-naruto18_28-2\n",
      "https://web.archive.org/web/20160805043439/http://www.aniplex.co.jp/naruto/tv/18.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/18.html\n",
      "#cite_ref-naruto19_29-0\n",
      "#cite_ref-naruto19_29-1\n",
      "#cite_ref-naruto19_29-2\n",
      "#cite_ref-naruto19_29-3\n",
      "#cite_ref-naruto19_29-4\n",
      "https://web.archive.org/web/20160812201226/http://www.aniplex.co.jp/naruto/tv/19.html\n",
      "/wiki/Aniplex\n",
      "http://www.aniplex.co.jp/naruto/tv/19.html\n",
      "#cite_ref-30\n",
      "https://web.archive.org/web/20170302190048/http://www.neowing.co.jp/product/ANSB-12801\n",
      "http://www.neowing.co.jp/product/ANSB-12801\n",
      "#cite_ref-31\n",
      "https://web.archive.org/web/20170302190043/http://www.neowing.co.jp/product/ANSB-12802\n",
      "http://www.neowing.co.jp/product/ANSB-12802\n",
      "#cite_ref-32\n",
      "https://web.archive.org/web/20170302190051/http://www.neowing.co.jp/product/ANSB-12803\n",
      "http://www.neowing.co.jp/product/ANSB-12803\n",
      "#cite_ref-33\n",
      "https://web.archive.org/web/20170302190047/http://www.neowing.co.jp/product/ANSB-12804\n",
      "http://www.neowing.co.jp/product/ANSB-12804\n",
      "#cite_ref-34\n",
      "https://web.archive.org/web/20170302190853/http://www.neowing.co.jp/product/ANSB-12805\n",
      "http://www.neowing.co.jp/product/ANSB-12805\n",
      "#cite_ref-35\n",
      "https://web.archive.org/web/20170302190056/http://www.neowing.co.jp/product/ANSB-12806\n",
      "http://www.neowing.co.jp/product/ANSB-12806\n",
      "#cite_ref-36\n",
      "https://web.archive.org/web/20170302190849/http://www.neowing.co.jp/product/ANSB-12807\n",
      "http://www.neowing.co.jp/product/ANSB-12807\n",
      "#cite_ref-37\n",
      "https://web.archive.org/web/20170302190841/http://www.neowing.co.jp/product/ANSB-12808\n",
      "http://www.neowing.co.jp/product/ANSB-12808\n",
      "#cite_ref-38\n",
      "https://web.archive.org/web/20160225212120/http://www.neowing.co.jp/product/ANSB-12809\n",
      "http://www.neowing.co.jp/product/ANSB-12809\n",
      "#cite_ref-39\n",
      "https://web.archive.org/web/20170302191438/http://www.neowing.co.jp/product/ANSB-12810\n",
      "http://www.neowing.co.jp/product/ANSB-12810\n",
      "#cite_ref-40\n",
      "https://web.archive.org/web/20170302190837/http://www.neowing.co.jp/product/ANSB-12811\n",
      "http://www.neowing.co.jp/product/ANSB-12811\n",
      "#cite_ref-41\n",
      "https://web.archive.org/web/20170302190831/http://www.neowing.co.jp/product/ANSB-12812\n",
      "http://www.neowing.co.jp/product/ANSB-12812\n",
      "#cite_ref-42\n",
      "https://web.archive.org/web/20170302191441/http://www.neowing.co.jp/product/ANSB-12813\n",
      "http://www.neowing.co.jp/product/ANSB-12813\n",
      "#cite_ref-43\n",
      "https://web.archive.org/web/20170302191445/http://www.neowing.co.jp/product/ANSB-12814\n",
      "http://www.neowing.co.jp/product/ANSB-12814\n",
      "#cite_ref-44\n",
      "https://web.archive.org/web/20170302191449/http://www.neowing.co.jp/product/ANSB-12815\n",
      "http://www.neowing.co.jp/product/ANSB-12815\n",
      "#cite_ref-45\n",
      "https://web.archive.org/web/20170302191451/http://www.neowing.co.jp/product/ANSB-12816\n",
      "http://www.neowing.co.jp/product/ANSB-12816\n",
      "#cite_ref-46\n",
      "https://web.archive.org/web/20170317210809/http://www.neowing.co.jp/product/ANSB-12817\n",
      "http://www.neowing.co.jp/product/ANSB-12817\n",
      "#cite_ref-47\n",
      "https://web.archive.org/web/20170619021131/http://www.neowing.co.jp/product/ANSB-12818\n",
      "http://www.neowing.co.jp/product/ANSB-12818\n",
      "#cite_ref-48\n",
      "https://web.archive.org/web/20170619021131/http://www.neowing.co.jp/product/ANSB-12819\n",
      "http://www.neowing.co.jp/product/ANSB-12819\n",
      "#cite_ref-49\n",
      "https://web.archive.org/web/20170619021131/http://www.neowing.co.jp/product/ANSB-12820\n",
      "http://www.neowing.co.jp/product/ANSB-12820\n",
      "#cite_ref-50\n",
      "https://web.archive.org/web/20170619021131/http://www.neowing.co.jp/product/ANSB-12821\n",
      "http://www.neowing.co.jp/product/ANSB-12821\n",
      "#cite_ref-51\n",
      "https://web.archive.org/web/20170619021131/http://www.neowing.co.jp/product/ANSB-12822\n",
      "http://www.neowing.co.jp/product/ANSB-12822\n",
      "#cite_ref-52\n",
      "https://web.archive.org/web/20170619021131/http://www.neowing.co.jp/product/ANSB-12823\n",
      "http://www.neowing.co.jp/product/ANSB-12823\n",
      "#cite_ref-53\n",
      "http://www.neowing.co.jp/product/ANSB-12824\n",
      "#cite_ref-54\n",
      "http://www.neowing.co.jp/product/ANSB-12825\n",
      "#cite_ref-55\n",
      "http://www.neowing.co.jp/product/ANSB-12826\n",
      "#cite_ref-56\n",
      "http://www.neowing.co.jp/product/ANSB-12827\n",
      "https://es.wikipedia.org/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&oldid=151022104\n",
      "/wiki/Especial:Categor%C3%ADas\n",
      "/wiki/Categor%C3%ADa:Anexos:Naruto\n",
      "/wiki/Categor%C3%ADa:Anexos:Episodios_de_anime\n",
      "/wiki/Categor%C3%ADa:Wikipedia:Anexos_con_extractos\n",
      "https://creativecommons.org/licenses/by-sa/4.0/deed.es\n",
      "http://creativecommons.org/licenses/by-sa/4.0/deed.es\n",
      "//foundation.wikimedia.org/wiki/Terms_of_Use\n",
      "//foundation.wikimedia.org/wiki/Privacy_policy\n",
      "//www.wikimediafoundation.org/\n",
      "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy/es\n",
      "/wiki/Wikipedia:Acerca_de\n",
      "/wiki/Wikipedia:Limitaci%C3%B3n_general_de_responsabilidad\n",
      "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Universal_Code_of_Conduct\n",
      "//es.m.wikipedia.org/w/index.php?title=Anexo:Episodios_de_Naruto:_Shippuden&mobileaction=toggle_view_mobile\n",
      "https://developer.wikimedia.org\n",
      "https://stats.wikimedia.org/#/es.wikipedia.org\n",
      "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement/es\n",
      "https://wikimediafoundation.org/\n",
      "https://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all(\"a\")\n",
    "for link in links:\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejemplo imprime los valores del atributo \"`href`\" de todos los elementos de anclaje (`<a>`) encontrados en la p√°gina.\n",
    "\n",
    "5. **Procesar y utilizar los datos extra√≠dos:**\n",
    "\n",
    "Una vez que hayamos extra√≠do los datos de la p√°gina web, podemos procesarlos seg√∫n nuestras necesidades. Esto puede incluir la limpieza de los datos, su transformaci√≥n a un formato deseado o su almacenamiento en una base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links:\n",
    "    clean_link = link.get(\"href\").strip()\n",
    "    # Realizar acciones adicionales con el enlace extra√≠do\n",
    "    # ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se realiza una limpieza adicional eliminando los espacios en blanco alrededor del enlace extra√≠do antes de realizar cualquier acci√≥n adicional.\n",
    "\n",
    "Recuerda que al realizar Web Scraping, es importante seguir las buenas pr√°cticas √©ticas y respetar las pol√≠ticas del sitio web objetivo. Aseg√∫rate de revisar los t√©rminos de servicio y las restricciones de Web Scraping del sitio web antes de extraer datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Requests y lxml (Extracci√≥n de WIKIPEDIA)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la extracci√≥n de datos de Wikipedia utilizando las bibliotecas `requests` y `lxml`, necesitar√°s enviar una solicitud HTTP a la p√°gina de Wikipedia que deseas extraer y luego analizar el contenido HTML para obtener la informaci√≥n relevante.\n",
    "\n",
    "Aqu√≠ tienes una explicaci√≥n detallada con ejemplos de c√≥mo utilizar `requests` y `lxml` para extraer datos de Wikipedia:\n",
    "\n",
    "1. **Importar las bibliotecas necesarias:**\n",
    "\n",
    "Comienza importando las bibliotecas `requests` y `lxml`. `Requests` se utiliza para enviar solicitudes `HTTP` y `lxml` para analizar y extraer datos del contenido `HTML`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Enviar una solicitud HTTP y obtener el contenido de la p√°gina:**\n",
    "\n",
    "Utiliza la funci√≥n `get()` de `requests` para enviar una solicitud `GET` a la URL de la p√°gina de Wikipedia que deseas extraer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://es.wikipedia.org/wiki/Python\"\n",
    "response = requests.get(url)\n",
    "content = response.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, utilizamos la p√°gina de Wikipedia en espa√±ol sobre Python como ejemplo. Aseg√∫rate de reemplazar \"`https://es.wikipedia.org/wiki/Python`\" con la URL de la p√°gina de Wikipedia que deseas extraer.\n",
    "\n",
    "3. **Analizar el contenido HTML:**\n",
    "\n",
    "Utiliza la clase `etree.HTML()` de `lxml` para analizar el contenido `HTML` y crear un objeto `ElementTree` que representa la estructura del documento `HTML`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = etree.HTML(content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Utilizar XPath para extraer datos:**\n",
    "\n",
    "Utiliza `XPath` para seleccionar y extraer los datos espec√≠ficos que deseas obtener de la p√°gina de Wikipedia. Puedes utilizar la funci√≥n `xpath()` del objeto `ElementTree` para aplicar expresiones `XPath` y seleccionar los nodos deseados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontr√≥ el t√≠tulo.\n",
      "Primer p√°rrafo:  es un \n"
     ]
    }
   ],
   "source": [
    "# Extraer el t√≠tulo de la p√°gina\n",
    "title = tree.xpath(\"//h1[@id='firstHeading']/text()\")\n",
    "if title:\n",
    "    print(\"T√≠tulo:\", title[0])\n",
    "else:\n",
    "    print(\"No se encontr√≥ el t√≠tulo.\")\n",
    "\n",
    "# Extraer el contenido del primer p√°rrafo\n",
    "first_paragraph = tree.xpath(\"//div[@id='mw-content-text']//p[1]/text()\")\n",
    "if first_paragraph:\n",
    "    print(\"Primer p√°rrafo:\", first_paragraph[0])\n",
    "else:\n",
    "    print(\"No se encontr√≥ el primer p√°rrafo.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, utilizamos `XPath` para seleccionar el t√≠tulo de la p√°gina (que se encuentra dentro de un elemento `<h1>` con el atributo `id='firstHeading'`) y el contenido del primer p√°rrafo (que se encuentra dentro de un elemento `<div>` con el atributo `id='mw-content-text'` y cualquier elemento `<p>` hijo en la posici√≥n `1`).\n",
    "\n",
    "5. **Procesar y utilizar los datos extra√≠dos:**\n",
    "\n",
    "Una vez que hayas extra√≠do los datos de Wikipedia, puedes procesarlos seg√∫n tus necesidades y realizar las acciones adicionales deseadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar acciones adicionales con los datos extra√≠dos\n",
    "# ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, puedes realizar cualquier procesamiento adicional en los datos extra√≠dos, como limpiarlos, transformarlos o almacenarlos en una base de datos, seg√∫n tus requisitos espec√≠ficos.\n",
    "\n",
    "Recuerda que al realizar Web Scraping en Wikipedia u otros sitios web, es importante cumplir con las pol√≠ticas de uso y los t√©rminos de servicio del sitio. Aseg√∫rate de revisar y respetar cualquier restricci√≥n o pol√≠tica aplicable antes de realizar extracciones de datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Requests y Beautiful Soup (Extracci√≥n de STACKOVERFLOW)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la extracci√≥n de datos de Stack Overflow utilizando las bibliotecas `requests` y `BeautifulSoup`, necesitar√°s enviar una solicitud HTTP a la p√°gina de Stack Overflow que deseas extraer y luego utilizar `BeautifulSoup` para analizar y extraer los datos relevantes.\n",
    "\n",
    "Aqu√≠ tienes una explicaci√≥n detallada con ejemplos de c√≥mo utilizar `requests` y `BeautifulSoup` para extraer datos de Stack Overflow:\n",
    "\n",
    "1. **Importar las bibliotecas necesarias:**\n",
    "\n",
    "Comienza importando las bibliotecas `requests` y `BeautifulSoup`. `Requests` se utiliza para enviar solicitudes `HTTP` y `BeautifulSoup` para analizar y extraer datos del contenido `HTML`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Enviar una solicitud HTTP y obtener el contenido de la p√°gina:**\n",
    "\n",
    "Utiliza la funci√≥n `get()` de `requests` para enviar una solicitud `GET` a la URL de la p√°gina de Stack Overflow que deseas extraer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://stackoverflow.com/questions/tagged/python\"\n",
    "response = requests.get(url)\n",
    "content = response.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, utilizamos la p√°gina de preguntas con la etiqueta \"`python`\" en Stack Overflow como ejemplo. Aseg√∫rate de reemplazar \"`https://stackoverflow.com/questions/tagged/python`\" con la URL de la p√°gina de Stack Overflow que deseas extraer.\n",
    "\n",
    "3. **Analizar el contenido HTML:**\n",
    "\n",
    "Utiliza `BeautifulSoup` para analizar el contenido `HTML` y crear un objeto `soup` que representa la estructura del documento `HTML`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(content, \"html.parser\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Utilizar m√©todos de BeautifulSoup para extraer datos:**\n",
    "\n",
    "Utiliza los m√©todos y selectores proporcionados por `BeautifulSoup` para seleccionar y extraer los datos espec√≠ficos que deseas obtener de la p√°gina de Stack Overflow. Puedes utilizar m√©todos como `find_all()` o selectores `CSS` para seleccionar los elementos deseados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√≠tulo: \n",
      "                    Would electrically powered thrusters on JWST have made it last for decades longer?\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Geometry Nodes: Mesh curve index math\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Indeclinables: What are the strategies good Latinists use to deal with them?\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Why Is Hydrogen in HSO3- Connected to Oxygen Instead of Sulfur?\n",
      "                \n",
      "T√≠tulo: \n",
      "                    minting cNFTs with Solana Pay QR code\n",
      "                \n",
      "T√≠tulo: \n",
      "                    What adverb could I use before \"apologizing\" to mean \"a lot\"?\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Why is Putin translated as ÊôÆ‰∫¨ but not ÊôÆÂÆö?\n",
      "                \n",
      "T√≠tulo: \n",
      "                    A different way of saying \"rest of\"\n",
      "                \n",
      "T√≠tulo: \n",
      "                    One translation uses \"were used\" while the other uses \"could be used\"\n",
      "                \n",
      "T√≠tulo: \n",
      "                    If G is a connected Graph with a cycle, if you remove an edge from the cycle then G is still connected\n",
      "                \n",
      "T√≠tulo: \n",
      "                    How do I capitalize the first letter of a macro?\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Replacing a broken plastic switch for a range hood\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Is it ok that an oven circuit's wire is much larger than the oven's wire?\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Why are angles so weird?\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Do we need EN in this sentence?\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Should I agree to review a paper that I've already recommended that another journal reject?\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Do out-of-combat spells use spell slots?\n",
      "                \n",
      "T√≠tulo: \n",
      "                    How does battery make charge in conductor move?\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Discovered Phd topic has already been worked on\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Could we (Earth, Humanity, Solar System) be falling into a black hole?\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Gathered Swarm damage vs resistance and immunity\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Novel in which a derelict ship is observed entering the solar system, leading to an alien invasion\n",
      "                \n",
      "T√≠tulo: \n",
      "                    I want to copy files that do not have a number in their name\n",
      "                \n",
      "T√≠tulo: \n",
      "                    Last odd digit of power of 2\n",
      "                \n",
      "Etiqueta: python\n",
      "Etiqueta: canvas\n",
      "Etiqueta: python\n",
      "Etiqueta: sql\n",
      "Etiqueta: django-orm\n",
      "Etiqueta: python\n",
      "Etiqueta: macos\n",
      "Etiqueta: pytorch\n",
      "Etiqueta: python\n",
      "Etiqueta: pandas\n",
      "Etiqueta: python\n",
      "Etiqueta: jupyter-notebook\n",
      "Etiqueta: jupyter\n",
      "Etiqueta: folium\n",
      "Etiqueta: python\n",
      "Etiqueta: pyspark\n",
      "Etiqueta: apache-spark-sql\n",
      "Etiqueta: databricks\n",
      "Etiqueta: azure-databricks\n",
      "Etiqueta: python\n",
      "Etiqueta: apache-spark\n",
      "Etiqueta: pyspark\n",
      "Etiqueta: schema\n",
      "Etiqueta: python\n",
      "Etiqueta: python-3.x\n",
      "Etiqueta: string\n",
      "Etiqueta: string-length\n",
      "Etiqueta: python\n",
      "Etiqueta: image\n",
      "Etiqueta: user-interface\n",
      "Etiqueta: tkinter\n",
      "Etiqueta: python\n",
      "Etiqueta: pandas\n",
      "Etiqueta: excel\n",
      "Etiqueta: dataframe\n",
      "Etiqueta: split\n",
      "Etiqueta: python\n",
      "Etiqueta: pygame\n",
      "Etiqueta: typeerror\n",
      "Etiqueta: blit\n",
      "Etiqueta: python\n",
      "Etiqueta: automated-tests\n",
      "Etiqueta: pytest\n",
      "Etiqueta: parametrized-testing\n",
      "Etiqueta: parametrize\n",
      "Etiqueta: python\n",
      "Etiqueta: python\n",
      "Etiqueta: arrays\n",
      "Etiqueta: list\n",
      "Etiqueta: numpy\n",
      "Etiqueta: python\n",
      "Etiqueta: numpy\n",
      "Etiqueta: matrix\n",
      "Etiqueta: 3d\n",
      "Etiqueta: multiplication\n",
      "Etiqueta: python\n",
      "Etiqueta: image-processing\n",
      "Etiqueta: aws-lambda\n",
      "Etiqueta: python-imaging-library\n",
      "Etiqueta: image-resizing\n",
      "Etiqueta: python\n",
      "Etiqueta: python-3.x\n",
      "Etiqueta: count\n",
      "Etiqueta: python\n",
      "Etiqueta: numpy\n",
      "Etiqueta: python\n",
      "Etiqueta: postgresql\n",
      "Etiqueta: sqlalchemy\n",
      "Etiqueta: python\n",
      "Etiqueta: python-3.x\n",
      "Etiqueta: numpy\n",
      "Etiqueta: numpy-ndarray\n",
      "Etiqueta: python\n",
      "Etiqueta: python-3.x\n",
      "Etiqueta: simpy\n",
      "Etiqueta: python\n",
      "Etiqueta: visual-studio-code\n",
      "Etiqueta: pygame\n",
      "Etiqueta: python\n",
      "Etiqueta: html\n",
      "Etiqueta: flask\n",
      "Etiqueta: python\n",
      "Etiqueta: numpy\n",
      "Etiqueta: metpy\n",
      "Etiqueta: python\n",
      "Etiqueta: anaconda\n",
      "Etiqueta: spyder\n",
      "Etiqueta: python\n",
      "Etiqueta: selenium-webdriver\n",
      "Etiqueta: web-scraping\n",
      "Etiqueta: python\n",
      "Etiqueta: animation\n",
      "Etiqueta: ursina\n",
      "Etiqueta: rubiks-cube\n",
      "Etiqueta: python\n",
      "Etiqueta: kivy\n",
      "Etiqueta: kivymd\n",
      "Etiqueta: python\n",
      "Etiqueta: python\n",
      "Etiqueta: data-science\n",
      "Etiqueta: facebook-prophet\n",
      "Etiqueta: python\n",
      "Etiqueta: python\n",
      "Etiqueta: python-3.x\n",
      "Etiqueta: sockets\n",
      "Etiqueta: tcp\n",
      "Etiqueta: ip\n",
      "Etiqueta: python\n",
      "Etiqueta: python\n",
      "Etiqueta: numpy\n",
      "Etiqueta: matplotlib\n",
      "Etiqueta: python\n",
      "Etiqueta: oop\n",
      "Etiqueta: recursion\n",
      "Etiqueta: module\n",
      "Etiqueta: project\n",
      "Etiqueta: python\n",
      "Etiqueta: discord\n",
      "Etiqueta: discord.py\n",
      "Etiqueta: arguments\n",
      "Etiqueta: bots\n",
      "Etiqueta: python\n",
      "Etiqueta: python\n",
      "Etiqueta: simulation\n",
      "Etiqueta: gekko\n",
      "Etiqueta: arx\n",
      "Etiqueta: python\n",
      "Etiqueta: r\n",
      "Etiqueta: text-mining\n",
      "Etiqueta: non-relational-database\n",
      "Etiqueta: python\n",
      "Etiqueta: pandas\n",
      "Etiqueta: numpy\n",
      "Etiqueta: python\n",
      "Etiqueta: python-turtle\n",
      "Etiqueta: python\n",
      "Etiqueta: mongodb\n",
      "Etiqueta: jupyter-notebook\n",
      "Etiqueta: python\n",
      "Etiqueta: swig\n",
      "Etiqueta: memoryview\n",
      "Etiqueta: python\n",
      "Etiqueta: python\n",
      "Etiqueta: pandas\n",
      "Etiqueta: dataframe\n",
      "Etiqueta: python\n",
      "Etiqueta: pytest\n",
      "Etiqueta: ini\n",
      "Etiqueta: python\n",
      "Etiqueta: web-scraping\n",
      "Etiqueta: pycharm\n",
      "Etiqueta: python\n",
      "Etiqueta: list\n",
      "Etiqueta: integer\n",
      "Etiqueta: multiplication\n",
      "Etiqueta: python\n",
      "Etiqueta: postgresql\n",
      "Etiqueta: sqlalchemy\n",
      "Etiqueta: python\n",
      "Etiqueta: pandas\n",
      "Etiqueta: dataframe\n"
     ]
    }
   ],
   "source": [
    "# Extraer los t√≠tulos de las preguntas\n",
    "question_titles = soup.find_all(\"a\", class_=\"question-hyperlink\")\n",
    "for title in question_titles:\n",
    "    print(\"T√≠tulo:\", title.text)\n",
    "\n",
    "# Extraer las etiquetas de las preguntas\n",
    "question_tags = soup.select(\".tags .post-tag\")\n",
    "for tag in question_tags:\n",
    "    print(\"Etiqueta:\", tag.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, utilizamos `find_all()` para extraer los t√≠tulos de las preguntas, seleccionando los elementos `<a>` con la clase `question-hyperlink`. Tambi√©n utilizamos `select()` para extraer las etiquetas de las preguntas, utilizando el selector `CSS` `.tags` `.post-tag`.\n",
    "\n",
    "5. **Procesar y utilizar los datos extra√≠dos:**\n",
    "\n",
    "Una vez que hayas extra√≠do los datos de Stack Overflow, puedes procesarlos seg√∫n tus necesidades y realizar las acciones adicionales deseadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar acciones adicionales con los datos extra√≠dos\n",
    "# ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, puedes realizar cualquier procesamiento adicional en los datos extra√≠dos, como limpiarlos, transformarlos o almacenarlos en una base de datos, seg√∫n tus requisitos espec√≠ficos.\n",
    "\n",
    "Recuerda que al realizar Web Scraping en Stack Overflow u otros sitios web, es importante cumplir con las pol√≠ticas de uso y los t√©rminos de servicio del sitio. Aseg√∫rate de revisar y respetar cualquier restricci√≥n o pol√≠tica aplicable antes de realizar extracciones de datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scrapy: Introducci√≥n**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Scrapy es un framework de Python utilizado para realizar web scraping de manera eficiente y estructurada. Proporciona una forma poderosa de extraer datos de sitios web al automatizar el proceso de env√≠o de solicitudes, el manejo de respuestas y la extracci√≥n de datos de las p√°ginas. Scrapy es altamente escalable y se utiliza ampliamente en proyectos de web scraping de gran envergadura.\n",
    "\n",
    "Aqu√≠ tienes una explicaci√≥n detallada de Scrapy con ejemplos:\n",
    "\n",
    "1. **Instalaci√≥n de Scrapy:**\n",
    "\n",
    "Para comenzar, debes instalar `Scrapy` en tu entorno de Python. Puedes hacerlo ejecutando el siguiente comando en tu terminal:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install scrapy`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Creaci√≥n de un nuevo proyecto Scrapy:**\n",
    "\n",
    "Utiliza el comando `scrapy startproject` para crear un nuevo proyecto `Scrapy`. Esto crear√° una estructura b√°sica de directorios y archivos para tu proyecto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy startproject mi_proyecto`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Creaci√≥n de un spider:**\n",
    "\n",
    "Un spider es un componente fundamental de Scrapy que define c√≥mo se realizar√° el web scraping en un sitio web espec√≠fico. Puedes crear un nuevo spider utilizando el comando `scrapy genspider` seguido del nombre del `spider` y el dominio del sitio web que deseas rastrear."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd mi_proyecto\n",
    "scrapy genspider mi_spider ejemplo.com\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto crear√° un archivo Python en la carpeta spiders de tu proyecto que contiene la estructura b√°sica de un spider.\n",
    "\n",
    "4. **Configuraci√≥n del spider:**\n",
    "\n",
    "Abre el archivo del spider reci√©n creado y personal√≠zalo seg√∫n tus necesidades. Deber√°s definir la URL inicial, los selectores `XPath` o `CSS` para extraer los datos y la l√≥gica para seguir los enlaces y navegar por las p√°ginas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import scrapy\n",
    "\n",
    "class MiSpider(scrapy.Spider):\n",
    "    name = 'mi_spider'\n",
    "    start_urls = ['http://ejemplo.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extracci√≥n de datos utilizando selectores XPath o CSS\n",
    "        # L√≥gica para seguir los enlaces y navegar por las p√°ginas\n",
    "        # ...\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Ejecuci√≥n del spider:**\n",
    "\n",
    "Para ejecutar el spider y comenzar el proceso de web scraping, utiliza el comando `scrapy crawl` seguido del nombre del spider que deseas ejecutar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy crawl mi_spider`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy enviar√° solicitudes HTTP a las URL especificadas, analizar√° las respuestas y ejecutar√° el m√©todo `parse()` del spider para extraer los datos.\n",
    "\n",
    "6. **Procesamiento de los datos extra√≠dos:**\n",
    "\n",
    "Dentro del m√©todo `parse()` del spider, puedes utilizar los selectores `XPath` o `CSS` para extraer los datos de la respuesta `HTTP` y procesarlos seg√∫n tus necesidades. Puedes almacenarlos en un archivo, guardarlos en una base de datos o realizar cualquier otro tipo de procesamiento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def parse(self, response):\n",
    "    # Extracci√≥n de datos utilizando selectores XPath o CSS\n",
    "    datos = response.xpath('//div[@class=\"mi-clase\"]/text()').getall()\n",
    "\n",
    "    # Procesamiento de los datos extra√≠dos\n",
    "    for dato in datos:\n",
    "        # Realizar acciones adicionales con los datos extra√≠dos\n",
    "        # ...\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Configuraci√≥n adicional:**\n",
    "\n",
    "Scrapy ofrece muchas opciones de configuraci√≥n adicional para ajustar el comportamiento del web scraping. Puedes personalizar la velocidad de rastreo, establecer l√≠mites de profundidad, habilitar el seguimiento de cookies, entre otras opciones. Revisa la documentaci√≥n oficial de Scrapy para conocer todas las posibilidades de configuraci√≥n.\n",
    "\n",
    "Scrapy es una herramienta poderosa y flexible para realizar web scraping en Python. Permite automatizar el proceso de extracci√≥n de datos de sitios web y ofrece una gran cantidad de funcionalidades para manejar de manera eficiente diferentes escenarios de extracci√≥n. Recuerda siempre ser √©tico y respetar los t√©rminos de servicio de los sitios web que est√°s rastreando."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scrapy (Extracci√≥n de STACKOVERFLOW)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la extracci√≥n de datos de Stack Overflow utilizando Scrapy, seguir√°s los siguientes pasos:\n",
    "\n",
    "1. **Crear un nuevo proyecto Scrapy:**\n",
    "\n",
    "Utiliza el comando `scrapy startproject` para crear un nuevo proyecto `Scrapy` en tu entorno de trabajo. Esto generar√° una estructura de directorios y archivos para tu proyecto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy startproject stackoverflow_scraper`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Crear un spider:**\n",
    "\n",
    "Un spider en Scrapy define c√≥mo se realizar√° el web scraping en un sitio web espec√≠fico. Utiliza el comando `scrapy genspider` para crear un nuevo spider dentro de tu proyecto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd stackoverflow_scraper\n",
    "scrapy genspider stackoverflow_spider stackoverflow.com\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto crear√° un archivo Python en la carpeta `spiders` de tu proyecto que contiene la estructura b√°sica de un `spider`.\n",
    "\n",
    "3. **Configurar el spider:**\n",
    "\n",
    "Abre el archivo del `spider` reci√©n creado y personal√≠zalo para extraer los datos espec√≠ficos de Stack Overflow. Deber√°s definir las URLs iniciales, los selectores `XPath` o `CSS` para extraer los datos y la l√≥gica para seguir los enlaces y navegar por las p√°ginas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import scrapy\n",
    "\n",
    "class StackOverflowSpider(scrapy.Spider):\n",
    "    name = 'stackoverflow'\n",
    "    start_urls = ['https://stackoverflow.com/questions']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extracci√≥n de datos utilizando selectores XPath o CSS\n",
    "        # L√≥gica para seguir los enlaces y navegar por las p√°ginas\n",
    "        # ...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Definir los selectores XPath o CSS:**\n",
    "\n",
    "Utiliza los selectores `XPath` o `CSS` para identificar y extraer los elementos deseados de las p√°ginas de Stack Overflow. Puedes utilizar el inspector de elementos del navegador para encontrar los selectores adecuados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def parse(self, response):\n",
    "    # Extracci√≥n de datos utilizando selectores XPath o CSS\n",
    "    question_titles = response.css('.question-summary h3 a::text').getall()\n",
    "\n",
    "    # Procesamiento de los datos extra√≠dos\n",
    "    for title in question_titles:\n",
    "        print(\"T√≠tulo:\", title)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, utilizamos el selector `CSS` `.question-summary h3 a::text` para extraer los t√≠tulos de las preguntas de Stack Overflow.\n",
    "\n",
    "5. **Ejecutar el spider:**\n",
    "\n",
    "Para ejecutar el `spider` y comenzar el proceso de web scraping, utiliza el comando `scrapy crawl` seguido del nombre del `spider`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy crawl stackoverflow`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy enviar√° solicitudes HTTP a las URL especificadas, analizar√° las respuestas y ejecutar√° el m√©todo `parse()` del spider para extraer los datos.\n",
    "\n",
    "Scrapy manejar√° autom√°ticamente el manejo de solicitudes y respuestas, el seguimiento de enlaces y la navegaci√≥n por las p√°ginas. Puedes agregar m√°s l√≥gica dentro del m√©todo `parse()` para seguir enlaces adicionales, paginaci√≥n o cualquier otra acci√≥n requerida para extraer los datos completos que deseas.\n",
    "\n",
    "Recuerda que al utilizar Scrapy para web scraping, es importante respetar las pol√≠ticas de uso y los t√©rminos de servicio del sitio web que est√°s raspando. Aseg√∫rate de revisar y cumplir cualquier restricci√≥n o pol√≠tica aplicable antes de realizar extracciones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ATENCI√ìN: Scrapy no funciona con todas las p√°ginas webs**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es cierto que Scrapy puede no funcionar con todas las p√°ginas web, especialmente aquellas que tienen estructuras y mecanismos de protecci√≥n m√°s complejos. A continuaci√≥n, te explicar√© algunas razones por las cuales Scrapy puede no ser efectivo en ciertos casos, junto con ejemplos ilustrativos:\n",
    "\n",
    "1. **Protecci√≥n anti-scraping:**\n",
    "\n",
    " Algunos sitios web implementan medidas para evitar el web scraping, como el bloqueo de direcciones IP, la detecci√≥n de patrones de acceso automatizado o el uso de `CAPTCHAs`. Estas medidas pueden dificultar o impedir que Scrapy acceda y extraiga datos del sitio. Por ejemplo, si un sitio web bloquea el acceso a ciertas `IP` sospechosas, Scrapy podr√≠a encontrarse con errores de conexi√≥n o bloqueo.\n",
    "\n",
    "2. **Generaci√≥n din√°mica de contenido:**\n",
    "\n",
    " Algunos sitios web utilizan tecnolog√≠as como JavaScript para generar contenido de manera din√°mica. Scrapy no interpreta JavaScript de forma autom√°tica, lo que significa que solo obtendr√° el c√≥digo HTML inicial de la p√°gina sin ejecutar el JavaScript. Como resultado, los datos generados din√°micamente pueden no estar disponibles para Scrapy. Por ejemplo, si un sitio web carga contenido adicional mediante solicitudes AJAX, es posible que Scrapy no pueda extraer estos datos adicionales.\n",
    "\n",
    "3. **Estructura compleja de la p√°gina:**\n",
    "\n",
    " Si la estructura HTML de una p√°gina web es compleja o utiliza formatos no est√°ndar, los selectores `XPath` o `CSS` utilizados por Scrapy pueden no ser suficientes para extraer los datos deseables. Puede ser necesario escribir reglas de extracci√≥n personalizadas o utilizar otras bibliotecas espec√≠ficas para analizar y procesar la p√°gina. Por ejemplo, si una p√°gina web utiliza atributos de datos personalizados o una estructura HTML anidada compleja, los selectores predeterminados de Scrapy pueden no ser suficientes.\n",
    "\n",
    "4. **Acceso a contenido protegido:**\n",
    "\n",
    " Algunos sitios web requieren autenticaci√≥n o tienen restricciones de acceso para cierto contenido. Scrapy puede tener dificultades para realizar la autenticaci√≥n o superar estas restricciones. En estos casos, puede ser necesario implementar l√≥gica adicional para enviar datos de inicio de sesi√≥n o manejar cookies de manera adecuada.\n",
    "\n",
    "Si te encuentras con alguna de estas situaciones, podr√≠as considerar alternativas a Scrapy, como utilizar otras bibliotecas de web scraping m√°s espec√≠ficas para casos especiales o incluso desarrollar un script personalizado para extraer los datos necesarios. En algunos casos, es posible que debas comunicarte directamente con los propietarios del sitio web para obtener permisos o buscar una API oficial para acceder a los datos de manera m√°s estructurada.\n",
    "\n",
    "Recuerda que cuando realizas web scraping, es importante respetar las pol√≠ticas de uso de los sitios web y cumplir con los t√©rminos de servicio. Aseg√∫rate de leer y comprender las pol√≠ticas del sitio antes de realizar cualquier extracci√≥n de datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scrapy (Extracci√≥n de DIARIO EL UNIVERSO)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la extracci√≥n de datos del Diario El Universo utilizando Scrapy, seguir√°s los siguientes pasos:\n",
    "\n",
    "1. **Crear un nuevo proyecto Scrapy:**\n",
    "\n",
    "Utiliza el comando `scrapy startproject` para crear un nuevo proyecto `Scrapy` en tu entorno de trabajo. Esto generar√° una estructura de directorios y archivos para tu proyecto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Crear un spider:**\n",
    "\n",
    "Un spider en Scrapy define c√≥mo se realizar√° el web scraping en un sitio web espec√≠fico. Utiliza el comando `scrapy genspider` para crear un nuevo spider dentro de tu proyecto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd universo_scraper\n",
    "scrapy genspider universo_spider www.eluniverso.com\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto crear√° un archivo Python en la carpeta `spiders` de tu proyecto que contiene la estructura b√°sica de un spider.\n",
    "\n",
    "3. **Configurar el spider:**\n",
    "\n",
    "Abre el archivo del `spider` reci√©n creado y personal√≠zalo para extraer los datos espec√≠ficos del Diario El Universo. Deber√°s definir las URLs iniciales, los selectores `XPath` o `CSS` para extraer los datos y la l√≥gica para seguir los enlaces y navegar por las p√°ginas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import scrapy\n",
    "\n",
    "class UniversoSpider(scrapy.Spider):\n",
    "    name = 'universo'\n",
    "    start_urls = ['https://www.eluniverso.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extracci√≥n de datos utilizando selectores XPath o CSS\n",
    "        # L√≥gica para seguir los enlaces y navegar por las p√°ginas\n",
    "        # ...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Definir los selectores XPath o CSS:**\n",
    "\n",
    "Utiliza los selectores `XPath` o `CSS` para identificar y extraer los elementos deseados de las p√°ginas del Diario El Universo. Puedes utilizar el inspector de elementos del navegador para encontrar los selectores adecuados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def parse(self, response):\n",
    "    # Extracci√≥n de datos utilizando selectores XPath o CSS\n",
    "    news_titles = response.css('.post-title a::text').getall()\n",
    "\n",
    "    # Procesamiento de los datos extra√≠dos\n",
    "    for title in news_titles:\n",
    "        print(\"T√≠tulo:\", title)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, utilizamos el selector `CSS` `.post-title a::text` para extraer los t√≠tulos de las noticias del Diario El Universo.\n",
    "\n",
    "5. **Ejecutar el spider:**\n",
    "\n",
    "Para ejecutar el spider y comenzar el proceso de web scraping, utiliza el comando `scrapy crawl` seguido del nombre del `spider`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy crawl universo`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy enviar√° solicitudes HTTP a las URL especificadas, analizar√° las respuestas y ejecutar√° el m√©todo `parse()` del spider para extraer los datos.\n",
    "\n",
    "Scrapy manejar√° autom√°ticamente el manejo de solicitudes y respuestas, el seguimiento de enlaces y la navegaci√≥n por las p√°ginas. Puedes agregar m√°s l√≥gica dentro del m√©todo `parse()` para seguir enlaces adicionales, paginaci√≥n o cualquier otra acci√≥n requerida para extraer los datos completos que deseas.\n",
    "\n",
    "Recuerda que al utilizar Scrapy para web scraping, es importante respetar las pol√≠ticas de uso y los t√©rminos de servicio del sitio web que est√°s raspando. Aseg√∫rate de revisar y cumplir cualquier restricci√≥n o pol√≠tica aplicable antes de realizar extracciones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejecutar Scrapy sin la Terminal (+ Jupyter Notebook, Google Colab o Similares)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar Scrapy en entornos como Jupyter Notebook, Google Colab o similares sin utilizar la terminal, puedes seguir los siguientes pasos detallados con ejemplos:\n",
    "\n",
    "1. **Instalar Scrapy:**\n",
    "\n",
    "Aseg√∫rate de tener `Scrapy` instalado en tu entorno. Puedes instalarlo utilizando el comando `pip`:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install scrapy`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Importar las bibliotecas necesarias:**\n",
    "\n",
    "En tu entorno, importa las bibliotecas necesarias para ejecutar `Scrapy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.spiders import Spider\n",
    "from scrapy.http import Request"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Definir el Spider:**\n",
    "\n",
    "Crea una clase que herede de Spider y defina la l√≥gica de extracci√≥n de datos dentro del m√©todo `parse()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySpider(Spider):\n",
    "    name = 'myspider'\n",
    "\n",
    "    def start_requests(self):\n",
    "        # Definir las URLs iniciales\n",
    "        start_urls = ['https://naruto-official.com/es']\n",
    "\n",
    "        for url in start_urls:\n",
    "            yield Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extracci√≥n de datos utilizando selectores XPath o CSS\n",
    "        # ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Ejecutar el proceso de Scrapy:**\n",
    "\n",
    "Crea una instancia de `CrawlerProcess` y pasa tu spider como argumento. Luego, llama al m√©todo `start()` para iniciar el proceso de scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 19:26:39 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2023-06-16 19:26:39 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.12 (main, Apr  5 2022, 06:56:58) - [GCC 7.5.0], pyOpenSSL 21.0.0 (OpenSSL 1.1.1n  15 Mar 2022), cryptography 3.4.8, Platform Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\n",
      "2023-06-16 19:26:39 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-16 19:26:39 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2023-06-16 19:26:40 [scrapy.extensions.telnet] INFO: Telnet Password: f01ab8d5085bec13\n",
      "2023-06-16 19:26:41 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-06-16 19:26:43 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-06-16 19:26:43 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-06-16 19:26:43 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-06-16 19:26:43 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-06-16 19:26:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-06-16 19:26:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-06-16 19:26:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://naruto-official.com/es> (referer: None)\n",
      "2023-06-16 19:26:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://naruto-official.com/es> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/site-packages/twisted/internet/defer.py\", line 857, in _runCallbacks\n",
      "    current.result = callback(  # type: ignore[misc]\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/site-packages/scrapy/spiders/__init__.py\", line 70, in parse\n",
      "    raise NotImplementedError(f'{self.__class__.__name__}.parse callback is not defined')\n",
      "NotImplementedError: MySpider.parse callback is not defined\n",
      "2023-06-16 19:26:45 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-06-16 19:26:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 225,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 14784,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 1.358902,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 6, 17, 0, 26, 45, 948224),\n",
      " 'httpcompression/response_bytes': 104333,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 220614656,\n",
      " 'memusage/startup': 220614656,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'spider_exceptions/NotImplementedError': 1,\n",
      " 'start_time': datetime.datetime(2023, 6, 17, 0, 26, 44, 589322)}\n",
      "2023-06-16 19:26:45 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess()\n",
    "process.crawl(MySpider)\n",
    "process.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el c√≥digo, Scrapy realizar√° las solicitudes HTTP a las URL especificadas, analizar√° las respuestas y ejecutar√° el m√©todo `parse()` para extraer los datos. Puedes adaptar y ampliar la l√≥gica dentro del m√©todo `parse()` seg√∫n tus necesidades de extracci√≥n de datos.\n",
    "\n",
    "Recuerda que al ejecutar Scrapy en entornos diferentes a la l√≠nea de comandos, es posible que no veas los registros detallados de Scrapy como lo har√≠as en la ejecuci√≥n desde la terminal. Sin embargo, los datos extra√≠dos estar√°n disponibles y puedes manejarlos seg√∫n tus necesidades.\n",
    "\n",
    "Ten en cuenta que las limitaciones y consideraciones de recursos y tiempo de ejecuci√≥n de tu entorno espec√≠fico tambi√©n se aplicar√°n al ejecutar Scrapy de esta manera."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr√°s 2** | **Siguiente 4** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [üè†](../../README.md) | [‚è™](./2.Introduccion_al_Web_Scraping.ipynb)| [‚è©](./4.Varias_Paginas_del_mismo_dominio.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
