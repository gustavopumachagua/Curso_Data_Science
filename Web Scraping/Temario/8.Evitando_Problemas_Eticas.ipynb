{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr√°s 7** | **Siguiente 9** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [üè†](../../README.md) | [‚è™](./7.Autenticaci%C3%B3n_y_Captchas.ipynb)| [‚è©](./9.Automatizacion_almacenamiento_y_actualizacion_de_datos.ipynb)|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. Evitando Problemas: √âticas**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Web Scraping responsable**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El web scraping responsable es una pr√°ctica √©tica y responsable que implica recopilar datos de sitios web de manera √©tica y respetando los t√©rminos de servicio del sitio objetivo. Consiste en seguir pautas y principios para garantizar que el proceso de extracci√≥n de datos no cause ning√∫n da√±o al sitio web, a los usuarios ni viole la privacidad o los derechos de autor.\n",
    "\n",
    "A continuaci√≥n, se presentan algunos principios clave del web scraping responsable:\n",
    "\n",
    "1. **Cumplir los t√©rminos de servicio:** Antes de realizar cualquier actividad de web scraping, es fundamental leer y comprender los t√©rminos de servicio del sitio web objetivo. Algunos sitios web pueden prohibir o restringir el web scraping en sus pol√≠ticas, por lo que es importante respetar estas restricciones y solicitar permiso cuando sea necesario.\n",
    "\n",
    "2. **Limitar la frecuencia y la carga:** Es importante evitar sobrecargar el servidor del sitio web objetivo al realizar solicitudes excesivas o frecuentes. Se debe respetar cualquier l√≠mite de velocidad o restricci√≥n de acceso establecido por el sitio web. Utilizar retrasos apropiados entre solicitudes (por ejemplo, utilizando la funci√≥n `time.sleep()` en Python) para no abrumar el servidor con una gran cantidad de solicitudes en poco tiempo.\n",
    "\n",
    "3. **No da√±ar ni interferir con el sitio web:** El web scraping responsable implica no causar da√±os al sitio web objetivo. Esto significa evitar cualquier acci√≥n que pueda afectar negativamente el rendimiento del sitio, como realizar solicitudes masivas o enviar solicitudes que requieran una cantidad excesiva de recursos del servidor.\n",
    "\n",
    "4. **Respetar la privacidad y los derechos de autor:** Durante el web scraping, es fundamental respetar la privacidad de los usuarios y los derechos de autor de los datos recopilados. No se deben recopilar datos personales sensibles sin el consentimiento adecuado y se deben respetar los derechos de autor y atribuir correctamente cualquier contenido utilizado.\n",
    "\n",
    "5. **Obtener datos p√∫blicos y accesibles:** El web scraping responsable se enfoca en recopilar datos que sean p√∫blicos y accesibles para todos los usuarios. No se deben violar restricciones de acceso o intentar acceder a √°reas restringidas sin autorizaci√≥n.\n",
    "\n",
    "A continuaci√≥n, se muestra un ejemplo de c√≥digo Python que ilustra el uso responsable del web scraping utilizando la biblioteca requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Establecer el encabezado del agente de usuario para simular una solicitud de navegador\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "# Realizar una solicitud HTTP a la p√°gina web\n",
    "response = requests.get('http://www.example.com', headers=headers)\n",
    "\n",
    "# Verificar el c√≥digo de respuesta para asegurarse de que la solicitud sea exitosa\n",
    "if response.status_code == 200:\n",
    "    # Procesar la respuesta y extraer los datos necesarios\n",
    "    data = response.text\n",
    "    # Realizar m√°s operaciones de extracci√≥n de datos aqu√≠\n",
    "    ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se establece un encabezado de \"`User-Agent`\" para simular una solicitud de navegador y evitar ser identificado como un bot. Adem√°s, se verifica el c√≥digo de respuesta para asegurarse de que la solicitud haya sido exitosa antes de procesar los datos. Estos pasos ayudan a mostrar un comportamiento responsable al realizar el web scraping.\n",
    "\n",
    "Es importante tener en cuenta que el web scraping responsable no solo se trata de seguir buenas pr√°cticas t√©cnicas, sino tambi√©n de respetar la √©tica, la privacidad y los derechos de los sitios web y los usuarios. Se recomienda siempre tener en cuenta los t√©rminos de servicio y las pol√≠ticas del sitio web objetivo, y obtener el consentimiento adecuado cuando sea necesario."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **¬øC√≥mo evitar que me baneen?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evitar ser baneado al realizar web scraping es crucial para mantener un comportamiento responsable y √©tico. Aqu√≠ hay algunas pr√°cticas recomendadas que puedes seguir para minimizar el riesgo de ser bloqueado o baneado por los sitios web:\n",
    "\n",
    "1. **Leer y respetar los t√©rminos de servicio:** Antes de realizar cualquier web scraping, es importante leer y comprender los t√©rminos de servicio del sitio web objetivo. Algunos sitios web pueden prohibir o restringir el web scraping en sus pol√≠ticas, por lo que es fundamental respetar estas restricciones. Si un sitio web tiene una API oficial disponible, considera utilizarla en lugar de realizar scraping directo.\n",
    "\n",
    "2. **Utilizar encabezados y agentes de usuario adecuados:** Al enviar solicitudes HTTP a trav√©s de tu c√≥digo de web scraping, aseg√∫rate de utilizar encabezados y agentes de usuario adecuados. Esto ayudar√° a simular el comportamiento de un navegador y evitar√° que el sitio web te identifique como un bot. Puedes utilizar la biblioteca `requests` en Python para configurar encabezados personalizados, como el User-Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "}\n",
    "\n",
    "response = requests.get('http://www.example.com', headers=headers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Limitar la velocidad y la frecuencia de las solicitudes:** Evita enviar una gran cantidad de solicitudes en un corto per√≠odo de tiempo. Respeta cualquier l√≠mite de velocidad o restricci√≥n de acceso establecido por el sitio web. Puedes agregar retrasos entre las solicitudes utilizando la funci√≥n `time.sleep()` en Python para evitar sobrecargar el servidor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Esperar 1 segundo entre cada solicitud\n",
    "time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Evitar el rastreo masivo o agresivo:** No realices rastreos masivos o agresivos que puedan sobrecargar el servidor del sitio web objetivo. Aseg√∫rate de ajustar tu c√≥digo de web scraping para obtener solo los datos necesarios y no descargar contenido innecesario.\n",
    "\n",
    "5. **Seguir el archivo robots.txt:** El archivo robots.txt es una forma en que los sitios web indican qu√© partes de su sitio son accesibles para los rastreadores y qu√© partes no. Antes de realizar web scraping en un sitio web, verifica si tienen un archivo robots.txt y resp√©talo. No accedas a √°reas restringidas o que no est√©n permitidas para los rastreadores.\n",
    "\n",
    "6. **Utilizar proxies o servicios de rotaci√≥n de IP:** Si planeas realizar web scraping a gran escala, considera utilizar proxies o servicios de rotaci√≥n de IP para evitar ser detectado y bloqueado. Esto te permite cambiar tu direcci√≥n IP y evitar que el sitio web identifique tus solicitudes como provenientes de una sola fuente.\n",
    "\n",
    "7. **Manejar los errores y reintentos adecuadamente:** Es posible que encuentres errores o respuestas incorrectas al realizar web scraping. Aseg√∫rate de manejar estos errores adecuadamente en tu c√≥digo y considera implementar mecanismos de reintentos en caso de fallos temporales.\n",
    "\n",
    "Siguiendo estas mejores pr√°cticas, puedes reducir el riesgo de ser bloqueado o baneado al realizar web scraping. Sin embargo, es importante tener en cuenta que cada sitio web tiene sus propias pol√≠ticas y restricciones, por lo que es fundamental adaptar tu enfoque a cada caso espec√≠fico y monitorear cualquier cambio en los t√©rminos de servicio del sitio web objetivo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **User Agents y VPNs**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **User Agents:**\n",
    "\n",
    "En el contexto del web scraping, un User Agent es una cadena de texto que se env√≠a junto con las solicitudes HTTP para identificar el tipo de navegador o cliente que realiza la solicitud. Los User Agents permiten al servidor web adaptar su respuesta en funci√≥n del cliente que realiza la solicitud. Los sitios web a menudo utilizan User Agents para distinguir entre solicitudes realizadas por humanos y aquellas realizadas por bots o scripts de web scraping.\n",
    "\n",
    "Los User Agents generalmente contienen informaci√≥n sobre el navegador, el sistema operativo y otros detalles relevantes. Algunos ejemplos de User Agents pueden ser:\n",
    "\n",
    "* **User-Agent:** Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\n",
    "* **User-Agent:** Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36\n",
    "\n",
    "Al realizar web scraping, es com√∫n utilizar User Agents para simular el comportamiento de un navegador y evitar ser identificado como un bot. Puedes configurar el User Agent en tus solicitudes HTTP para que coincida con el User Agent de un navegador espec√≠fico. Esto puede ayudar a evitar bloqueos o restricciones basadas en User Agents sospechosos.\n",
    "\n",
    "Aqu√≠ tienes un ejemplo de c√≥mo establecer el User Agent utilizando la biblioteca `requests` en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "}\n",
    "\n",
    "response = requests.get('http://www.example.com', headers=headers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **VPNs:**\n",
    "\n",
    "Una VPN (Virtual Private Network) es una tecnolog√≠a que crea una conexi√≥n segura y cifrada entre tu dispositivo y un servidor remoto a trav√©s de una red p√∫blica, como Internet. Al conectarte a trav√©s de una VPN, tu tr√°fico de Internet se enruta a trav√©s del servidor remoto, ocultando tu direcci√≥n IP real y proporcion√°ndote una direcci√≥n IP virtual.\n",
    "\n",
    "Las VPN se utilizan por varias razones, como el anonimato en l√≠nea, la seguridad y la privacidad. En el contexto del web scraping, las VPN pueden ser √∫tiles para evitar ser detectado o bloqueado al realizar solicitudes a un sitio web. Al cambiar tu direcci√≥n IP mediante una VPN, puedes evitar que el sitio web identifique tus solicitudes como provenientes de una sola fuente.\n",
    "\n",
    "Algunos servicios de VPN populares incluyen NordVPN, ExpressVPN y CyberGhost. Estos servicios te permiten seleccionar un servidor en una ubicaci√≥n espec√≠fica y enrutar tu tr√°fico a trav√©s de ese servidor para ocultar tu direcci√≥n IP real.\n",
    "\n",
    "Es importante tener en cuenta que aunque una VPN puede proporcionar cierto nivel de anonimato y ocultar tu direcci√≥n IP, no garantiza un anonimato completo. Algunos sitios web pueden detectar el uso de VPN y tomar medidas para bloquear el acceso desde esas direcciones IP.\n",
    "\n",
    "En resumen, los User Agents se utilizan para simular el comportamiento de un navegador y evitar ser identificado como un bot al realizar web scraping. Las VPN, por otro lado, te permiten cambiar tu direcci√≥n IP y ocultar tu identidad en l√≠nea, lo que puede ayudar a evitar ser detectado o bloqueado al acceder a sitios web. Ambas t√©cnicas son √∫tiles para garantizar una experiencia de web scraping m√°s segura y evitar restricciones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Rotaci√≥n de User Agents con Scrapy**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Scrapy, la rotaci√≥n de User Agents se puede lograr utilizando el middleware `RandomUserAgentMiddleware`, que selecciona aleatoriamente un User Agent de una lista predefinida en cada solicitud. Esto ayuda a simular diferentes navegadores y evita que el servidor detecte un patr√≥n constante en los User Agents.\n",
    "\n",
    "Aqu√≠ tienes un ejemplo de c√≥mo habilitar la rotaci√≥n de User Agents en Scrapy:\n",
    "\n",
    "1. Primero, aseg√∫rate de tener instalada la biblioteca `scrapy-user-agents`:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install scrapy-user-agents`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Luego, en tu proyecto de Scrapy, agrega el middleware `RandomUserAgentMiddleware` en el archivo `settings.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n",
    "    'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 400,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ahora, cada vez que realices una solicitud en tu spider, Scrapy seleccionar√° aleatoriamente un User Agent de la lista predefinida y lo incluir√° en la solicitud.\n",
    "\n",
    "Aqu√≠ tienes un ejemplo completo de c√≥mo usar la rotaci√≥n de User Agents en Scrapy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy_user_agents.middlewares import RandomUserAgentMiddleware\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'my_spider'\n",
    "    start_urls = ['http://www.example.com']\n",
    "\n",
    "    custom_settings = {\n",
    "        'DOWNLOADER_MIDDLEWARES': {\n",
    "            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n",
    "            'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 400,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Procesar la respuesta de la solicitud\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta configuraci√≥n, cada vez que `MySpider` realice una solicitud, Scrapy seleccionar√° aleatoriamente un User Agent de la lista predefinida y lo incluir√° en la solicitud. Esto ayuda a evitar la detecci√≥n de patrones y mejora la capacidad de web scraping responsable.\n",
    "\n",
    "Recuerda que tambi√©n puedes personalizar la lista de User Agents que se utilizar√°n en la rotaci√≥n. Para ello, puedes modificar la configuraci√≥n `USER_AGENT_LIST` en el archivo `settings.py`. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENT_LIST = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36',\n",
    "    # Agregar m√°s User Agents aqu√≠\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, la rotaci√≥n de User Agents en Scrapy se logra utilizando el middleware `RandomUserAgentMiddleware`, que selecciona aleatoriamente un User Agent de una lista predefinida en cada solicitud. Esto ayuda a simular diferentes navegadores y evitar ser detectado como un bot en web scraping."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr√°s 7** | **Siguiente 9** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [üè†](../../README.md) | [‚è™](./7.Autenticaci%C3%B3n_y_Captchas.ipynb)| [‚è©](./9.Automatizacion_almacenamiento_y_actualizacion_de_datos.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
