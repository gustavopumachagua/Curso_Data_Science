{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr√°s 3** | **Siguiente 5** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [üè†](../../README.md) | [‚è™](./3.Una_sola_pagina_estatica.ipynb)| [‚è©](./5.Paginas_Dinamicas.ipynb)|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Varias Paginas del mismo dominio**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introducci√≥n: Scraping Vertical y Horizontal**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El scraping vertical y horizontal se refiere a dos enfoques diferentes para el proceso de extracci√≥n de datos mediante web scraping. El scraping vertical se centra en extraer datos de una sola p√°gina web, mientras que el scraping horizontal implica la extracci√≥n de datos de m√∫ltiples p√°ginas web enlazadas. A continuaci√≥n, te explicar√© c√≥mo ejecutar Scrapy sin la terminal en entornos como Jupyter Notebook, Google Colab o similares, utilizando ejemplos tanto de scraping vertical como horizontal.\n",
    "\n",
    "* **Scraping Vertical:**\n",
    "\n",
    "1. **Crear un nuevo proyecto Scrapy y un spider:**\n",
    "\n",
    "Sigue los pasos mencionados anteriormente para crear un nuevo proyecto `Scrapy` y un `spider` dentro de ese proyecto.\n",
    "\n",
    "2. **Definir el spider para el scraping vertical:**\n",
    "\n",
    "Dentro del `spider`, define el punto de inicio (URL inicial) y el m√©todo `parse()` para extraer los datos deseados de una sola p√°gina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.spiders import Spider\n",
    "from scrapy.http import Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerticalSpider(scrapy.Spider):\n",
    "    name = 'vertical_spider'\n",
    "    start_urls = ['https://naruto-official.com/es']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # L√≥gica de extracci√≥n de datos de una sola p√°gina\n",
    "        # ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Ejecutar el scraping vertical:**\n",
    "\n",
    "Utiliza el m√≥dulo `CrawlerProcess` de `Scrapy` para ejecutar el `scraping` vertical en tu entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 20:12:14 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2023-06-16 20:12:14 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.2.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.12 (main, Apr  5 2022, 06:56:58) - [GCC 7.5.0], pyOpenSSL 21.0.0 (OpenSSL 1.1.1n  15 Mar 2022), cryptography 3.4.8, Platform Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\n",
      "2023-06-16 20:12:14 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2023-06-16 20:12:14 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2023-06-16 20:12:14 [scrapy.extensions.telnet] INFO: Telnet Password: 452b2b666a90b919\n",
      "2023-06-16 20:12:14 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-06-16 20:12:15 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-06-16 20:12:15 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-06-16 20:12:15 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-06-16 20:12:15 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-06-16 20:12:16 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-06-16 20:12:16 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-06-16 20:12:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://naruto-official.com/es> (referer: None)\n",
      "2023-06-16 20:12:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://naruto-official.com/es> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/site-packages/twisted/internet/defer.py\", line 857, in _runCallbacks\n",
      "    current.result = callback(  # type: ignore[misc]\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/site-packages/scrapy/spiders/__init__.py\", line 67, in _parse\n",
      "    return self.parse(response, **kwargs)\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/site-packages/scrapy/spiders/__init__.py\", line 70, in parse\n",
      "    raise NotImplementedError(f'{self.__class__.__name__}.parse callback is not defined')\n",
      "NotImplementedError: VerticalSpider.parse callback is not defined\n",
      "2023-06-16 20:12:17 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-06-16 20:12:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 225,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 14784,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 1.287444,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 6, 17, 1, 12, 17, 955599),\n",
      " 'httpcompression/response_bytes': 104333,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 226881536,\n",
      " 'memusage/startup': 226881536,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'spider_exceptions/NotImplementedError': 1,\n",
      " 'start_time': datetime.datetime(2023, 6, 17, 1, 12, 16, 668155)}\n",
      "2023-06-16 20:12:17 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawl(VerticalSpider)\n",
    "process.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy realizar√° las solicitudes a la URL especificada, analizar√° la respuesta y ejecutar√° el m√©todo `parse()` para extraer los datos de la p√°gina.\n",
    "\n",
    "* **Scraping Horizontal:**\n",
    "\n",
    "1. **Crear un nuevo proyecto Scrapy y un spider:**\n",
    "\n",
    "Al igual que en el scraping vertical, crea un nuevo proyecto Scrapy y un spider dentro de ese proyecto.\n",
    "\n",
    "2. **Definir el spider para el scraping horizontal:**\n",
    "\n",
    "En este caso, el spider debe estar configurado para seguir los enlaces y extraer datos de m√∫ltiples p√°ginas enlazadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.spiders import Spider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorizontalSpider(scrapy.Spider):\n",
    "    name = 'horizontal_spider'\n",
    "    start_urls = ['https://naruto-official.com/es']\n",
    "    allowed_domains = ['naruto-official.com']\n",
    "    rules = [Rule(LinkExtractor(), callback='parse_page', follow=True)]\n",
    "\n",
    "    def parse_page(self, response):\n",
    "        # L√≥gica de extracci√≥n de datos de cada p√°gina\n",
    "        # ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El spider utiliza el `LinkExtractor` para encontrar enlaces en las p√°ginas y la regla `Rule` para seguir esos enlaces y ejecutar el m√©todo `parse_page()` para extraer los datos de cada p√°gina.\n",
    "\n",
    "3. **Ejecutar el scraping horizontal:**\n",
    "\n",
    "Al igual que en el scraping vertical, utiliza el m√≥dulo `CrawlerProcess` de Scrapy para ejecutar el scraping horizontal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawl(HorizontalSpider)\n",
    "process.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy realizar√° las solicitudes a la URL inicial, seguir√° los enlaces, analizar√° las respuestas y ejecutar√° el m√©todo `parse_page()` para extraer los datos de cada p√°gina enlazada.\n",
    "\n",
    "Al ejecutar el c√≥digo, Scrapy ejecutar√° el scraping vertical o horizontal seg√∫n el spider que hayas configurado. Recuerda personalizar la l√≥gica dentro de los m√©todos `parse()` o `parse_page()` seg√∫n tus necesidades de extracci√≥n de datos.\n",
    "\n",
    "Estos ejemplos te permitir√°n ejecutar Scrapy sin la terminal en entornos como Jupyter Notebook, Google Colab o similares, y realizar scraping vertical o horizontal seg√∫n tus requerimientos. Aseg√∫rate de tener en cuenta las limitaciones y consideraciones de recursos y tiempo de ejecuci√≥n de tu entorno espec√≠fico."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Web Scraping Vertical (Extracci√≥n de TRIPADVISOR con Scrapy PT. 1)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar Scrapy sin utilizar la terminal y trabajar con Jupyter Notebook, Google Colab u otro entorno similar, puedes utilizar la biblioteca scrapydo. A continuaci√≥n, te explicar√© los pasos para extraer datos verticales de `TripAdvisor` utilizando Scrapy en un entorno sin terminal:\n",
    "\n",
    "1. **Instalar las bibliotecas necesarias:**\n",
    "\n",
    "Aseg√∫rate de tener instaladas las bibliotecas `scrapy` y `scrapydo`. Puedes instalarlas ejecutando el siguiente comando en una celda de c√≥digo:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!pip install scrapy scrapydo`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Importar las bibliotecas y configurar scrapydo:**\n",
    "\n",
    "En la primera celda de c√≥digo, importa las bibliotecas necesarias y configura scrapydo para trabajar con Jupyter Notebook o Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread CrochetReactor:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/site-packages/crochet/_eventloop.py\", line 372, in <lambda>\n",
      "    target=lambda: self._reactor.run(installSignalHandlers=False),\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/site-packages/twisted/internet/base.py\", line 1314, in run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    self.startRunning(installSignalHandlers=installSignalHandlers)\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/site-packages/twisted/internet/base.py\", line 1296, in startRunning\n",
      "    ReactorBase.startRunning(cast(ReactorBase, self))\n",
      "  File \"/home/puma/anaconda3/lib/python3.9/site-packages/twisted/internet/base.py\", line 840, in startRunning\n",
      "    raise error.ReactorNotRestartable()\n",
      "twisted.internet.error.ReactorNotRestartable\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import scrapydo\n",
    "\n",
    "scrapydo.setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Definir el spider de Scrapy:**\n",
    "\n",
    "Crea una clase que herede de `scrapy.Spider` y defina la l√≥gica de extracci√≥n de datos dentro del m√©todo `parse()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripAdvisorSpider(scrapy.Spider):\n",
    "    name = 'tripadvisor_spider'\n",
    "    start_urls = ['https://www.tripadvisor.com/Attractions-g294073-Activities-Quito_Pichincha_Province.html']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # L√≥gica de extracci√≥n de datos de la p√°gina\n",
    "        # ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aseg√∫rate de reemplazar la URL en `start_urls` con la p√°gina de `TripAdvisor` que deseas extraer.\n",
    "\n",
    "4. **Ejecutar el spider:**\n",
    "\n",
    "En la siguiente celda de c√≥digo, utiliza `scrapydo.run_spider()` para ejecutar el spider de Scrapy y obtener los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapydo.run_spider(TripAdvisorSpider)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto ejecutar√° el spider y comenzar√° la extracci√≥n de datos de `TripAdvisor`.\n",
    "\n",
    "5. **Procesar los resultados:**\n",
    "\n",
    "Despu√©s de ejecutar el spider, puedes acceder a los datos extra√≠dos dentro del m√©todo `parse()` y procesarlos seg√∫n tus necesidades. Puedes imprimirlos en la salida, guardarlos en un archivo CSV o JSON, o realizar cualquier otra operaci√≥n deseada.\n",
    "\n",
    "Con estos pasos, podr√°s ejecutar Scrapy en un entorno como Jupyter Notebook o Google Colab sin utilizar la terminal y extraer datos verticales de TripAdvisor. Recuerda personalizar la l√≥gica de extracci√≥n de datos dentro del m√©todo `parse()` para adaptarla a tus necesidades espec√≠ficas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scrapy Map Compose (Extracci√≥n de TRIPADVISOR con Scrapy PT.2)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutar Scrapy sin utilizar la terminal y trabajar con Jupyter Notebook, Google Colab u otro entorno similar, y realizar una extracci√≥n de datos de TripAdvisor utilizando Scrapy Map Compose, puedes seguir los siguientes pasos:\n",
    "\n",
    "1. **Instalar las bibliotecas necesarias:**\n",
    "\n",
    "Aseg√∫rate de tener instaladas las bibliotecas `scrapy`, `scrapydo` y `scrapy-map-compose`. Puedes instalarlas ejecutando el siguiente comando en una celda de c√≥digo:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!pip install scrapy scrapydo scrapy-map-compose`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Importar las bibliotecas y configurar scrapydo:**\n",
    "\n",
    "En la primera celda de c√≥digo, importa las bibliotecas necesarias y configura scrapydo para trabajar con Jupyter Notebook o Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import scrapydo\n",
    "from scrapy_map_compose import MapCompose\n",
    "\n",
    "scrapydo.setup()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Definir el spider de Scrapy:**\n",
    "\n",
    "Crea una clase que herede de `scrapy.Spider` y defina la l√≥gica de extracci√≥n de datos dentro del m√©todo `parse()` utilizando `Scrapy Map Compose`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripAdvisorSpider(scrapy.Spider):\n",
    "    name = 'tripadvisor_spider'\n",
    "    start_urls = ['https://www.tripadvisor.com/Attractions-g294073-Activities-Quito_Pichincha_Province.html']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extracci√≥n de datos utilizando Scrapy Map Compose\n",
    "        titles = response.css('h3.title::text').getall()\n",
    "        descriptions = response.css('div.description::text').getall()\n",
    "\n",
    "        yield {\n",
    "            'title': MapCompose(str.strip)(titles),\n",
    "            'description': MapCompose(str.strip)(descriptions)\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aseg√∫rate de reemplazar la URL en `start_urls` con la p√°gina de `TripAdvisor` que deseas extraer.\n",
    "\n",
    "4. **Ejecutar el spider:**\n",
    "\n",
    "En la siguiente celda de c√≥digo, utiliza `scrapydo.run_spider()` para ejecutar el spider de Scrapy y obtener los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapydo.run_spider(TripAdvisorSpider)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto ejecutar√° el spider y comenzar√° la extracci√≥n de datos de `TripAdvisor` utilizando `Scrapy Map Compose`.\n",
    "\n",
    "5. **Procesar los resultados:**\n",
    "\n",
    "Despu√©s de ejecutar el spider, puedes acceder a los datos extra√≠dos dentro del m√©todo `parse()` y procesarlos seg√∫n tus necesidades. En este ejemplo, se utiliza `MapCompose(str.strip)` para aplicar una funci√≥n de limpieza a los t√≠tulos y descripciones extra√≠das. Puedes modificar esta l√≥gica de procesamiento seg√∫n tus necesidades.\n",
    "\n",
    "Con estos pasos, podr√°s ejecutar Scrapy en un entorno como Jupyter Notebook o Google Colab sin utilizar la terminal y realizar una extracci√≥n de datos de TripAdvisor utilizando Scrapy Map Compose. Recuerda personalizar la l√≥gica de extracci√≥n de datos dentro del m√©todo `parse()` y el procesamiento de resultados seg√∫n tus necesidades espec√≠ficas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Web Scraping Horizontal y Vertical (Extracci√≥n de MERCADO LIBRE con Scrapy)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El web scraping horizontal y vertical se refiere a dos enfoques diferentes para extraer datos de un sitio web utilizando Scrapy. En el web scraping horizontal, se recopilan datos de varias p√°ginas dentro de un mismo sitio web, mientras que en el web scraping vertical se recopilan datos de diferentes sitios web que comparten una estructura similar. A continuaci√≥n, te dar√© una explicaci√≥n detallada de cada uno de ellos junto con ejemplos utilizando Scrapy para extraer datos de Mercado Libre.\n",
    "\n",
    "* **Web Scraping Horizontal:**\n",
    "\n",
    "El web scraping horizontal implica extraer datos de varias p√°ginas dentro de un mismo sitio web. Por ejemplo, si deseas obtener informaci√≥n de varios productos en Mercado Libre, puedes recorrer las diferentes p√°ginas de resultados de b√∫squeda y extraer los detalles de cada producto. Aqu√≠ tienes un ejemplo de c√≥mo hacerlo con Scrapy:\n",
    "\n",
    "1. **Definir el spider de Scrapy:**\n",
    "\n",
    "Crea una clase que herede de `scrapy.Spider` y defina la l√≥gica de extracci√≥n de datos dentro del m√©todo `parse()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MercadoLibreSpider(scrapy.Spider):\n",
    "    name = 'mercado_libre_spider'\n",
    "    start_urls = ['https://www.mercadolibre.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extraer datos de la p√°gina actual\n",
    "        # ...\n",
    "\n",
    "        # Seguir a la siguiente p√°gina\n",
    "        next_page_url = response.css('a.next-page-link::attr(href)').get()\n",
    "        if next_page_url:\n",
    "            yield response.follow(next_page_url, callback=self.parse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se define un spider llamado `MercadoLibreSpider` con una URL inicial de Mercado Libre. En el m√©todo `parse()`, se extraen los datos de la p√°gina actual y se busca el enlace hacia la siguiente p√°gina de resultados. Si se encuentra un enlace, se utiliza `response.follow()` para seguirlo y llamar nuevamente al m√©todo `parse()` para extraer datos de la siguiente p√°gina.\n",
    "\n",
    "2. **Definir la l√≥gica de extracci√≥n de datos:**\n",
    "\n",
    "Dentro del m√©todo `parse()`, puedes utilizar selectores `CSS` o `XPath` para extraer los datos deseados de la p√°gina actual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    # Extraer datos de la p√°gina actual\n",
    "    product_titles = response.css('.product-title::text').getall()\n",
    "    product_prices = response.css('.product-price::text').getall()\n",
    "\n",
    "    for title, price in zip(product_titles, product_prices):\n",
    "        yield {\n",
    "            'title': title,\n",
    "            'price': price\n",
    "        }\n",
    "\n",
    "    # Seguir a la siguiente p√°gina\n",
    "    next_page_url = response.css('a.next-page-link::attr(href)').get()\n",
    "    if next_page_url:\n",
    "        yield response.follow(next_page_url, callback=self.parse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se utilizan selectores CSS para extraer los t√≠tulos y precios de los productos en la p√°gina actual. Luego, se itera sobre los datos extra√≠dos y se genera un diccionario con los campos deseados. `Mediante yield`, los datos se env√≠an a la salida del spider. Luego, se busca el enlace hacia la siguiente p√°gina de resultados y se sigue el enlace utilizando `response.follow()` para continuar extrayendo datos de las p√°ginas subsiguientes.\n",
    "\n",
    "3. **Ejecutar el spider:**\n",
    "\n",
    "Para ejecutar el spider, puedes utilizar el siguiente c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy.cmdline.execute(['scrapy', 'crawl', 'mercado_libre_spider', '-o', 'output.csv'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este comando ejecutar√° el spider `MercadoLibreSpider` y guardar√° los datos extra√≠dos en un archivo CSV llamado `output.csv`.\n",
    "\n",
    "Con estos pasos, podr√°s realizar web scraping horizontal en Mercado Libre utilizando Scrapy. El spider recorrer√° las diferentes p√°ginas de resultados de b√∫squeda y extraer√° los detalles de los productos en cada p√°gina.\n",
    "\n",
    "* **Web Scraping Vertical:**\n",
    "\n",
    "El web scraping vertical implica extraer datos de diferentes sitios web que comparten una estructura similar. Por ejemplo, si deseas obtener informaci√≥n de productos de diferentes categor√≠as en Mercado Libre, puedes extraer datos de p√°ginas de categor√≠as espec√≠ficas. Aqu√≠ tienes un ejemplo de c√≥mo hacerlo con Scrapy:\n",
    "\n",
    "1. **Definir el spider de Scrapy:**\n",
    "\n",
    "Crea una clase que herede de `scrapy.Spider` y defina la l√≥gica de extracci√≥n de datos dentro del m√©todo `parse()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MercadoLibreSpider(scrapy.Spider):\n",
    "    name = 'mercado_libre_spider'\n",
    "    start_urls = [\n",
    "        'https://www.mercadolibre.com/categorias/celulares',\n",
    "        'https://www.mercadolibre.com/categorias/laptops',\n",
    "        'https://www.mercadolibre.com/categorias/televisores'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extraer datos de la p√°gina actual\n",
    "        # ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se define un spider llamado `MercadoLibreSpider` con URLs iniciales de diferentes categor√≠as en Mercado Libre. En el m√©todo `parse()`, se realizar√° la extracci√≥n de datos de cada p√°gina.\n",
    "\n",
    "2. **Definir la l√≥gica de extracci√≥n de datos:**\n",
    "\n",
    "Dentro del m√©todo `parse()`, puedes utilizar selectores CSS o XPath para extraer los datos deseados de la p√°gina actual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    # Extraer datos de la p√°gina actual\n",
    "    product_titles = response.css('.product-title::text').getall()\n",
    "    product_prices = response.css('.product-price::text').getall()\n",
    "\n",
    "    for title, price in zip(product_titles, product_prices):\n",
    "        yield {\n",
    "            'category': response.url.split('/')[-1],\n",
    "            'title': title,\n",
    "            'price': price\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se utilizan selectores CSS para extraer los t√≠tulos y precios de los productos en la p√°gina actual. Adem√°s, se incluye el campo \"`category`\" que indica la categor√≠a actual bas√°ndose en la URL. Mediante `yield`, los datos se env√≠an a la salida del `spider`.\n",
    "\n",
    "3. **Ejecutar el spider:**\n",
    "\n",
    "Para ejecutar el spider, puedes utilizar el siguiente c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy.cmdline.execute(['scrapy', 'crawl', 'mercado_libre_spider', '-o', 'output.csv'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este comando ejecutar√° el spider `MercadoLibreSpider` y guardar√° los datos extra√≠dos en un archivo CSV llamado output.csv.\n",
    "\n",
    "Con estos pasos, podr√°s realizar web scraping vertical en Mercado Libre utilizando Scrapy. El spider extraer√° datos de diferentes categor√≠as y guardar√° los resultados en un archivo CSV."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Varios tipos de Items y 2 dimensiones horizontales (Extracci√≥n de IGN)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para extraer informaci√≥n de una p√°gina web utilizando Scrapy, puedes definir diferentes tipos de elementos llamados \"`items`\" que representan los datos que deseas extraer. Cada item puede tener diferentes campos para almacenar informaci√≥n espec√≠fica. En el caso de la extracci√≥n de `IGN`, podr√≠amos definir dos tipos de items para representar los art√≠culos y los comentarios de los usuarios. A continuaci√≥n, te mostrar√© c√≥mo puedes hacerlo con ejemplos:\n",
    "\n",
    "1. **Definir los Items:**\n",
    "\n",
    "Primero, debes definir los items en el archivo `items.py`. Aqu√≠ est√° un ejemplo de c√≥mo podr√≠an ser los items para los art√≠culos y los comentarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ArticleItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    author = scrapy.Field()\n",
    "    content = scrapy.Field()\n",
    "\n",
    "class CommentItem(scrapy.Item):\n",
    "    username = scrapy.Field()\n",
    "    comment = scrapy.Field()\n",
    "    likes = scrapy.Field()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos definido dos items: `ArticleItem` y `CommentItem`. Cada item tiene diferentes campos que corresponden a los datos que queremos extraer, como el t√≠tulo del art√≠culo, el autor, el contenido, el nombre de usuario del comentario, el texto del comentario y la cantidad de likes.\n",
    "\n",
    "2. **Configurar el Spider:**\n",
    "\n",
    "A continuaci√≥n, debes configurar el spider en el archivo `spiders.py` para realizar la extracci√≥n de datos. Aqu√≠ hay un ejemplo b√°sico de c√≥mo podr√≠a ser el spider para extraer art√≠culos y comentarios de `IGN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from myproject.items import ArticleItem, CommentItem\n",
    "\n",
    "class IGNSpider(CrawlSpider):\n",
    "    name = 'ign_spider'\n",
    "    allowed_domains = ['ign.com']\n",
    "    start_urls = ['https://www.ign.com/articles']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=r'/article/'), callback='parse_article', follow=True),\n",
    "        Rule(LinkExtractor(allow=r'/comments/'), callback='parse_comment', follow=True),\n",
    "    )\n",
    "\n",
    "    def parse_article(self, response):\n",
    "        article = ArticleItem()\n",
    "        article['title'] = response.css('h1.article-title::text').get()\n",
    "        article['author'] = response.css('.author-name::text').get()\n",
    "        article['content'] = response.css('.article-content > p::text').getall()\n",
    "        yield article\n",
    "\n",
    "    def parse_comment(self, response):\n",
    "        comments = response.css('.comment')\n",
    "        for comment in comments:\n",
    "            item = CommentItem()\n",
    "            item['username'] = comment.css('.username::text').get()\n",
    "            item['comment'] = comment.css('.comment-body::text').get()\n",
    "            item['likes'] = comment.css('.comment-likes::text').get()\n",
    "            yield item"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos configurado un spider llamado `IGNSpider` que sigue dos reglas. La primera regla busca enlaces que coincidan con `/article/` y llama a la funci√≥n `parse_article()` para extraer los datos del art√≠culo. La segunda regla busca enlaces que coincidan con `/comments/` y llama a la funci√≥n `parse_comment()` para extraer los datos de los comentarios.\n",
    "\n",
    "En las funciones `parse_article()` y `parse_comment()`, utilizamos selectores CSS para extraer los datos espec√≠ficos que nos interesan y los asignamos a los campos correspondientes en los items.\n",
    "\n",
    "3. **Ejecutar el Spider:**\n",
    "\n",
    "Para ejecutar el spider y extraer los datos, puedes usar el siguiente comando en la terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl ign_spider -o output.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto ejecutar√° el spider `ign_spider` y guardar√° los datos extra√≠dos en un archivo `JSON` llamado `output.json`.\n",
    "\n",
    "Tambi√©n puedes ejecutar el spider directamente en Jupyter Notebook o Google Colab utilizando el siguiente c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "process.crawl('ign_spider')\n",
    "process.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto ejecutar√° el spider y mostrar√° los datos extra√≠dos en la salida de la celda.\n",
    "\n",
    "Con este enfoque, puedes extraer los art√≠culos y los comentarios de IGN utilizando Scrapy. Aseg√∫rate de ajustar los selectores CSS y las reglas del spider seg√∫n la estructura espec√≠fica del sitio web que est√°s raspando."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 niveles de profundidad (Extracci√≥n de TRIP ADVISOR con Scrapy PT.3)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para extraer informaci√≥n de varias p√°ginas web con niveles de profundidad utilizando Scrapy, puedes configurar reglas de extracci√≥n que te permitan seguir enlaces en diferentes niveles. En el caso de la extracci√≥n de datos de `TripAdvisor`, puedes definir un spider que extraiga informaci√≥n de las p√°ginas de hoteles y, a su vez, siga los enlaces a las p√°ginas de comentarios de cada hotel para extraer informaci√≥n adicional. A continuaci√≥n, te mostrar√© c√≥mo hacerlo con ejemplos:\n",
    "\n",
    "1. **Configurar el Spider:**\n",
    "\n",
    "En el archivo `spiders.py`, puedes definir el spider para extraer informaci√≥n de los hoteles y comentarios de `TripAdvisor`. Aqu√≠ hay un ejemplo b√°sico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from myproject.items import HotelItem, CommentItem\n",
    "\n",
    "class TripAdvisorSpider(CrawlSpider):\n",
    "    name = 'tripadvisor_spider'\n",
    "    allowed_domains = ['tripadvisor.com']\n",
    "    start_urls = ['https://www.tripadvisor.com/Hotels']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=r'/Hotel_Review-'), callback='parse_hotel', follow=True),\n",
    "        Rule(LinkExtractor(allow=r'/ShowUserReviews-'), callback='parse_comment', follow=True),\n",
    "    )\n",
    "\n",
    "    def parse_hotel(self, response):\n",
    "        hotel = HotelItem()\n",
    "        hotel['name'] = response.css('.listing_title::text').get()\n",
    "        hotel['address'] = response.css('.address .street-address::text').get()\n",
    "        # Otros campos del hotel\n",
    "\n",
    "        yield hotel\n",
    "\n",
    "    def parse_comment(self, response):\n",
    "        comment = CommentItem()\n",
    "        comment['username'] = response.css('.info_text .username span::text').get()\n",
    "        comment['comment'] = response.css('.review-container .partial_entry::text').get()\n",
    "        # Otros campos del comentario\n",
    "\n",
    "        yield comment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos definido un spider llamado `TripAdvisorSpider`. Hemos configurado dos reglas: una para los enlaces que coinciden con `/Hotel_Review-`, que llamar√° a la funci√≥n `parse_hotel()` para extraer los datos del hotel, y otra para los enlaces que coinciden con `/ShowUserReviews-`, que llamar√° a la funci√≥n `parse_comment()` para extraer los datos de los comentarios.\n",
    "\n",
    "En las funciones `parse_hotel()` y `parse_comment()`, utilizamos selectores CSS para extraer los datos espec√≠ficos que nos interesan y los asignamos a los campos correspondientes en los `items HotelItem` y `CommentItem`.\n",
    "\n",
    "2. **Ejecutar el Spider:**\n",
    "\n",
    "Para ejecutar el spider y extraer los datos, puedes usar el siguiente comando en la terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl tripadvisor_spider -o output.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto ejecutar√° el spider `tripadvisor_spider` y guardar√° los datos extra√≠dos en un archivo `JSON` llamado `output.json`.\n",
    "\n",
    "Tambi√©n puedes ejecutar el spider directamente en Jupyter Notebook o Google Colab utilizando el siguiente c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "process.crawl('tripadvisor_spider')\n",
    "process.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto ejecutar√° el spider y mostrar√° los datos extra√≠dos en la salida de la celda.\n",
    "\n",
    "Con este enfoque, puedes extraer informaci√≥n de los hoteles y comentarios de `TripAdvisor`, siguiendo enlaces a diferentes niveles de profundidad. Aseg√∫rate de ajustar los selectores CSS y las reglas del spider seg√∫n la estructura espec√≠fica del sitio web de `TripAdvisor`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scrapy Link Extractor (Extracci√≥n de FARMACIA CRUZ VERDE)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la extracci√≥n de datos de un sitio web como Farmacia Cruz Verde utilizando Scrapy y el m√≥dulo `LinkExtractor`, puedes seguir los siguientes pasos:\n",
    "\n",
    "1. **Configurar el Spider:**\n",
    "\n",
    "En el archivo `spiders.py`, define el spider para realizar la extracci√≥n de datos. Aqu√≠ tienes un ejemplo b√°sico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "\n",
    "class CruzVerdeSpider(CrawlSpider):\n",
    "    name = 'cruzverde_spider'\n",
    "    allowed_domains = ['cruzverde.cl']\n",
    "    start_urls = ['https://www.cruzverde.cl/']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=r'/producto/'), callback='parse_product'),\n",
    "    )\n",
    "\n",
    "    def parse_product(self, response):\n",
    "        # Extraer datos del producto\n",
    "        product = {}\n",
    "        product['title'] = response.css('h1::text').get()\n",
    "        product['price'] = response.css('.price::text').get()\n",
    "        # Otros campos del producto\n",
    "\n",
    "        yield product"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos definido un spider llamado `CruzVerdeSpider`. Hemos configurado una regla utilizando LinkExtractor que coincide con los enlaces que contienen `/producto/`, y se llamar√° a la funci√≥n `parse_product()` para extraer los datos del producto.\n",
    "\n",
    "Dentro de la funci√≥n `parse_product()`, utilizamos selectores CSS para extraer los datos espec√≠ficos que nos interesan, como el t√≠tulo del producto y el precio, y los asignamos a un diccionario `product`. Finalmente, utilizamos `yield` para enviar el diccionario product como resultado de la extracci√≥n.\n",
    "\n",
    "2. **Ejecutar el Spider:**\n",
    "\n",
    "Para ejecutar el spider y extraer los datos, puedes utilizar el siguiente comando en la terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl cruzverde_spider -o output.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto ejecutar√° el spider `cruzverde_spider` y guardar√° los datos extra√≠dos en un archivo `JSON` llamado `output.json`.\n",
    "\n",
    "Tambi√©n puedes ejecutar el spider directamente en Jupyter Notebook o Google Colab utilizando el siguiente c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "process.crawl('cruzverde_spider')\n",
    "process.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto ejecutar√° el spider y mostrar√° los datos extra√≠dos en la salida de la celda.\n",
    "\n",
    "Con esto, puedes utilizar el m√≥dulo `LinkExtractor` de Scrapy para realizar la extracci√≥n de datos del sitio web de Farmacia Cruz Verde siguiendo los enlaces que cumplen con ciertas reglas. Aseg√∫rate de ajustar los selectores CSS y las reglas del spider seg√∫n la estructura espec√≠fica del sitio web de Farmacia Cruz Verde."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **M√∫ltiples URLs Semilla (Extracci√≥n de URBANIA PT. 1)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la extracci√≥n de datos de m√∫ltiples URLs semilla utilizando Scrapy para el sitio web Urbania, puedes seguir estos pasos:\n",
    "\n",
    "1. **Configurar el Spider:**\n",
    "\n",
    "En el archivo `spiders.py`, define el spider para realizar la extracci√≥n de datos. Aqu√≠ tienes un ejemplo b√°sico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "class UrbaniaSpider(CrawlSpider):\n",
    "    name = 'urbania_spider'\n",
    "    allowed_domains = ['urbania.pe']\n",
    "    start_urls = [\n",
    "        'https://urbania.pe/buscar/proyectos',\n",
    "        'https://urbania.pe/buscar/propiedades-en-venta'\n",
    "    ]\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=r'/proyecto/'), callback='parse_project'),\n",
    "        Rule(LinkExtractor(allow=r'/inmueble/'), callback='parse_property'),\n",
    "    )\n",
    "\n",
    "    def parse_project(self, response):\n",
    "        # Extraer datos del proyecto\n",
    "        project = {}\n",
    "        project['title'] = response.css('h1::text').get()\n",
    "        project['location'] = response.css('.location::text').get()\n",
    "        # Otros campos del proyecto\n",
    "\n",
    "        yield project\n",
    "\n",
    "    def parse_property(self, response):\n",
    "        # Extraer datos de la propiedad\n",
    "        property = {}\n",
    "        property['title'] = response.css('h1::text').get()\n",
    "        property['price'] = response.css('.price::text').get()\n",
    "        # Otros campos de la propiedad\n",
    "\n",
    "        yield property"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos definido un spider llamado `UrbaniaSpider`. Hemos configurado dos URLs semilla en la lista `start_urls`, que son las p√°ginas de b√∫squeda de proyectos y propiedades en venta en el sitio web de Urbania.\n",
    "\n",
    "Hemos definido dos reglas utilizando `LinkExtractor`. La primera regla coincide con los enlaces que contienen `/proyecto/` y llamar√° a la funci√≥n `parse_project()` para extraer los datos del proyecto. La segunda regla coincide con los enlaces que contienen `/inmueble/` y llamar√° a la funci√≥n `parse_property()` para extraer los datos de la propiedad.\n",
    "\n",
    "Dentro de las funciones `parse_project()` y `parse_property()`, utilizamos selectores CSS para extraer los datos espec√≠ficos que nos interesan, como el t√≠tulo, la ubicaci√≥n y el precio, y los asignamos a diccionarios (`project` y `property`). Finalmente, utilizamos yield para enviar los diccionarios como resultados de la extracci√≥n.\n",
    "\n",
    "2. **Ejecutar el Spider:**\n",
    "\n",
    "Para ejecutar el spider y extraer los datos, puedes utilizar el siguiente comando en la terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl urbania_spider -o output.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto ejecutar√° el spider `urbania_spider` y guardar√° los datos extra√≠dos en un archivo `JSON` llamado `output.json`.\n",
    "\n",
    "Tambi√©n puedes ejecutar el spider directamente en Jupyter Notebook o Google Colab utilizando el siguiente c√≥digo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "process = CrawlerProcess(get_project_settings())\n",
    "process.crawl('urbania_spider')\n",
    "process.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto ejecutar√° el spider y mostrar√° los datos extra√≠dos en la salida de la celda.\n",
    "\n",
    "Con esto, puedes utilizar m√∫ltiples URLs semilla en Scrapy para realizar la extracci√≥n de datos del sitio web de Urbania, siguiendo las reglas definidas en el spider. Aseg√∫rate de ajustar los selectores CSS y las reglas seg√∫n la estructura espec√≠fica del sitio web de Urbania."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Web Scraping en la Nube con CRAWLERA (Extracci√≥n de URBANIA PT. 2)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawlera es un servicio de proxy web desarrollado por Scrapinghub que se utiliza para realizar web scraping de manera eficiente y sin bloqueos. Proporciona una infraestructura escalable de IPs y proxies para evitar la detecci√≥n y bloqueo por parte de los sitios web objetivo.\n",
    "\n",
    "Para utilizar Crawlera en tu proyecto de extracci√≥n de datos de Urbania, sigue estos pasos:\n",
    "\n",
    "1. **Configurar la API de Crawlera:**\n",
    "\n",
    "Antes de comenzar, necesitar√°s una cuenta en Crawlera y obtener tu clave de API. Puedes crear una cuenta en el sitio web de Crawlera (https://www.crawlera.com/).\n",
    "\n",
    "2. **Instalar y configurar el middleware de Crawlera:**\n",
    "\n",
    "En tu proyecto Scrapy, instala el paquete `scrapy-crawlera` utilizando el siguiente comando:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install scrapy-crawlera`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instalado, agrega el `middleware` de Crawlera a tu configuraci√≥n de Scrapy. Abre el archivo `settings.py` y agrega la siguiente l√≠nea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'scrapy_crawlera.CrawleraMiddleware': 610\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adem√°s, configura la variable `CRAWLERA_APIKEY` con tu clave de `API` de `Crawlera`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAWLERA_APIKEY = 'tu_clave_de_api'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Utilizar Crawlera en tu spider:**\n",
    "\n",
    "Ahora, puedes utilizar Crawlera en tu spider para realizar las solicitudes HTTP a Urbania a trav√©s de Crawlera. Para ello, simplemente incluye el siguiente encabezado en tus solicitudes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request.headers['X-Crawlera-Use-HTTPS'] = '1'\n",
    "request.headers['X-Crawlera-Cookies'] = 'disable'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu√≠ tienes un ejemplo de c√≥mo se ver√≠a el spider de Urbania modificado para utilizar `Crawlera`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "class UrbaniaSpider(CrawlSpider):\n",
    "    name = 'urbania_spider'\n",
    "    allowed_domains = ['urbania.pe']\n",
    "    start_urls = ['https://urbania.pe/buscar/proyectos']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=r'/proyecto/'), callback='parse_project'),\n",
    "    )\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield scrapy.Request(url, headers={\n",
    "                'X-Crawlera-Use-HTTPS': '1',\n",
    "                'X-Crawlera-Cookies': 'disable'\n",
    "            })\n",
    "\n",
    "    def parse_project(self, response):\n",
    "        # L√≥gica para extraer los datos del proyecto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos agregado el encabezado necesario en el m√©todo `start_requests()` para que todas las solicitudes utilizan `Crawlera`. Adem√°s, hemos eliminado la segunda regla y la funci√≥n `parse_property` para simplificar el ejemplo.\n",
    "\n",
    "4. **Ejecutar el spider con Crawlera:**\n",
    "\n",
    "Para ejecutar el spider y realizar la extracci√≥n de datos utilizando Crawlera, simplemente ejecuta el comando habitual de Scrapy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl urbania_spider -o output.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto enviar√° todas las solicitudes a trav√©s de Crawlera y te permitir√° extraer los datos de Urbania sin preocuparte por el bloqueo o la detecci√≥n.\n",
    "\n",
    "Con estos pasos, has configurado y utilizado Crawlera en tu proyecto de extracci√≥n de datos de Urbania. Crawlera manejar√° la rotaci√≥n de IP, evitar√° los bloqueos y te proporcionar√° una infraestructura escalable para realizar el web scraping de manera efectiva y confiable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scrapy bajo el Microscopio (Primer Requerimiento, Delay, CSVs y Concurrencia)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy es un framework de web scraping poderoso y flexible que permite extraer datos de manera eficiente de sitios web. En esta explicaci√≥n detallada, abordaremos los siguientes aspectos importantes de Scrapy: primer requerimiento, delay, CSVs y concurrencia.\n",
    "\n",
    "1. **Primer requerimiento:**\n",
    "\n",
    "Cuando realizas solicitudes web con Scrapy, es importante enviar un \"`primer requerimiento`\" a la p√°gina objetivo antes de comenzar la extracci√≥n de datos. Esto se hace para establecer una conexi√≥n inicial con el servidor y asegurarse de que est√°s autorizado para acceder al sitio web.\n",
    "\n",
    "Aqu√≠ tienes un ejemplo de c√≥mo enviar un primer requerimiento en Scrapy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'my_spider'\n",
    "    start_urls = ['https://www.example.com']\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.start_urls:\n",
    "            yield scrapy.Request(url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        # L√≥gica para extraer los datos de la respuesta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, el m√©todo `start_requests()` se utiliza para enviar el primer requerimiento a la URL de inicio especificada. Luego, el m√©todo `parse()` se encarga de extraer los datos de la respuesta.\n",
    "\n",
    "2. **Delay:**\n",
    "\n",
    "El delay (retraso) se refiere a la pausa entre las solicitudes realizadas por Scrapy. Es una pr√°ctica recomendada incluir un retraso entre las solicitudes para evitar sobrecargar los servidores y reducir las posibilidades de ser bloqueado.\n",
    "\n",
    "Puedes agregar un retraso entre las solicitudes utilizando el atributo `download_delay` en la configuraci√≥n de Scrapy. Aqu√≠ tienes un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'my_spider'\n",
    "    start_urls = ['https://www.example.com']\n",
    "    download_delay = 1  # Retraso de 1 segundo\n",
    "\n",
    "    def parse(self, response):\n",
    "        # L√≥gica para extraer los datos de la respuesta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos configurado un retraso de 1 segundo utilizando download_delay. Esto significa que Scrapy esperar√° 1 segundo antes de enviar la siguiente solicitud.\n",
    "\n",
    "3. **CSVs:**\n",
    "\n",
    "Scrapy permite guardar los datos extra√≠dos en varios formatos, incluido el formato CSV. Puedes guardar los datos en un archivo CSV utilizando el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl my_spider -o output.csv -t csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, estamos ejecutando el spider `my_spider` y guardando los datos extra√≠dos en un archivo llamado `output.csv` en formato `CSV`.\n",
    "\n",
    "4. **Concurrencia:**\n",
    "\n",
    "Scrapy es capaz de manejar la concurrencia y realizar m√∫ltiples solicitudes simult√°neamente para acelerar el proceso de extracci√≥n de datos. La concurrencia en Scrapy se logra mediante el uso de hilos o procesos.\n",
    "\n",
    "Puedes controlar la concurrencia en Scrapy a trav√©s de la configuraci√≥n `CONCURRENT_REQUESTS` en el archivo `settings.py`. Aqu√≠ tienes un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.py\n",
    "CONCURRENT_REQUESTS = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos configurado `CONCURRENT_REQUESTS` en `10`, lo que significa que Scrapy realizar√° hasta 10 solicitudes simult√°neamente.\n",
    "\n",
    "Ten en cuenta que debes ser cuidadoso al configurar la concurrencia, ya que un valor demasiado alto puede sobrecargar el servidor y provocar bloqueos o rechazo de solicitudes.\n",
    "\n",
    "Estos son algunos aspectos importantes de Scrapy que puedes considerar al realizar web scraping. Recuerda que es importante ser √©tico y respetar los t√©rminos de servicio de los sitios web que est√°s raspando, adem√°s de ajustar la configuraci√≥n de Scrapy seg√∫n sea necesario para evitar sobrecargar los servidores y respetar los l√≠mites establecidos por los sitios web objetivo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Inicio** | **atr√°s 3** | **Siguiente 5** |\n",
    "|----------- |-------------- |---------------|\n",
    "| [üè†](../../README.md) | [‚è™](./3.Una_sola_pagina_estatica.ipynb)| [‚è©](./5.Paginas_Dinamicas.ipynb)|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
